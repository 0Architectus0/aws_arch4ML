{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecting for Machine Learning: Forecasting Alternative Fuels\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast project for AWS ML course.\n",
    "\n",
    "## Preparing Virtual Environment\n",
    "\n",
    "**Installing netcdf4 python library**: *The below code is confirmed to work on AWS SageMaker notebook with 'conda python 3' kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.5/dist-packages (0.9.1)\n",
      "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.5/dist-packages (from seaborn) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (0.24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas>=0.17.1->seaborn) (2019.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->seaborn) (40.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from cycler>=0.10->matplotlib>=1.5.3->seaborn) (1.12.0)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.17.9-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore==1.14.9 in /usr/local/lib/python3.5/dist-packages (from awscli) (1.14.9)\n",
      "Collecting colorama<0.4.2,>=0.2.5\n",
      "  Downloading colorama-0.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from awscli) (0.3.2)\n",
      "Collecting rsa<=3.5.0,>=3.1.2\n",
      "  Downloading rsa-3.4.2-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.5/dist-packages (from awscli) (0.15.2)\n",
      "Collecting PyYAML<5.3,>=3.10\n",
      "  Downloading PyYAML-5.2.tar.gz (265 kB)\n",
      "\u001b[K     |████████████████████████████████| 265 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.5/dist-packages (from botocore==1.14.9->awscli) (1.25.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.5/dist-packages (from botocore==1.14.9->awscli) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.5/dist-packages (from botocore==1.14.9->awscli) (2.8.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.14.9->awscli) (1.12.0)\n",
      "Building wheels for collected packages: PyYAML\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.2-cp35-cp35m-linux_x86_64.whl size=45501 sha256=d264e0c25c69ddd66c35b8c6581ba593f4d8de7789765200f19cfb499cc54695\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/ed/b3/f0e50db5ee9b90afa727f6b84f49ea8cdab7a5c747471b8c5e\n",
      "Successfully built PyYAML\n",
      "Installing collected packages: colorama, pyasn1, rsa, PyYAML, awscli\n",
      "Successfully installed PyYAML-5.2 awscli-1.17.9 colorama-0.4.1 pyasn1-0.4.8 rsa-3.4.2\n"
     ]
    }
   ],
   "source": [
    "# needed for zoltar image\n",
    "!pip install seaborn\n",
    "!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set creds\n",
    "## creds inop. awscli: unable to locate creds using script below exe here and in notebook terminal \n",
    "!export AWS_ACCESS_KEY_ID=\"ASIA3SD3GJ4K7WPIW3BZ\"\n",
    "!export AWS_SECRET_ACCESS_KEY=\"JjJfRbAWFI+ytyvPxnXwp+rhTzCHAODFse21LttS\"\n",
    "!export AWS_SESSION_TOKEN=\"IQoJb3JpZ2luX2VjEHsaCXVzLWVhc3QtMiJHMEUCIH2gGhHIuGceAO6fv84p7ZKuagA6RltumwkfIhqK90k9AiEA6xGlQ0E8/d+wEbxk45BjliFIcBnkjoiesas6BuL1FS4qgwIIJBACGgw3OTQ4MjczMTI5MTciDAzAy/5NUlTrbkrb5yrgAae47gM+6EWaGgSoEYBUzCWOpJHJGCm1mYC89JJ3jgZsRVg2jR5yg/+pjRAup9fVL9Q7P/o+TWTkFrYflFc0bR7t7G/+bPQJkedeCQ7C3VpemRKYey7WMr53h4kf+KCh/wahugQBTIN/Eey9c4RyT8S62byQ1N9I+R+W48mQmHE6yZPPht79y5CTPleWPtDmRMk4qtkvBCaa0ZDGMqxEuFG0NZDIZmI3bYpkWy+tDCpudP9PaW+eF/dMp1YhNF1s6/LNOrnzMiVKpoO3DoGgELB+TWBgiy7tWG9Ub4twMv59MM7U0/EFOukBcC6SN7OHiP0cezex0uAFG68w/6UQt0Pi3FN15BgSyu9r9G0Cqx51HKQMIZwycBLEQyq8eGesHqmtGdkoeczKx++O6BP3mhLlcrFDroMz1quWdEi4wj0Mz/Cee5mK6b8E2DM9BxVh3WD9iGGdZQZ7qpcZvU4QISK5fB5x4Qfr2dqa/zF2O1DiAQI5Uge0ksshGsvnfA6BRrk/xXcDwGqo0G4pBefHJxgY4FlZwRAWZwo4eXVvIGzv85AN0HsC4CJ+3G/yRgym224O71IN4xqIIQCTdqsXm/DIh2XGm4pLXPGbGhga+0+Jh2A=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda netcdf4 --yesx\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data to S3 Bucket\n",
    "\n",
    "The data is hosted on the Kaggle website [here](https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest#description). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kgl = pd.read_csv('data/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5113, 99)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_kgl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_kgl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_kgl['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kgl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    data = {'date': frame.to_numpy().ravel('F'),\n",
    "            'variable': np.asarray(frame.columns).repeat(N),\n",
    "            'time': np.tile(np.asarray(frame.index), K)}\n",
    "    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "\n",
    "\n",
    "df = unpivot(train_kgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date variable value\n",
       "0  19940101     Date   NaN\n",
       "1  19940102     Date   NaN\n",
       "2  19940103     Date   NaN\n",
       "3  19940104     Date   NaN\n",
       "4  19940105     Date   NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd7789070b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for names in train_kgl.columns[1:]:\n",
    "\n",
    "    plt.hist(train_kgl[names], edgecolor = 'black',\n",
    "             bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mapper = {station:idx for idx, station in enumerate(list(train_kgl)[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, freq='D', split_type = 'train', cols_to_use = ['ADAX']):\n",
    "    rt_set = []\n",
    "    \n",
    "    # use 70% for training\n",
    "    if split_type == 'train':\n",
    "        lower_bound = 0\n",
    "        upper_bound = round(df.shape[0] * .7)\n",
    "        \n",
    "    # use 15% for validation\n",
    "    elif split_type == 'validation':\n",
    "        lower_bound = round(df.shape[0] * .7)\n",
    "        upper_bound = round(df.shape[0] * .85)\n",
    "        \n",
    "    # use 15% for test\n",
    "    elif split_type == 'test':\n",
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n",
    "            \n",
    "    # loop through columns you want to use\n",
    "    for h in list(df):\n",
    "        if h in cols_to_use:\n",
    "            \n",
    "            target_column = df[h].values.tolist()[lower_bound:upper_bound]\n",
    "            \n",
    "            date_str = str(df.iloc[0]['Date'])\n",
    "            \n",
    "            year = date_str[0:4]\n",
    "            month = date_str[4:6]\n",
    "            date = date_str[7:]\n",
    "                                                \n",
    "            start_dataset = pd.Timestamp(\"{}-{}-{} 00:00:00\".format(year, month, date, freq=freq))\n",
    "                        \n",
    "            # create a new json object for each column\n",
    "            json_obj = {'start': str(start_dataset),\n",
    "                        'target':target_column,\n",
    "                        'cat': [cat_mapper[h]] }\n",
    "    \n",
    "            rt_set.append(json_obj)\n",
    "    \n",
    "    return rt_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_split(train_kgl, cols_to_use = train_kgl.columns[1:] )\n",
    "\n",
    "test_set = get_split(train_kgl, split_type = 'test', cols_to_use = train_kgl.columns[1:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file('train_kgl.json', train_set)\n",
    "write_dicts_to_file('test_kgl.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload failed: ./train_kgl.json to s3://forecastingincomingsolarenergyjan2020/vincent/train/train.json Unable to locate credentials\n",
      "upload failed: ./test_kgl.json to s3://forecastingincomingsolarenergyjan2020/vincent/test/test.json Unable to locate credentials\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train_kgl.json s3://forecastingincomingsolarenergyjan2020/vincent/train/train.json\n",
    "!aws s3 cp test_kgl.json s3://forecastingincomingsolarenergyjan2020/vincent/test/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "image = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='VRforecasting',\n",
    "    output_path='s3://forecastingincomingsolarenergyjan2020/vincent/output/output_deepar1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "    # frequency interval is once per day\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"10E-4\",\n",
    "    \n",
    "    # let's use the last 30 days for context\n",
    "    \"context_length\": str(30),\n",
    "    \n",
    "    # let's forecast for 30 days\n",
    "    \"prediction_length\": str(30)\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:03:53 Starting - Starting the training job...\n",
      "2020-01-30 20:03:55 Starting - Launching requested ML instances......\n",
      "2020-01-30 20:05:01 Starting - Preparing the instances for training......\n",
      "2020-01-30 20:06:22 Downloading - Downloading input data\n",
      "2020-01-30 20:06:22 Training - Downloading the training image...\n",
      "2020-01-30 20:06:35 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:38 INFO 139704039475008] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:38 INFO 139704039475008] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'10E-4', u'prediction_length': u'30', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'30', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:38 INFO 139704039475008] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'10E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'D', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:38 INFO 139704039475008] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] [cardinality=auto] Inferred value of cardinality=[98] from dataset.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Training set statistics:\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Integer time series\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] number of time series: 98\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] number of observations: 350742\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] mean target length: 3579\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] min/mean/max target: 300.0/16712640.5821/39442800.0\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] mean abs(target): 16712640.5821\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Small number of time series. Doing 6 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Test set statistics:\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Integer time series\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] number of time series: 98\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] number of observations: 75166\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] mean target length: 767\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] min/mean/max target: 4200.0/16273434.2382/32884800.0\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] mean abs(target): 16273434.2382\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] nvidia-smi took: 0.0252060890198 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 174.67093467712402, \"sum\": 174.67093467712402, \"min\": 174.67093467712402}}, \"EndTime\": 1580414799.434825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414799.259323}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:39 INFO 139704039475008] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 423.08616638183594, \"sum\": 423.08616638183594, \"min\": 423.08616638183594}}, \"EndTime\": 1580414799.682527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414799.434903}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] Epoch[0] Batch[0] avg_epoch_loss=19.721336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=19.7213363647\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] Epoch[0] Batch[5] avg_epoch_loss=18.936190\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=18.9361896515\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] Epoch[0] Batch [5]#011Speed: 833.76 samples/sec#011loss=18.936190\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] processed a total of 570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 1207.7951431274414, \"sum\": 1207.7951431274414, \"min\": 1207.7951431274414}}, \"EndTime\": 1580414800.890523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414799.682596}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=471.875001604 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=0, train loss <loss>=18.6498432159\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:40 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_e01ef993-8fd1-448f-a5c6-5858b51658e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.65804862976074, \"sum\": 36.65804862976074, \"min\": 36.65804862976074}}, \"EndTime\": 1580414800.927856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414800.890629}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:41 INFO 139704039475008] Epoch[1] Batch[0] avg_epoch_loss=17.763346\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=17.7633457184\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:41 INFO 139704039475008] Epoch[1] Batch[5] avg_epoch_loss=17.556911\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=17.5569108327\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:41 INFO 139704039475008] Epoch[1] Batch [5]#011Speed: 663.85 samples/sec#011loss=17.556911\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1370.6119060516357, \"sum\": 1370.6119060516357, \"min\": 1370.6119060516357}}, \"EndTime\": 1580414802.298596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414800.927918}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=459.605461809 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=1, train loss <loss>=17.4978343964\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_e99fa293-e014-44e5-811f-a21c711a2eb5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.6099910736084, \"sum\": 28.6099910736084, \"min\": 28.6099910736084}}, \"EndTime\": 1580414802.327829, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414802.298686}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] Epoch[2] Batch[0] avg_epoch_loss=17.336662\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=17.3366622925\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] Epoch[2] Batch[5] avg_epoch_loss=17.335539\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=17.3355394999\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] Epoch[2] Batch [5]#011Speed: 942.25 samples/sec#011loss=17.335539\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.1809463500977, \"sum\": 1089.1809463500977, \"min\": 1089.1809463500977}}, \"EndTime\": 1580414803.417141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414802.327894}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=545.287895166 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=2, train loss <loss>=17.3090789795\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_32ad80fe-c30a-4c3e-a62f-29231bee344f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.75785255432129, \"sum\": 34.75785255432129, \"min\": 34.75785255432129}}, \"EndTime\": 1580414803.452523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414803.417253}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] Epoch[3] Batch[0] avg_epoch_loss=17.212311\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=17.212310791\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] Epoch[3] Batch[5] avg_epoch_loss=17.183479\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=17.1834786733\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] Epoch[3] Batch [5]#011Speed: 898.94 samples/sec#011loss=17.183479\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1109.9369525909424, \"sum\": 1109.9369525909424, \"min\": 1109.9369525909424}}, \"EndTime\": 1580414804.562595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414803.452591}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=561.228887105 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=3, train loss <loss>=17.1878015518\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:44 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_5c35147d-32db-43f5-ac4d-658a6956b2eb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.517919540405273, \"sum\": 22.517919540405273, \"min\": 22.517919540405273}}, \"EndTime\": 1580414804.585712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414804.562682}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] Epoch[4] Batch[0] avg_epoch_loss=17.157856\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=17.1578559875\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] Epoch[4] Batch[5] avg_epoch_loss=17.078363\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=17.0783627828\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] Epoch[4] Batch [5]#011Speed: 929.71 samples/sec#011loss=17.078363\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.9949893951416, \"sum\": 1078.9949893951416, \"min\": 1078.9949893951416}}, \"EndTime\": 1580414805.66485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414804.585791}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=562.50845726 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=4, train loss <loss>=17.1092046738\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:45 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_394a37db-68ad-4a3e-b38d-f0fedeaa7355-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.015975952148438, \"sum\": 23.015975952148438, \"min\": 23.015975952148438}}, \"EndTime\": 1580414805.688476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414805.664917}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] Epoch[5] Batch[0] avg_epoch_loss=17.116741\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=17.1167411804\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] Epoch[5] Batch[5] avg_epoch_loss=17.116699\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=17.116698583\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] Epoch[5] Batch [5]#011Speed: 925.16 samples/sec#011loss=17.116699\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] processed a total of 577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1057.7178001403809, \"sum\": 1057.7178001403809, \"min\": 1057.7178001403809}}, \"EndTime\": 1580414806.74634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414805.688554}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=545.460886765 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=5, train loss <loss>=17.0852466583\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:46 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_aa2c428e-e228-4861-814e-66ace00522c9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.499011993408203, \"sum\": 23.499011993408203, \"min\": 23.499011993408203}}, \"EndTime\": 1580414806.770468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414806.746406}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] Epoch[6] Batch[0] avg_epoch_loss=17.009819\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=17.0098190308\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] Epoch[6] Batch[5] avg_epoch_loss=17.059971\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=17.0599711736\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] Epoch[6] Batch [5]#011Speed: 924.98 samples/sec#011loss=17.059971\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.608980178833, \"sum\": 1088.608980178833, \"min\": 1088.608980178833}}, \"EndTime\": 1580414807.859217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414806.770539}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=554.769677568 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=6, train loss <loss>=17.0489692688\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:47 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_583ea2eb-ffcf-4d46-948e-5262ff7e34d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.93682098388672, \"sum\": 22.93682098388672, \"min\": 22.93682098388672}}, \"EndTime\": 1580414807.882776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414807.859306}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] Epoch[7] Batch[0] avg_epoch_loss=17.134958\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=17.1349582672\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] Epoch[7] Batch[5] avg_epoch_loss=17.039404\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=17.0394042333\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] Epoch[7] Batch [5]#011Speed: 918.48 samples/sec#011loss=17.039404\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.467056274414, \"sum\": 1056.467056274414, \"min\": 1056.467056274414}}, \"EndTime\": 1580414808.939386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414807.88285}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=579.217479189 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=7, train loss <loss>=17.0054298401\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:48 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_172b9db5-a3a8-4b79-898a-10a8aa6ab4f7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.76412391662598, \"sum\": 33.76412391662598, \"min\": 33.76412391662598}}, \"EndTime\": 1580414808.973761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414808.939475}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:49 INFO 139704039475008] Epoch[8] Batch[0] avg_epoch_loss=17.066650\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=17.0666503906\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:49 INFO 139704039475008] Epoch[8] Batch[5] avg_epoch_loss=17.037898\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=17.0378977458\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:49 INFO 139704039475008] Epoch[8] Batch [5]#011Speed: 919.96 samples/sec#011loss=17.037898\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.4237880706787, \"sum\": 1051.4237880706787, \"min\": 1051.4237880706787}}, \"EndTime\": 1580414810.025363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414808.97383}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=556.325829778 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=8, train loss <loss>=17.0538600922\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] Epoch[9] Batch[0] avg_epoch_loss=17.080284\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=17.0802841187\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] Epoch[9] Batch[5] avg_epoch_loss=16.988368\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=16.9883680344\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:50 INFO 139704039475008] Epoch[9] Batch [5]#011Speed: 911.30 samples/sec#011loss=16.988368\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] processed a total of 549 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.2951755523682, \"sum\": 1036.2951755523682, \"min\": 1036.2951755523682}}, \"EndTime\": 1580414811.062254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414810.025444}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=529.703957632 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=9, train loss <loss>=17.0086917877\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] Epoch[10] Batch[0] avg_epoch_loss=16.958982\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=16.9589824677\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] Epoch[10] Batch[5] avg_epoch_loss=16.993052\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=16.9930515289\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:51 INFO 139704039475008] Epoch[10] Batch [5]#011Speed: 940.71 samples/sec#011loss=16.993052\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.1481285095215, \"sum\": 1101.1481285095215, \"min\": 1101.1481285095215}}, \"EndTime\": 1580414812.164109, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414811.062343}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=534.844828982 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=10, train loss <loss>=16.9656589508\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_e5d02fcb-3017-4c55-b0e9-6e4bb09549f3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.646116256713867, \"sum\": 23.646116256713867, \"min\": 23.646116256713867}}, \"EndTime\": 1580414812.188389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414812.164175}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] Epoch[11] Batch[0] avg_epoch_loss=17.022699\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=17.0226993561\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] Epoch[11] Batch[5] avg_epoch_loss=17.028655\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=17.0286553701\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:52 INFO 139704039475008] Epoch[11] Batch [5]#011Speed: 937.26 samples/sec#011loss=17.028655\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 978.7919521331787, \"sum\": 978.7919521331787, \"min\": 978.7919521331787}}, \"EndTime\": 1580414813.167323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414812.188459}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=576.14093998 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=11, train loss <loss>=17.0400002797\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] Epoch[12] Batch[0] avg_epoch_loss=17.024008\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=17.0240077972\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] Epoch[12] Batch[5] avg_epoch_loss=16.994910\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=16.9949099223\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:53 INFO 139704039475008] Epoch[12] Batch [5]#011Speed: 935.81 samples/sec#011loss=16.994910\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.196907043457, \"sum\": 1053.196907043457, \"min\": 1053.196907043457}}, \"EndTime\": 1580414814.221154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414813.167414}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=552.522417687 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=12, train loss <loss>=16.9667615891\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] Epoch[13] Batch[0] avg_epoch_loss=16.949284\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=16.9492835999\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] Epoch[13] Batch[5] avg_epoch_loss=16.983154\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=16.9831542969\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:54 INFO 139704039475008] Epoch[13] Batch [5]#011Speed: 930.95 samples/sec#011loss=16.983154\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.5618324279785, \"sum\": 1051.5618324279785, \"min\": 1051.5618324279785}}, \"EndTime\": 1580414815.273333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414814.221263}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=551.499133655 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=13, train loss <loss>=16.9913068771\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] Epoch[14] Batch[0] avg_epoch_loss=16.993975\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=16.9939746857\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] Epoch[14] Batch[5] avg_epoch_loss=16.959534\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=16.9595340093\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] Epoch[14] Batch [5]#011Speed: 895.96 samples/sec#011loss=16.959534\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.1351165771484, \"sum\": 1070.1351165771484, \"min\": 1070.1351165771484}}, \"EndTime\": 1580414816.344035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414815.273413}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=569.940464814 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=14, train loss <loss>=16.9388347626\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_f5574060-96de-4095-97ca-0a5fdaac1960-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.190902709960938, \"sum\": 24.190902709960938, \"min\": 24.190902709960938}}, \"EndTime\": 1580414816.368914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414816.344142}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] Epoch[15] Batch[0] avg_epoch_loss=16.979023\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=16.9790229797\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] Epoch[15] Batch[5] avg_epoch_loss=16.945156\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=16.9451560974\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] Epoch[15] Batch [5]#011Speed: 809.06 samples/sec#011loss=16.945156\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1113.8949394226074, \"sum\": 1113.8949394226074, \"min\": 1113.8949394226074}}, \"EndTime\": 1580414817.482951, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414816.368988}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=563.722184544 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=15, train loss <loss>=16.9550374985\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] Epoch[16] Batch[0] avg_epoch_loss=16.987392\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=16.9873924255\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] Epoch[16] Batch[5] avg_epoch_loss=16.951557\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=16.9515568415\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] Epoch[16] Batch [5]#011Speed: 859.66 samples/sec#011loss=16.951557\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1022.4459171295166, \"sum\": 1022.4459171295166, \"min\": 1022.4459171295166}}, \"EndTime\": 1580414818.505993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414817.48304}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=560.356683003 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=16, train loss <loss>=16.9535403781\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] Epoch[17] Batch[0] avg_epoch_loss=17.018799\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=17.0187988281\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] Epoch[17] Batch[5] avg_epoch_loss=16.946336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=16.9463364283\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] Epoch[17] Batch [5]#011Speed: 920.27 samples/sec#011loss=16.946336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 987.3950481414795, \"sum\": 987.3950481414795, \"min\": 987.3950481414795}}, \"EndTime\": 1580414819.493959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414818.506071}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=580.253752954 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=17, train loss <loss>=16.9456956651\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] Epoch[18] Batch[0] avg_epoch_loss=16.869171\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:06:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=16.8691711426\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] Epoch[18] Batch[5] avg_epoch_loss=16.906774\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=16.9067738851\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] Epoch[18] Batch [5]#011Speed: 920.11 samples/sec#011loss=16.906774\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1063.1508827209473, \"sum\": 1063.1508827209473, \"min\": 1063.1508827209473}}, \"EndTime\": 1580414820.557676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414819.494026}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=544.547470104 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=18, train loss <loss>=16.9262334824\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:00 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_547cda49-e92f-4248-b6d0-a716e355d384-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.324966430664062, \"sum\": 23.324966430664062, \"min\": 23.324966430664062}}, \"EndTime\": 1580414820.581689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414820.557756}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] Epoch[19] Batch[0] avg_epoch_loss=16.920938\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=16.9209384918\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] Epoch[19] Batch[5] avg_epoch_loss=16.896533\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=16.8965333303\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] Epoch[19] Batch [5]#011Speed: 895.97 samples/sec#011loss=16.896533\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.2982063293457, \"sum\": 1083.2982063293457, \"min\": 1083.2982063293457}}, \"EndTime\": 1580414821.665124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414820.581758}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=558.388583105 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=19, train loss <loss>=16.8996234894\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:01 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_66ed9895-9bd1-4205-bfc8-c9c8fd655231-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.716129302978516, \"sum\": 34.716129302978516, \"min\": 34.716129302978516}}, \"EndTime\": 1580414821.700488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414821.665257}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] Epoch[20] Batch[0] avg_epoch_loss=17.044538\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=17.0445384979\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] Epoch[20] Batch[5] avg_epoch_loss=16.923097\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=16.9230966568\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] Epoch[20] Batch [5]#011Speed: 927.92 samples/sec#011loss=16.923097\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] processed a total of 570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1057.1930408477783, \"sum\": 1057.1930408477783, \"min\": 1057.1930408477783}}, \"EndTime\": 1580414822.757817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414821.700555}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=539.095268257 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=20, train loss <loss>=16.9175766839\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:02 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] Epoch[21] Batch[0] avg_epoch_loss=16.863249\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=16.8632488251\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] Epoch[21] Batch[5] avg_epoch_loss=16.900603\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=16.9006032944\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] Epoch[21] Batch [5]#011Speed: 919.73 samples/sec#011loss=16.900603\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.6401290893555, \"sum\": 1076.6401290893555, \"min\": 1076.6401290893555}}, \"EndTime\": 1580414823.835105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414822.757907}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=545.146952262 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=21, train loss <loss>=16.9305164337\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:03 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] Epoch[22] Batch[0] avg_epoch_loss=16.784515\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=16.7845153809\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] Epoch[22] Batch[5] avg_epoch_loss=16.862237\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=16.8622366587\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] Epoch[22] Batch [5]#011Speed: 946.47 samples/sec#011loss=16.862237\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1152.501106262207, \"sum\": 1152.501106262207, \"min\": 1152.501106262207}}, \"EndTime\": 1580414824.988165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414823.835196}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=517.078293203 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=22, train loss <loss>=16.8663955688\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:04 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_318eac9e-176f-4a3d-817a-a4da58ff6f6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.956932067871094, \"sum\": 34.956932067871094, \"min\": 34.956932067871094}}, \"EndTime\": 1580414825.023724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414824.988252}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] Epoch[23] Batch[0] avg_epoch_loss=17.001089\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=17.0010890961\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] Epoch[23] Batch[5] avg_epoch_loss=16.892914\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=16.8929141363\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:05 INFO 139704039475008] Epoch[23] Batch [5]#011Speed: 931.45 samples/sec#011loss=16.892914\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.0048751831055, \"sum\": 1038.0048751831055, \"min\": 1038.0048751831055}}, \"EndTime\": 1580414826.06187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414825.023794}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=550.962102985 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=23, train loss <loss>=16.8968671163\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] Epoch[24] Batch[0] avg_epoch_loss=16.870150\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=16.8701496124\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] Epoch[24] Batch[5] avg_epoch_loss=16.863602\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=16.8636020025\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:06 INFO 139704039475008] Epoch[24] Batch [5]#011Speed: 916.39 samples/sec#011loss=16.863602\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1107.8178882598877, \"sum\": 1107.8178882598877, \"min\": 1107.8178882598877}}, \"EndTime\": 1580414827.170422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414826.062004}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=549.665344973 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=24, train loss <loss>=16.8552110672\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_eb972471-3269-43a1-b5ab-24b3daab2518-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.831119537353516, \"sum\": 33.831119537353516, \"min\": 33.831119537353516}}, \"EndTime\": 1580414827.204839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414827.17051}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] Epoch[25] Batch[0] avg_epoch_loss=16.855757\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=16.8557567596\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] Epoch[25] Batch[5] avg_epoch_loss=16.881468\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=16.8814681371\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] Epoch[25] Batch [5]#011Speed: 875.63 samples/sec#011loss=16.881468\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1107.017993927002, \"sum\": 1107.017993927002, \"min\": 1107.017993927002}}, \"EndTime\": 1580414828.311993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414827.204907}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.739443097 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=25, train loss <loss>=16.8727863312\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] Epoch[26] Batch[0] avg_epoch_loss=16.886980\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=16.8869800568\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] Epoch[26] Batch[5] avg_epoch_loss=16.879258\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=16.8792578379\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] Epoch[26] Batch [5]#011Speed: 938.17 samples/sec#011loss=16.879258\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1093.3759212493896, \"sum\": 1093.3759212493896, \"min\": 1093.3759212493896}}, \"EndTime\": 1580414829.405945, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414828.312082}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=561.499333584 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=26, train loss <loss>=16.8836950302\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] Epoch[27] Batch[0] avg_epoch_loss=16.899734\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=16.8997344971\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] Epoch[27] Batch[5] avg_epoch_loss=16.829404\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=16.8294035594\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] Epoch[27] Batch [5]#011Speed: 941.92 samples/sec#011loss=16.829404\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.6739540100098, \"sum\": 1083.6739540100098, \"min\": 1083.6739540100098}}, \"EndTime\": 1580414830.490138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414829.406029}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=569.291183098 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=27, train loss <loss>=16.8440126419\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_d7807c5e-f7d8-478c-9519-d36ef07503e2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.604869842529297, \"sum\": 23.604869842529297, \"min\": 23.604869842529297}}, \"EndTime\": 1580414830.514345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414830.490227}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] Epoch[28] Batch[0] avg_epoch_loss=16.916210\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=16.9162101746\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] Epoch[28] Batch[5] avg_epoch_loss=16.856656\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=16.8566560745\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] Epoch[28] Batch [5]#011Speed: 839.61 samples/sec#011loss=16.856656\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] processed a total of 561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.4290008544922, \"sum\": 1035.4290008544922, \"min\": 1035.4290008544922}}, \"EndTime\": 1580414831.549903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414830.51441}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=541.738921944 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=28, train loss <loss>=16.8541723887\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] Epoch[29] Batch[0] avg_epoch_loss=16.824699\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=16.8246994019\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] Epoch[29] Batch[5] avg_epoch_loss=16.853504\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=16.8535044988\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] Epoch[29] Batch [5]#011Speed: 918.76 samples/sec#011loss=16.853504\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1082.3497772216797, \"sum\": 1082.3497772216797, \"min\": 1082.3497772216797}}, \"EndTime\": 1580414832.63277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414831.549991}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=545.045695802 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=29, train loss <loss>=16.8443538666\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:12 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] Epoch[30] Batch[0] avg_epoch_loss=16.917192\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=16.9171924591\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] Epoch[30] Batch[5] avg_epoch_loss=16.859664\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=16.8596636454\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] Epoch[30] Batch [5]#011Speed: 932.56 samples/sec#011loss=16.859664\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] processed a total of 570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.77592086792, \"sum\": 1038.77592086792, \"min\": 1038.77592086792}}, \"EndTime\": 1580414833.672129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414832.632857}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=548.655020792 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=30, train loss <loss>=16.8409737481\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:13 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_d16e4f0d-3e90-49f0-b6d4-53f8dee8d6a6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.474828720092773, \"sum\": 31.474828720092773, \"min\": 31.474828720092773}}, \"EndTime\": 1580414833.704209, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414833.672216}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] Epoch[31] Batch[0] avg_epoch_loss=16.909002\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=16.9090023041\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] Epoch[31] Batch[5] avg_epoch_loss=16.841755\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=16.8417545954\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] Epoch[31] Batch [5]#011Speed: 946.41 samples/sec#011loss=16.841755\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.9599838256836, \"sum\": 1084.9599838256836, \"min\": 1084.9599838256836}}, \"EndTime\": 1580414834.789483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414833.704462}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=554.793217515 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=31, train loss <loss>=16.8198026657\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:14 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_8432a6be-7486-43cc-a5fa-8608f6577a34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.6989631652832, \"sum\": 34.6989631652832, \"min\": 34.6989631652832}}, \"EndTime\": 1580414834.824795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414834.789571}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] Epoch[32] Batch[0] avg_epoch_loss=16.762566\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=16.7625656128\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] Epoch[32] Batch[5] avg_epoch_loss=16.822956\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=16.8229557673\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] Epoch[32] Batch [5]#011Speed: 919.12 samples/sec#011loss=16.822956\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.6139125823975, \"sum\": 1066.6139125823975, \"min\": 1066.6139125823975}}, \"EndTime\": 1580414835.891543, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414834.824863}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=575.583865341 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=32, train loss <loss>=16.8092987061\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:15 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_a7590657-5685-41f9-9852-3b64b64be3ff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.869897842407227, \"sum\": 21.869897842407227, \"min\": 21.869897842407227}}, \"EndTime\": 1580414835.914008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414835.891632}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] Epoch[33] Batch[0] avg_epoch_loss=16.868271\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=16.868270874\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] Epoch[33] Batch[5] avg_epoch_loss=16.878791\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=16.8787914912\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] Epoch[33] Batch [5]#011Speed: 875.37 samples/sec#011loss=16.878791\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1081.8889141082764, \"sum\": 1081.8889141082764, \"min\": 1081.8889141082764}}, \"EndTime\": 1580414836.996039, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414835.914083}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=540.655537732 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=33, train loss <loss>=16.8222305298\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:16 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:17 INFO 139704039475008] Epoch[34] Batch[0] avg_epoch_loss=16.816128\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=16.8161277771\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:17 INFO 139704039475008] Epoch[34] Batch[5] avg_epoch_loss=16.832079\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=16.8320786158\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:17 INFO 139704039475008] Epoch[34] Batch [5]#011Speed: 917.45 samples/sec#011loss=16.832079\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] processed a total of 577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.897050857544, \"sum\": 1074.897050857544, \"min\": 1074.897050857544}}, \"EndTime\": 1580414838.071517, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414836.99613}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=536.730710869 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=34, train loss <loss>=16.8277938843\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] Epoch[35] Batch[0] avg_epoch_loss=16.857634\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=16.8576335907\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] Epoch[35] Batch[5] avg_epoch_loss=16.831204\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=16.8312037786\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:18 INFO 139704039475008] Epoch[35] Batch [5]#011Speed: 932.74 samples/sec#011loss=16.831204\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.4311065673828, \"sum\": 1052.4311065673828, \"min\": 1052.4311065673828}}, \"EndTime\": 1580414839.124505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414838.071604}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=598.540106701 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=35, train loss <loss>=16.836866951\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] Epoch[36] Batch[0] avg_epoch_loss=16.848776\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=16.8487758636\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] Epoch[36] Batch[5] avg_epoch_loss=16.820015\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=16.8200146357\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:19 INFO 139704039475008] Epoch[36] Batch [5]#011Speed: 912.64 samples/sec#011loss=16.820015\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.2341842651367, \"sum\": 1065.2341842651367, \"min\": 1065.2341842651367}}, \"EndTime\": 1580414840.190314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414839.124595}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=546.291729776 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=36, train loss <loss>=16.878837204\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] Epoch[37] Batch[0] avg_epoch_loss=16.784754\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=16.7847537994\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] Epoch[37] Batch[5] avg_epoch_loss=16.836815\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=16.8368148804\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] Epoch[37] Batch [5]#011Speed: 905.58 samples/sec#011loss=16.836815\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1054.52299118042, \"sum\": 1054.52299118042, \"min\": 1054.52299118042}}, \"EndTime\": 1580414841.245415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414840.190403}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.306083363 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=37, train loss <loss>=16.8002484639\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_357bc6f0-7c22-4241-adcf-30105d292db5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.83414649963379, \"sum\": 34.83414649963379, \"min\": 34.83414649963379}}, \"EndTime\": 1580414841.280851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414841.245504}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] Epoch[38] Batch[0] avg_epoch_loss=16.783106\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=16.7831058502\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] Epoch[38] Batch[5] avg_epoch_loss=16.819964\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=16.8199644089\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] Epoch[38] Batch [5]#011Speed: 941.55 samples/sec#011loss=16.819964\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] processed a total of 574 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1000.8749961853027, \"sum\": 1000.8749961853027, \"min\": 1000.8749961853027}}, \"EndTime\": 1580414842.281857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414841.280917}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=573.423063915 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=38, train loss <loss>=16.7907833523\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_22307a26-1860-4f49-abb9-4df994fb27cb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.711992263793945, \"sum\": 22.711992263793945, \"min\": 22.711992263793945}}, \"EndTime\": 1580414842.305201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414842.281947}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] Epoch[39] Batch[0] avg_epoch_loss=16.911505\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=16.9115047455\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] Epoch[39] Batch[5] avg_epoch_loss=16.833694\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=16.8336935043\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] Epoch[39] Batch [5]#011Speed: 930.38 samples/sec#011loss=16.833694\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.80202293396, \"sum\": 1043.80202293396, \"min\": 1043.80202293396}}, \"EndTime\": 1580414843.349129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414842.305262}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=579.530155482 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=39, train loss <loss>=16.8581655502\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] Epoch[40] Batch[0] avg_epoch_loss=16.868177\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=16.8681774139\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] Epoch[40] Batch[5] avg_epoch_loss=16.803846\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=16.8038463593\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] Epoch[40] Batch [5]#011Speed: 944.89 samples/sec#011loss=16.803846\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.4339809417725, \"sum\": 1088.4339809417725, \"min\": 1088.4339809417725}}, \"EndTime\": 1580414844.438144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414843.349236}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=542.917012227 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=40, train loss <loss>=16.7896997452\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_09e49db8-1b0a-4e87-b480-a5bad653c5cc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.81191062927246, \"sum\": 25.81191062927246, \"min\": 25.81191062927246}}, \"EndTime\": 1580414844.464554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414844.438232}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] Epoch[41] Batch[0] avg_epoch_loss=16.718670\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=16.7186698914\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] Epoch[41] Batch[5] avg_epoch_loss=16.818014\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=16.8180141449\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] Epoch[41] Batch [5]#011Speed: 938.86 samples/sec#011loss=16.818014\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 991.2521839141846, \"sum\": 991.2521839141846, \"min\": 991.2521839141846}}, \"EndTime\": 1580414845.455937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414844.464615}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=568.901639809 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=41, train loss <loss>=16.7950367398\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] Epoch[42] Batch[0] avg_epoch_loss=16.858284\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=16.8582839966\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] Epoch[42] Batch[5] avg_epoch_loss=16.758217\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=16.7582174937\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] Epoch[42] Batch [5]#011Speed: 919.84 samples/sec#011loss=16.758217\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] processed a total of 547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1014.2941474914551, \"sum\": 1014.2941474914551, \"min\": 1014.2941474914551}}, \"EndTime\": 1580414846.47084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414845.456024}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=539.222212638 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=42, train loss <loss>=16.7587576972\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_8e42780a-5045-4b48-ac5d-66cb58f0787c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.33814239501953, \"sum\": 32.33814239501953, \"min\": 32.33814239501953}}, \"EndTime\": 1580414846.503785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414846.470928}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] Epoch[43] Batch[0] avg_epoch_loss=16.733444\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=16.7334442139\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] Epoch[43] Batch[5] avg_epoch_loss=16.811620\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=16.8116203944\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] Epoch[43] Batch [5]#011Speed: 884.30 samples/sec#011loss=16.811620\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1087.4500274658203, \"sum\": 1087.4500274658203, \"min\": 1087.4500274658203}}, \"EndTime\": 1580414847.591365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414846.503851}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=534.214997318 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] #quality_metric: host=algo-1, epoch=43, train loss <loss>=16.7908124924\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:27 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] Epoch[44] Batch[0] avg_epoch_loss=16.862957\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=16.8629570007\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] Epoch[44] Batch[5] avg_epoch_loss=16.876547\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=16.8765468597\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] Epoch[44] Batch [5]#011Speed: 947.28 samples/sec#011loss=16.876547\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.0611038208008, \"sum\": 1078.0611038208008, \"min\": 1078.0611038208008}}, \"EndTime\": 1580414848.669982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414847.591452}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=545.358983091 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=44, train loss <loss>=16.8438142776\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:28 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] Epoch[45] Batch[0] avg_epoch_loss=16.664085\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=16.6640853882\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] Epoch[45] Batch[5] avg_epoch_loss=16.771352\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=16.7713518143\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] Epoch[45] Batch [5]#011Speed: 931.91 samples/sec#011loss=16.771352\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] processed a total of 521 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 997.0479011535645, \"sum\": 997.0479011535645, \"min\": 997.0479011535645}}, \"EndTime\": 1580414849.667566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414848.670069}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=522.475006713 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=45, train loss <loss>=16.7835617065\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:29 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] Epoch[46] Batch[0] avg_epoch_loss=16.839718\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=16.839717865\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] Epoch[46] Batch[5] avg_epoch_loss=16.861067\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=16.8610665003\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] Epoch[46] Batch [5]#011Speed: 937.40 samples/sec#011loss=16.861067\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.7561359405518, \"sum\": 1084.7561359405518, \"min\": 1084.7561359405518}}, \"EndTime\": 1580414850.752882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414849.667654}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=568.723310907 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=46, train loss <loss>=16.865369606\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:30 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] Epoch[47] Batch[0] avg_epoch_loss=16.816036\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=16.8160362244\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] Epoch[47] Batch[5] avg_epoch_loss=16.795008\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=16.7950077057\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] Epoch[47] Batch [5]#011Speed: 919.92 samples/sec#011loss=16.795008\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1002.1271705627441, \"sum\": 1002.1271705627441, \"min\": 1002.1271705627441}}, \"EndTime\": 1580414851.755599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414850.752971}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=573.705085267 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=47, train loss <loss>=16.8219350179\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:31 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] Epoch[48] Batch[0] avg_epoch_loss=16.643940\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=16.6439399719\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] Epoch[48] Batch[5] avg_epoch_loss=16.788207\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=16.7882070541\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] Epoch[48] Batch [5]#011Speed: 915.49 samples/sec#011loss=16.788207\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] processed a total of 571 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.0021781921387, \"sum\": 1016.0021781921387, \"min\": 1016.0021781921387}}, \"EndTime\": 1580414852.77216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414851.755688}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=561.935461649 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=48, train loss <loss>=16.783294042\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:32 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] Epoch[49] Batch[0] avg_epoch_loss=16.807896\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=16.8078956604\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] Epoch[49] Batch[5] avg_epoch_loss=16.789925\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=16.7899246216\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] Epoch[49] Batch [5]#011Speed: 901.74 samples/sec#011loss=16.789925\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1020.0099945068359, \"sum\": 1020.0099945068359, \"min\": 1020.0099945068359}}, \"EndTime\": 1580414853.792731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414852.772248}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=560.708439273 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] #quality_metric: host=algo-1, epoch=49, train loss <loss>=16.793062422\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:33 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] Epoch[50] Batch[0] avg_epoch_loss=16.735903\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=16.7359027863\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] Epoch[50] Batch[5] avg_epoch_loss=16.788638\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=16.7886384328\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] Epoch[50] Batch [5]#011Speed: 933.32 samples/sec#011loss=16.788638\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1071.0830688476562, \"sum\": 1071.0830688476562, \"min\": 1071.0830688476562}}, \"EndTime\": 1580414854.864404, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414853.792817}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=585.317762124 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=50, train loss <loss>=16.7941036224\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:34 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] Epoch[51] Batch[0] avg_epoch_loss=16.708763\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=16.7087631226\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] Epoch[51] Batch[5] avg_epoch_loss=16.802410\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=16.8024104436\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] Epoch[51] Batch [5]#011Speed: 952.46 samples/sec#011loss=16.802410\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 990.2219772338867, \"sum\": 990.2219772338867, \"min\": 990.2219772338867}}, \"EndTime\": 1580414855.855203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414854.864493}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=569.493297537 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=51, train loss <loss>=16.7666903602\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:35 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] Epoch[52] Batch[0] avg_epoch_loss=16.665937\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=16.6659374237\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] Epoch[52] Batch[5] avg_epoch_loss=16.805279\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=16.805279096\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] Epoch[52] Batch [5]#011Speed: 887.57 samples/sec#011loss=16.805279\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1119.4279193878174, \"sum\": 1119.4279193878174, \"min\": 1119.4279193878174}}, \"EndTime\": 1580414856.975185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414855.855292}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=517.167897668 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=52, train loss <loss>=16.7978899002\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:36 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:37 INFO 139704039475008] Epoch[53] Batch[0] avg_epoch_loss=16.772184\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:37 INFO 139704039475008] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=16.7721843719\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:37 INFO 139704039475008] Epoch[53] Batch[5] avg_epoch_loss=16.775741\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:37 INFO 139704039475008] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=16.7757409414\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:37 INFO 139704039475008] Epoch[53] Batch [5]#011Speed: 925.54 samples/sec#011loss=16.775741\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.0379467010498, \"sum\": 1085.0379467010498, \"min\": 1085.0379467010498}}, \"EndTime\": 1580414858.060802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414856.975274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=550.145168487 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=53, train loss <loss>=16.7990026474\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] Epoch[54] Batch[0] avg_epoch_loss=16.827013\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=16.8270130157\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] Epoch[54] Batch[5] avg_epoch_loss=16.816210\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=16.8162097931\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:38 INFO 139704039475008] Epoch[54] Batch [5]#011Speed: 899.35 samples/sec#011loss=16.816210\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] processed a total of 578 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.2821502685547, \"sum\": 1094.2821502685547, \"min\": 1094.2821502685547}}, \"EndTime\": 1580414859.155663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414858.06089}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=528.136520776 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=54, train loss <loss>=16.8039714813\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] Epoch[55] Batch[0] avg_epoch_loss=16.796076\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=16.7960758209\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] Epoch[55] Batch[5] avg_epoch_loss=16.809350\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=16.8093500137\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:39 INFO 139704039475008] Epoch[55] Batch [5]#011Speed: 946.97 samples/sec#011loss=16.809350\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1061.697006225586, \"sum\": 1061.697006225586, \"min\": 1061.697006225586}}, \"EndTime\": 1580414860.217932, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414859.155751}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=546.231925841 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=55, train loss <loss>=16.7688028336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] Epoch[56] Batch[0] avg_epoch_loss=16.825567\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=16.8255672455\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] Epoch[56] Batch[5] avg_epoch_loss=16.819083\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=16.8190832138\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] Epoch[56] Batch [5]#011Speed: 935.58 samples/sec#011loss=16.819083\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1061.5878105163574, \"sum\": 1061.5878105163574, \"min\": 1061.5878105163574}}, \"EndTime\": 1580414861.280086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414860.218012}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=553.818263231 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=56, train loss <loss>=16.8053491592\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] Epoch[57] Batch[0] avg_epoch_loss=16.840279\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=16.8402786255\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] Epoch[57] Batch[5] avg_epoch_loss=16.765251\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=16.7652514776\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] Epoch[57] Batch [5]#011Speed: 923.29 samples/sec#011loss=16.765251\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 997.3540306091309, \"sum\": 997.3540306091309, \"min\": 997.3540306091309}}, \"EndTime\": 1580414862.278019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414861.280176}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=577.453579639 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=57, train loss <loss>=16.7694820828\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] Epoch[58] Batch[0] avg_epoch_loss=16.762943\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=16.7629432678\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] Epoch[58] Batch[5] avg_epoch_loss=16.758776\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=16.7587763468\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] Epoch[58] Batch [5]#011Speed: 889.83 samples/sec#011loss=16.758776\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] processed a total of 578 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.00785446167, \"sum\": 1089.00785446167, \"min\": 1089.00785446167}}, \"EndTime\": 1580414863.367587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414862.278106}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=530.694901422 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=58, train loss <loss>=16.7688898087\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] Epoch[59] Batch[0] avg_epoch_loss=16.851326\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=16.8513259888\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] Epoch[59] Batch[5] avg_epoch_loss=16.810746\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=16.810745875\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] Epoch[59] Batch [5]#011Speed: 940.47 samples/sec#011loss=16.810746\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] processed a total of 569 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1015.7909393310547, \"sum\": 1015.7909393310547, \"min\": 1015.7909393310547}}, \"EndTime\": 1580414864.383929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414863.367674}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=560.082987392 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=59, train loss <loss>=16.7944691976\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] Epoch[60] Batch[0] avg_epoch_loss=16.749989\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=16.7499885559\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] Epoch[60] Batch[5] avg_epoch_loss=16.721336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=16.721335729\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] Epoch[60] Batch [5]#011Speed: 927.79 samples/sec#011loss=16.721336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1082.1659564971924, \"sum\": 1082.1659564971924, \"min\": 1082.1659564971924}}, \"EndTime\": 1580414865.466667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414864.384017}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=578.400432649 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=60, train loss <loss>=16.7548130035\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_1155d629-8e3b-4514-8447-16e400985637-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.68485450744629, \"sum\": 28.68485450744629, \"min\": 28.68485450744629}}, \"EndTime\": 1580414865.495955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414865.466755}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] Epoch[61] Batch[0] avg_epoch_loss=16.729898\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=16.7298984528\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] Epoch[61] Batch[5] avg_epoch_loss=16.774529\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=16.7745291392\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] Epoch[61] Batch [5]#011Speed: 937.29 samples/sec#011loss=16.774529\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.0720596313477, \"sum\": 1065.0720596313477, \"min\": 1065.0720596313477}}, \"EndTime\": 1580414866.561187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414865.49603}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=582.977577861 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=61, train loss <loss>=16.7916959763\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:46 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] Epoch[62] Batch[0] avg_epoch_loss=16.815857\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=16.8158569336\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] Epoch[62] Batch[5] avg_epoch_loss=16.808638\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=16.8086382548\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] Epoch[62] Batch [5]#011Speed: 901.67 samples/sec#011loss=16.808638\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1071.4619159698486, \"sum\": 1071.4619159698486, \"min\": 1071.4619159698486}}, \"EndTime\": 1580414867.633213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414866.561277}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=572.977561498 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=62, train loss <loss>=16.8199800491\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:47 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] Epoch[63] Batch[0] avg_epoch_loss=16.830460\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=16.8304595947\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] Epoch[63] Batch[5] avg_epoch_loss=16.779577\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=16.7795769374\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] Epoch[63] Batch [5]#011Speed: 932.35 samples/sec#011loss=16.779577\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1018.5790061950684, \"sum\": 1018.5790061950684, \"min\": 1018.5790061950684}}, \"EndTime\": 1580414868.652351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414867.633303}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=561.495020917 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=63, train loss <loss>=16.7656241523\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:48 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] Epoch[64] Batch[0] avg_epoch_loss=16.797918\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=16.7979183197\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] Epoch[64] Batch[5] avg_epoch_loss=16.808852\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=16.80885156\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] Epoch[64] Batch [5]#011Speed: 942.35 samples/sec#011loss=16.808852\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.2981433868408, \"sum\": 1074.2981433868408, \"min\": 1074.2981433868408}}, \"EndTime\": 1580414869.727197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414868.65244}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.544989235 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=64, train loss <loss>=16.8316865921\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:49 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] Epoch[65] Batch[0] avg_epoch_loss=16.808018\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=16.8080177307\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] Epoch[65] Batch[5] avg_epoch_loss=16.817872\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=16.8178720474\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] Epoch[65] Batch [5]#011Speed: 942.72 samples/sec#011loss=16.817872\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.370994567871, \"sum\": 1059.370994567871, \"min\": 1059.370994567871}}, \"EndTime\": 1580414870.787152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414869.727287}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=554.977275924 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=65, train loss <loss>=16.7971639633\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:50 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] Epoch[66] Batch[0] avg_epoch_loss=16.777905\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=16.7779045105\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] Epoch[66] Batch[5] avg_epoch_loss=16.768253\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=16.7682526906\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] Epoch[66] Batch [5]#011Speed: 862.79 samples/sec#011loss=16.768253\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1012.1679306030273, \"sum\": 1012.1679306030273, \"min\": 1012.1679306030273}}, \"EndTime\": 1580414871.799882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414870.78724}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=568.013426619 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=66, train loss <loss>=16.7793087429\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:51 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] Epoch[67] Batch[0] avg_epoch_loss=16.795595\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=16.7955951691\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] Epoch[67] Batch[5] avg_epoch_loss=16.743889\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=16.7438894908\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] Epoch[67] Batch [5]#011Speed: 866.74 samples/sec#011loss=16.743889\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] processed a total of 583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.3999919891357, \"sum\": 1103.3999919891357, \"min\": 1103.3999919891357}}, \"EndTime\": 1580414872.903847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414871.799973}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=528.303628632 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=67, train loss <loss>=16.7293231964\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:52 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_817c1b2c-45d8-4c05-ad5b-522d80b9c69b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.76283836364746, \"sum\": 31.76283836364746, \"min\": 31.76283836364746}}, \"EndTime\": 1580414872.936222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414872.903936}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:53 INFO 139704039475008] Epoch[68] Batch[0] avg_epoch_loss=16.825823\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=16.8258228302\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:53 INFO 139704039475008] Epoch[68] Batch[5] avg_epoch_loss=16.773489\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=16.7734889984\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:53 INFO 139704039475008] Epoch[68] Batch [5]#011Speed: 887.87 samples/sec#011loss=16.773489\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1116.1868572235107, \"sum\": 1116.1868572235107, \"min\": 1116.1868572235107}}, \"EndTime\": 1580414874.052534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414872.936282}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=546.440799596 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=68, train loss <loss>=16.7324792862\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] Epoch[69] Batch[0] avg_epoch_loss=16.876602\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=16.8766021729\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] Epoch[69] Batch[5] avg_epoch_loss=16.811969\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=16.8119691213\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:54 INFO 139704039475008] Epoch[69] Batch [5]#011Speed: 909.64 samples/sec#011loss=16.811969\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.2101154327393, \"sum\": 1078.2101154327393, \"min\": 1078.2101154327393}}, \"EndTime\": 1580414875.131306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414874.052622}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=549.891329782 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=69, train loss <loss>=16.8173072815\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] Epoch[70] Batch[0] avg_epoch_loss=16.793756\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=16.793756485\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] Epoch[70] Batch[5] avg_epoch_loss=16.742554\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=16.7425543467\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:55 INFO 139704039475008] Epoch[70] Batch [5]#011Speed: 942.32 samples/sec#011loss=16.742554\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.1860370635986, \"sum\": 1101.1860370635986, \"min\": 1101.1860370635986}}, \"EndTime\": 1580414876.23312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414875.131396}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=532.985677042 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=70, train loss <loss>=16.7257944107\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_4afbb3c5-9948-4441-9004-6aad1462d26f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.67599105834961, \"sum\": 22.67599105834961, \"min\": 22.67599105834961}}, \"EndTime\": 1580414876.256489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414876.233235}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] Epoch[71] Batch[0] avg_epoch_loss=16.500731\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=16.5007305145\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] Epoch[71] Batch[5] avg_epoch_loss=16.720476\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=16.7204755147\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] Epoch[71] Batch [5]#011Speed: 930.72 samples/sec#011loss=16.720476\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1068.774938583374, \"sum\": 1068.774938583374, \"min\": 1068.774938583374}}, \"EndTime\": 1580414877.325394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414876.256551}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=577.22719467 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=71, train loss <loss>=16.7574090958\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] Epoch[72] Batch[0] avg_epoch_loss=16.799879\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=16.7998790741\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] Epoch[72] Batch[5] avg_epoch_loss=16.766547\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=16.7665468852\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] Epoch[72] Batch [5]#011Speed: 932.47 samples/sec#011loss=16.766547\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1000.5791187286377, \"sum\": 1000.5791187286377, \"min\": 1000.5791187286377}}, \"EndTime\": 1580414878.32653, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414877.325481}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=563.600375291 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=72, train loss <loss>=16.7812847561\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] Epoch[73] Batch[0] avg_epoch_loss=16.760838\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=16.7608375549\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] Epoch[73] Batch[5] avg_epoch_loss=16.721189\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=16.7211894989\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] Epoch[73] Batch [5]#011Speed: 822.32 samples/sec#011loss=16.721189\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] processed a total of 565 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.6649856567383, \"sum\": 1062.6649856567383, \"min\": 1062.6649856567383}}, \"EndTime\": 1580414879.389754, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414878.326618}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=531.624059169 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=73, train loss <loss>=16.7082956102\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_c12e6712-fa23-4f5c-b93d-63daa47a69cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.951053619384766, \"sum\": 23.951053619384766, \"min\": 23.951053619384766}}, \"EndTime\": 1580414879.414346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414879.389836}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] Epoch[74] Batch[0] avg_epoch_loss=16.622726\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:07:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=16.6227264404\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] Epoch[74] Batch[5] avg_epoch_loss=16.743273\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=16.7432730993\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] Epoch[74] Batch [5]#011Speed: 919.47 samples/sec#011loss=16.743273\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 991.4000034332275, \"sum\": 991.4000034332275, \"min\": 991.4000034332275}}, \"EndTime\": 1580414880.405913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414879.414429}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=580.91805535 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=74, train loss <loss>=16.7217782338\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] Epoch[75] Batch[0] avg_epoch_loss=16.783369\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=16.7833690643\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] Epoch[75] Batch[5] avg_epoch_loss=16.779208\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=16.7792075475\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] Epoch[75] Batch [5]#011Speed: 915.16 samples/sec#011loss=16.779208\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.0039253234863, \"sum\": 1084.0039253234863, \"min\": 1084.0039253234863}}, \"EndTime\": 1580414881.490516, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414880.406001}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.281762238 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=75, train loss <loss>=16.742300415\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] Epoch[76] Batch[0] avg_epoch_loss=16.821056\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=16.821056366\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] Epoch[76] Batch[5] avg_epoch_loss=16.756882\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=16.7568820318\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] Epoch[76] Batch [5]#011Speed: 880.06 samples/sec#011loss=16.756882\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1097.2309112548828, \"sum\": 1097.2309112548828, \"min\": 1097.2309112548828}}, \"EndTime\": 1580414882.588422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414881.490616}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=546.765293392 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=76, train loss <loss>=16.7507997513\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:02 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] Epoch[77] Batch[0] avg_epoch_loss=16.770103\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=16.7701034546\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] Epoch[77] Batch[5] avg_epoch_loss=16.757051\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=16.7570505142\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] Epoch[77] Batch [5]#011Speed: 927.26 samples/sec#011loss=16.757051\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1064.655065536499, \"sum\": 1064.655065536499, \"min\": 1064.655065536499}}, \"EndTime\": 1580414883.653643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414882.588511}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=591.667678749 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=77, train loss <loss>=16.7362388611\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:03 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] Epoch[78] Batch[0] avg_epoch_loss=16.709911\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=16.7099113464\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] Epoch[78] Batch[5] avg_epoch_loss=16.753349\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=16.7533486684\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] Epoch[78] Batch [5]#011Speed: 924.86 samples/sec#011loss=16.753349\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1044.8410511016846, \"sum\": 1044.8410511016846, \"min\": 1044.8410511016846}}, \"EndTime\": 1580414884.699086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414883.653732}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=556.953769198 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=78, train loss <loss>=16.7232376099\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:04 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] Epoch[79] Batch[0] avg_epoch_loss=16.758261\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=16.7582607269\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] Epoch[79] Batch[5] avg_epoch_loss=16.738823\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=16.7388232549\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] Epoch[79] Batch [5]#011Speed: 914.99 samples/sec#011loss=16.738823\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.2519931793213, \"sum\": 1066.2519931793213, \"min\": 1066.2519931793213}}, \"EndTime\": 1580414885.765923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414884.699174}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=583.288916434 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=79, train loss <loss>=16.7471118927\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:05 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] Epoch[80] Batch[0] avg_epoch_loss=16.717068\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=16.7170677185\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] Epoch[80] Batch[5] avg_epoch_loss=16.772078\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=16.7720781962\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] Epoch[80] Batch [5]#011Speed: 874.11 samples/sec#011loss=16.772078\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1086.230993270874, \"sum\": 1086.230993270874, \"min\": 1086.230993270874}}, \"EndTime\": 1580414886.852742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414885.765994}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=565.189071603 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=80, train loss <loss>=16.7748222351\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:06 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] Epoch[81] Batch[0] avg_epoch_loss=16.772430\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=16.7724304199\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] Epoch[81] Batch[5] avg_epoch_loss=16.784999\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=16.7849992116\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] Epoch[81] Batch [5]#011Speed: 929.95 samples/sec#011loss=16.784999\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] processed a total of 566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 991.9788837432861, \"sum\": 991.9788837432861, \"min\": 991.9788837432861}}, \"EndTime\": 1580414887.845308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414886.85283}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=570.501795272 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=81, train loss <loss>=16.7943903605\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:07 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] Epoch[82] Batch[0] avg_epoch_loss=16.780285\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=16.7802848816\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] Epoch[82] Batch[5] avg_epoch_loss=16.751813\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=16.7518129349\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] Epoch[82] Batch [5]#011Speed: 947.07 samples/sec#011loss=16.751813\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.0238456726074, \"sum\": 1046.0238456726074, \"min\": 1046.0238456726074}}, \"EndTime\": 1580414888.89191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414887.845396}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=579.265136453 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=82, train loss <loss>=16.7358943939\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:08 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] Epoch[83] Batch[0] avg_epoch_loss=16.664045\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=16.6640453339\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] Epoch[83] Batch[5] avg_epoch_loss=16.753989\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=16.7539889018\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] Epoch[83] Batch [5]#011Speed: 930.43 samples/sec#011loss=16.753989\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.847032546997, \"sum\": 1047.847032546997, \"min\": 1047.847032546997}}, \"EndTime\": 1580414889.94033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414888.891997}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=590.679304109 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=83, train loss <loss>=16.7514005661\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:09 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] Epoch[84] Batch[0] avg_epoch_loss=16.759563\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=16.759563446\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] Epoch[84] Batch[5] avg_epoch_loss=16.766222\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=16.7662220001\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] Epoch[84] Batch [5]#011Speed: 938.13 samples/sec#011loss=16.766222\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 967.4179553985596, \"sum\": 967.4179553985596, \"min\": 967.4179553985596}}, \"EndTime\": 1580414890.908272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414889.940397}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=592.229690879 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=84, train loss <loss>=16.7535103692\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:10 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] Epoch[85] Batch[0] avg_epoch_loss=16.735523\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=16.7355232239\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] Epoch[85] Batch[5] avg_epoch_loss=16.742963\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=16.7429631551\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] Epoch[85] Batch [5]#011Speed: 857.58 samples/sec#011loss=16.742963\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 998.6088275909424, \"sum\": 998.6088275909424, \"min\": 998.6088275909424}}, \"EndTime\": 1580414891.907426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414890.908346}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=564.713178087 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=85, train loss <loss>=16.748099645\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:11 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] Epoch[86] Batch[0] avg_epoch_loss=16.756353\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=16.7563533783\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] Epoch[86] Batch[5] avg_epoch_loss=16.746708\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=16.7467075984\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] Epoch[86] Batch [5]#011Speed: 938.57 samples/sec#011loss=16.746708\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1086.2629413604736, \"sum\": 1086.2629413604736, \"min\": 1086.2629413604736}}, \"EndTime\": 1580414892.994229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414891.907513}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=534.796850153 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=86, train loss <loss>=16.7450752258\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:12 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] Epoch[87] Batch[0] avg_epoch_loss=16.788422\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=16.7884216309\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] Epoch[87] Batch[5] avg_epoch_loss=16.789955\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=16.7899545034\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] Epoch[87] Batch [5]#011Speed: 949.07 samples/sec#011loss=16.789955\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] processed a total of 566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 978.4419536590576, \"sum\": 978.4419536590576, \"min\": 978.4419536590576}}, \"EndTime\": 1580414893.973219, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414892.994318}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=578.392618525 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=87, train loss <loss>=16.790945265\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:13 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:14 INFO 139704039475008] Epoch[88] Batch[0] avg_epoch_loss=16.799259\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=16.7992591858\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:14 INFO 139704039475008] Epoch[88] Batch[5] avg_epoch_loss=16.728719\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=16.7287193934\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:14 INFO 139704039475008] Epoch[88] Batch [5]#011Speed: 916.75 samples/sec#011loss=16.728719\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1072.188138961792, \"sum\": 1072.188138961792, \"min\": 1072.188138961792}}, \"EndTime\": 1580414895.045965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414893.973308}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=552.074997754 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=88, train loss <loss>=16.7672374725\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] Epoch[89] Batch[0] avg_epoch_loss=16.863960\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=16.8639602661\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] Epoch[89] Batch[5] avg_epoch_loss=16.809755\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=16.8097553253\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:15 INFO 139704039475008] Epoch[89] Batch [5]#011Speed: 933.30 samples/sec#011loss=16.809755\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1069.8959827423096, \"sum\": 1069.8959827423096, \"min\": 1069.8959827423096}}, \"EndTime\": 1580414896.116442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414895.046053}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=547.651260716 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=89, train loss <loss>=16.8072319031\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] Epoch[90] Batch[0] avg_epoch_loss=16.792288\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=16.7922878265\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] Epoch[90] Batch[5] avg_epoch_loss=16.811477\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=16.8114767075\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:16 INFO 139704039475008] Epoch[90] Batch [5]#011Speed: 859.32 samples/sec#011loss=16.811477\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] processed a total of 570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1022.4580764770508, \"sum\": 1022.4580764770508, \"min\": 1022.4580764770508}}, \"EndTime\": 1580414897.139471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414896.116528}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=557.410271816 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=90, train loss <loss>=16.8195819855\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] Epoch[91] Batch[0] avg_epoch_loss=16.753063\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=16.7530632019\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] Epoch[91] Batch[5] avg_epoch_loss=16.788907\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=16.7889070511\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:17 INFO 139704039475008] Epoch[91] Batch [5]#011Speed: 944.71 samples/sec#011loss=16.788907\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1060.554027557373, \"sum\": 1060.554027557373, \"min\": 1060.554027557373}}, \"EndTime\": 1580414898.200593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414897.139556}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=562.85651719 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=91, train loss <loss>=16.7633710861\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] Epoch[92] Batch[0] avg_epoch_loss=16.746914\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=16.7469139099\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] Epoch[92] Batch[5] avg_epoch_loss=16.769039\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=16.7690388362\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:18 INFO 139704039475008] Epoch[92] Batch [5]#011Speed: 934.22 samples/sec#011loss=16.769039\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 977.9419898986816, \"sum\": 977.9419898986816, \"min\": 977.9419898986816}}, \"EndTime\": 1580414899.179124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414898.200658}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=585.844627364 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=92, train loss <loss>=16.7467717065\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] Epoch[93] Batch[0] avg_epoch_loss=16.775164\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=16.7751636505\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] Epoch[93] Batch[5] avg_epoch_loss=16.731834\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=16.7318340937\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:19 INFO 139704039475008] Epoch[93] Batch [5]#011Speed: 925.25 samples/sec#011loss=16.731834\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] processed a total of 561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1013.4479999542236, \"sum\": 1013.4479999542236, \"min\": 1013.4479999542236}}, \"EndTime\": 1580414900.193159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414899.179213}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=553.470105968 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=93, train loss <loss>=16.7294088999\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] Epoch[94] Batch[0] avg_epoch_loss=16.796637\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=16.7966365814\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] Epoch[94] Batch[5] avg_epoch_loss=16.740843\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=16.7408428192\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:20 INFO 139704039475008] Epoch[94] Batch [5]#011Speed: 917.51 samples/sec#011loss=16.740843\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] processed a total of 562 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 983.4480285644531, \"sum\": 983.4480285644531, \"min\": 983.4480285644531}}, \"EndTime\": 1580414901.177234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414900.193273}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=571.378704619 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=94, train loss <loss>=16.7554908329\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] Epoch[95] Batch[0] avg_epoch_loss=16.763823\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=16.7638225555\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] Epoch[95] Batch[5] avg_epoch_loss=16.726432\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=16.7264315287\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:21 INFO 139704039475008] Epoch[95] Batch [5]#011Speed: 918.99 samples/sec#011loss=16.726432\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.0489234924316, \"sum\": 1075.0489234924316, \"min\": 1075.0489234924316}}, \"EndTime\": 1580414902.25286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414901.177325}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=571.997852594 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=95, train loss <loss>=16.7082593918\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_82e005e3-89c7-4266-b9b0-ae10c9b0f585-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.31694412231445, \"sum\": 35.31694412231445, \"min\": 35.31694412231445}}, \"EndTime\": 1580414902.288847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414902.252949}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] Epoch[96] Batch[0] avg_epoch_loss=16.650951\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=16.6509513855\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] Epoch[96] Batch[5] avg_epoch_loss=16.726614\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=16.7266139984\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] Epoch[96] Batch [5]#011Speed: 934.92 samples/sec#011loss=16.726614\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1031.1081409454346, \"sum\": 1031.1081409454346, \"min\": 1031.1081409454346}}, \"EndTime\": 1580414903.320101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414902.288923}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=546.915918861 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=96, train loss <loss>=16.7231784397\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] Epoch[97] Batch[0] avg_epoch_loss=16.770006\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=16.7700061798\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] Epoch[97] Batch[5] avg_epoch_loss=16.738005\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=16.7380053202\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] Epoch[97] Batch [5]#011Speed: 914.09 samples/sec#011loss=16.738005\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] processed a total of 547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 999.4950294494629, \"sum\": 999.4950294494629, \"min\": 999.4950294494629}}, \"EndTime\": 1580414904.320181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414903.32019}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=547.2052197 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=97, train loss <loss>=16.6993181441\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_870a1c70-d623-4288-b2b6-bdf6369466f6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.684978485107422, \"sum\": 23.684978485107422, \"min\": 23.684978485107422}}, \"EndTime\": 1580414904.344496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414904.320269}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] Epoch[98] Batch[0] avg_epoch_loss=16.627491\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=16.6274909973\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] Epoch[98] Batch[5] avg_epoch_loss=16.728856\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=16.7288564046\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] Epoch[98] Batch [5]#011Speed: 940.28 samples/sec#011loss=16.728856\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] processed a total of 557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 971.491813659668, \"sum\": 971.491813659668, \"min\": 971.491813659668}}, \"EndTime\": 1580414905.316132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414904.344575}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=573.287061267 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=98, train loss <loss>=16.713865704\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] Epoch[99] Batch[0] avg_epoch_loss=16.777229\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=16.7772293091\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] Epoch[99] Batch[5] avg_epoch_loss=16.771807\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=16.7718070348\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] Epoch[99] Batch [5]#011Speed: 943.96 samples/sec#011loss=16.771807\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] processed a total of 541 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 958.78005027771, \"sum\": 958.78005027771, \"min\": 958.78005027771}}, \"EndTime\": 1580414906.275429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414905.316195}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=564.181694317 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=99, train loss <loss>=16.8077935113\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] Epoch[100] Batch[0] avg_epoch_loss=16.809702\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=16.8097019196\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] Epoch[100] Batch[5] avg_epoch_loss=16.746181\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=16.7461808523\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] Epoch[100] Batch [5]#011Speed: 940.04 samples/sec#011loss=16.746181\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] processed a total of 568 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 986.515998840332, \"sum\": 986.515998840332, \"min\": 986.515998840332}}, \"EndTime\": 1580414907.262511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414906.275519}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=575.704047387 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] #quality_metric: host=algo-1, epoch=100, train loss <loss>=16.740779029\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] Epoch[101] Batch[0] avg_epoch_loss=16.714100\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:27 INFO 139704039475008] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=16.714099884\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] Epoch[101] Batch[5] avg_epoch_loss=16.723264\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=16.7232643763\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] Epoch[101] Batch [5]#011Speed: 931.63 samples/sec#011loss=16.723264\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.9708251953125, \"sum\": 1066.9708251953125, \"min\": 1066.9708251953125}}, \"EndTime\": 1580414908.329983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414907.262576}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=559.458069123 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=101, train loss <loss>=16.7537881851\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] Epoch[102] Batch[0] avg_epoch_loss=16.772469\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:28 INFO 139704039475008] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=16.7724685669\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] Epoch[102] Batch[5] avg_epoch_loss=16.783313\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=16.7833131154\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] Epoch[102] Batch [5]#011Speed: 867.10 samples/sec#011loss=16.783313\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] processed a total of 567 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1072.8490352630615, \"sum\": 1072.8490352630615, \"min\": 1072.8490352630615}}, \"EndTime\": 1580414909.403403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414908.330072}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=528.437328112 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=102, train loss <loss>=16.7523142497\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] Epoch[103] Batch[0] avg_epoch_loss=16.778093\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:29 INFO 139704039475008] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=16.778093338\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] Epoch[103] Batch[5] avg_epoch_loss=16.796767\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=16.7967669169\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] Epoch[103] Batch [5]#011Speed: 919.42 samples/sec#011loss=16.796767\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.024024963379, \"sum\": 1083.024024963379, \"min\": 1083.024024963379}}, \"EndTime\": 1580414910.486972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414909.403489}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=541.934809951 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=103, train loss <loss>=16.7870670319\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] Epoch[104] Batch[0] avg_epoch_loss=16.774395\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:30 INFO 139704039475008] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=16.774394989\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] Epoch[104] Batch[5] avg_epoch_loss=16.701531\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=16.7015314102\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] Epoch[104] Batch [5]#011Speed: 932.16 samples/sec#011loss=16.701531\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] processed a total of 557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1023.8170623779297, \"sum\": 1023.8170623779297, \"min\": 1023.8170623779297}}, \"EndTime\": 1580414911.511339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414910.487063}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.975113697 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=104, train loss <loss>=16.6910175747\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_e4857003-aa89-482a-8592-28ae133c5c6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.724929809570312, \"sum\": 31.724929809570312, \"min\": 31.724929809570312}}, \"EndTime\": 1580414911.543651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414911.511426}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] Epoch[105] Batch[0] avg_epoch_loss=16.664028\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:31 INFO 139704039475008] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=16.6640281677\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] Epoch[105] Batch[5] avg_epoch_loss=16.723006\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=16.7230056127\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] Epoch[105] Batch [5]#011Speed: 930.81 samples/sec#011loss=16.723006\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 996.2828159332275, \"sum\": 996.2828159332275, \"min\": 996.2828159332275}}, \"EndTime\": 1580414912.54007, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414911.543723}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=578.074799067 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=105, train loss <loss>=16.7359095679\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] Epoch[106] Batch[0] avg_epoch_loss=16.814026\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:32 INFO 139704039475008] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=16.8140258789\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] Epoch[106] Batch[5] avg_epoch_loss=16.681739\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=16.6817394892\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] Epoch[106] Batch [5]#011Speed: 858.91 samples/sec#011loss=16.681739\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1112.4320030212402, \"sum\": 1112.4320030212402, \"min\": 1112.4320030212402}}, \"EndTime\": 1580414913.65307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414912.540158}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=531.208314064 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] #quality_metric: host=algo-1, epoch=106, train loss <loss>=16.7094926834\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:33 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] Epoch[107] Batch[0] avg_epoch_loss=16.678148\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=16.6781482697\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] Epoch[107] Batch[5] avg_epoch_loss=16.751229\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=16.7512286504\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] Epoch[107] Batch [5]#011Speed: 909.06 samples/sec#011loss=16.751229\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1133.331060409546, \"sum\": 1133.331060409546, \"min\": 1133.331060409546}}, \"EndTime\": 1580414914.786978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414913.653156}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=511.708032968 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] #quality_metric: host=algo-1, epoch=107, train loss <loss>=16.7517141342\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:34 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] Epoch[108] Batch[0] avg_epoch_loss=16.790522\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=16.7905216217\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] Epoch[108] Batch[5] avg_epoch_loss=16.747161\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=16.7471605937\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] Epoch[108] Batch [5]#011Speed: 918.90 samples/sec#011loss=16.747161\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.4349327087402, \"sum\": 1065.4349327087402, \"min\": 1065.4349327087402}}, \"EndTime\": 1580414915.852984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414914.787065}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=557.443450765 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] #quality_metric: host=algo-1, epoch=108, train loss <loss>=16.7457941055\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:35 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] Epoch[109] Batch[0] avg_epoch_loss=16.665894\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=16.6658935547\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] Epoch[109] Batch[5] avg_epoch_loss=16.731347\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=16.7313474019\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] Epoch[109] Batch [5]#011Speed: 922.15 samples/sec#011loss=16.731347\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1006.8528652191162, \"sum\": 1006.8528652191162, \"min\": 1006.8528652191162}}, \"EndTime\": 1580414916.860409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414915.853069}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=572.005796014 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] #quality_metric: host=algo-1, epoch=109, train loss <loss>=16.704521815\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:36 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] Epoch[110] Batch[0] avg_epoch_loss=16.870096\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=16.8700962067\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] Epoch[110] Batch[5] avg_epoch_loss=16.785538\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=16.7855383555\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] Epoch[110] Batch [5]#011Speed: 890.37 samples/sec#011loss=16.785538\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] processed a total of 568 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1012.2628211975098, \"sum\": 1012.2628211975098, \"min\": 1012.2628211975098}}, \"EndTime\": 1580414917.87331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414916.860496}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=561.062405694 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] #quality_metric: host=algo-1, epoch=110, train loss <loss>=16.7830032772\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:37 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] Epoch[111] Batch[0] avg_epoch_loss=16.797825\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=16.7978248596\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] Epoch[111] Batch[5] avg_epoch_loss=16.729109\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=16.7291088104\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] Epoch[111] Batch [5]#011Speed: 925.87 samples/sec#011loss=16.729109\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] processed a total of 567 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1002.1250247955322, \"sum\": 1002.1250247955322, \"min\": 1002.1250247955322}}, \"EndTime\": 1580414918.87602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414917.873376}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=565.725523835 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] #quality_metric: host=algo-1, epoch=111, train loss <loss>=16.7287743886\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:38 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] Epoch[112] Batch[0] avg_epoch_loss=16.692450\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=16.6924495697\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] Epoch[112] Batch[5] avg_epoch_loss=16.721292\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=16.7212915421\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] Epoch[112] Batch [5]#011Speed: 922.23 samples/sec#011loss=16.721292\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.9538459777832, \"sum\": 1070.9538459777832, \"min\": 1070.9538459777832}}, \"EndTime\": 1580414919.947541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414918.876107}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=578.860931233 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] #quality_metric: host=algo-1, epoch=112, train loss <loss>=16.7318567276\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:39 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] Epoch[113] Batch[0] avg_epoch_loss=16.780399\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=16.7803993225\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] Epoch[113] Batch[5] avg_epoch_loss=16.738877\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=16.7388772964\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] Epoch[113] Batch [5]#011Speed: 914.22 samples/sec#011loss=16.738877\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 981.1651706695557, \"sum\": 981.1651706695557, \"min\": 981.1651706695557}}, \"EndTime\": 1580414920.929265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414919.947614}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=586.996214525 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] #quality_metric: host=algo-1, epoch=113, train loss <loss>=16.7089576721\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:40 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:41 INFO 139704039475008] Epoch[114] Batch[0] avg_epoch_loss=16.656063\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=16.6560630798\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:41 INFO 139704039475008] Epoch[114] Batch[5] avg_epoch_loss=16.687949\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:41 INFO 139704039475008] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=16.6879485448\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:41 INFO 139704039475008] Epoch[114] Batch [5]#011Speed: 913.17 samples/sec#011loss=16.687949\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.1030654907227, \"sum\": 1078.1030654907227, \"min\": 1078.1030654907227}}, \"EndTime\": 1580414922.007934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414920.929332}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=575.944468669 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=114, train loss <loss>=16.7097394943\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] Epoch[115] Batch[0] avg_epoch_loss=16.752567\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=16.7525672913\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] Epoch[115] Batch[5] avg_epoch_loss=16.744087\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=16.7440872192\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:42 INFO 139704039475008] Epoch[115] Batch [5]#011Speed: 859.95 samples/sec#011loss=16.744087\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1121.2050914764404, \"sum\": 1121.2050914764404, \"min\": 1121.2050914764404}}, \"EndTime\": 1580414923.129749, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414922.00802}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=530.622360162 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=115, train loss <loss>=16.7815444946\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] Epoch[116] Batch[0] avg_epoch_loss=16.778061\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=16.7780609131\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] Epoch[116] Batch[5] avg_epoch_loss=16.714821\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=16.7148208618\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:43 INFO 139704039475008] Epoch[116] Batch [5]#011Speed: 945.61 samples/sec#011loss=16.714821\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] processed a total of 583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.1220626831055, \"sum\": 1033.1220626831055, \"min\": 1033.1220626831055}}, \"EndTime\": 1580414924.16345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414923.129829}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=564.2390005 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=116, train loss <loss>=16.2728413582\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/state_d616ed34-4c64-4258-a5c4-25319d6d73ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.71405601501465, \"sum\": 33.71405601501465, \"min\": 33.71405601501465}}, \"EndTime\": 1580414924.197743, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414924.163536}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] Epoch[117] Batch[0] avg_epoch_loss=16.782358\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=16.7823581696\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] Epoch[117] Batch[5] avg_epoch_loss=16.731069\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=16.731068929\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:44 INFO 139704039475008] Epoch[117] Batch [5]#011Speed: 946.60 samples/sec#011loss=16.731069\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] processed a total of 560 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 965.7080173492432, \"sum\": 965.7080173492432, \"min\": 965.7080173492432}}, \"EndTime\": 1580414925.163583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414924.197805}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=579.8061183 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=117, train loss <loss>=16.7483185662\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] Epoch[118] Batch[0] avg_epoch_loss=16.807255\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=16.8072547913\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] Epoch[118] Batch[5] avg_epoch_loss=16.758147\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=16.7581469218\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:45 INFO 139704039475008] Epoch[118] Batch [5]#011Speed: 938.73 samples/sec#011loss=16.758147\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.704086303711, \"sum\": 1062.704086303711, \"min\": 1062.704086303711}}, \"EndTime\": 1580414926.226871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414925.163673}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=544.777001883 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=118, train loss <loss>=16.7836231232\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] Epoch[119] Batch[0] avg_epoch_loss=16.746756\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:46 INFO 139704039475008] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=16.7467556\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] Epoch[119] Batch[5] avg_epoch_loss=16.644685\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=16.6446847916\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] Epoch[119] Batch [5]#011Speed: 886.68 samples/sec#011loss=16.644685\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1060.654878616333, \"sum\": 1060.654878616333, \"min\": 1060.654878616333}}, \"EndTime\": 1580414927.288093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414926.226951}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=582.587916933 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=119, train loss <loss>=16.6634239197\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] Epoch[120] Batch[0] avg_epoch_loss=16.715364\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:47 INFO 139704039475008] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=16.7153644562\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] Epoch[120] Batch[5] avg_epoch_loss=16.758710\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=16.7587095896\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] Epoch[120] Batch [5]#011Speed: 941.20 samples/sec#011loss=16.758710\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.1440296173096, \"sum\": 1049.1440296173096, \"min\": 1049.1440296173096}}, \"EndTime\": 1580414928.337791, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414927.28818}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=574.691310958 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=120, train loss <loss>=16.7367015839\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] Epoch[121] Batch[0] avg_epoch_loss=16.725658\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:48 INFO 139704039475008] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=16.7256584167\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] Epoch[121] Batch[5] avg_epoch_loss=16.767049\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=16.7670488358\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] Epoch[121] Batch [5]#011Speed: 940.32 samples/sec#011loss=16.767049\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] processed a total of 565 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 966.0050868988037, \"sum\": 966.0050868988037, \"min\": 966.0050868988037}}, \"EndTime\": 1580414929.304343, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414928.337863}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=584.812179033 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=121, train loss <loss>=16.7719080183\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] Epoch[122] Batch[0] avg_epoch_loss=16.796980\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:49 INFO 139704039475008] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=16.7969799042\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] Epoch[122] Batch[5] avg_epoch_loss=16.789014\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=16.7890144984\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] Epoch[122] Batch [5]#011Speed: 942.24 samples/sec#011loss=16.789014\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.1699256896973, \"sum\": 1070.1699256896973, \"min\": 1070.1699256896973}}, \"EndTime\": 1580414930.375098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414929.304424}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=560.598427302 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=122, train loss <loss>=16.7946050644\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] Epoch[123] Batch[0] avg_epoch_loss=16.734827\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:50 INFO 139704039475008] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=16.7348270416\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] Epoch[123] Batch[5] avg_epoch_loss=16.701471\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=16.7014710108\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] Epoch[123] Batch [5]#011Speed: 943.12 samples/sec#011loss=16.701471\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.0290489196777, \"sum\": 1040.0290489196777, \"min\": 1040.0290489196777}}, \"EndTime\": 1580414931.415676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414930.375175}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=569.148198516 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=123, train loss <loss>=16.7420028687\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] Epoch[124] Batch[0] avg_epoch_loss=16.650850\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:51 INFO 139704039475008] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=16.650850296\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] Epoch[124] Batch[5] avg_epoch_loss=16.717382\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=16.7173817952\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] Epoch[124] Batch [5]#011Speed: 927.37 samples/sec#011loss=16.717382\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.9829940795898, \"sum\": 1056.9829940795898, \"min\": 1056.9829940795898}}, \"EndTime\": 1580414932.473255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414931.415759}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=585.559757438 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=124, train loss <loss>=16.7158725739\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] Epoch[125] Batch[0] avg_epoch_loss=16.761438\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:52 INFO 139704039475008] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=16.7614383698\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] Epoch[125] Batch[5] avg_epoch_loss=16.686737\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=16.6867367427\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] Epoch[125] Batch [5]#011Speed: 931.08 samples/sec#011loss=16.686737\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.4139308929443, \"sum\": 1062.4139308929443, \"min\": 1062.4139308929443}}, \"EndTime\": 1580414933.5362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414932.473337}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=592.914434141 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=125, train loss <loss>=16.7067913055\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] Epoch[126] Batch[0] avg_epoch_loss=16.814953\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:53 INFO 139704039475008] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=16.8149528503\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] Epoch[126] Batch[5] avg_epoch_loss=16.768790\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=16.7687902451\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] Epoch[126] Batch [5]#011Speed: 938.69 samples/sec#011loss=16.768790\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.1789836883545, \"sum\": 1047.1789836883545, \"min\": 1047.1789836883545}}, \"EndTime\": 1580414934.583972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414933.536288}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=570.048084841 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] #quality_metric: host=algo-1, epoch=126, train loss <loss>=16.7655752182\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:54 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] Epoch[127] Batch[0] avg_epoch_loss=16.832033\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=16.8320331573\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] Epoch[127] Batch[5] avg_epoch_loss=16.772863\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=16.7728627523\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] Epoch[127] Batch [5]#011Speed: 932.03 samples/sec#011loss=16.772863\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] processed a total of 563 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 971.8449115753174, \"sum\": 971.8449115753174, \"min\": 971.8449115753174}}, \"EndTime\": 1580414935.55637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414934.584037}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=579.234231189 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=127, train loss <loss>=16.7683094872\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] Epoch[128] Batch[0] avg_epoch_loss=16.679310\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:55 INFO 139704039475008] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=16.679309845\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] Epoch[128] Batch[5] avg_epoch_loss=16.750676\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=16.750676473\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] Epoch[128] Batch [5]#011Speed: 938.05 samples/sec#011loss=16.750676\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] processed a total of 535 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 965.5849933624268, \"sum\": 965.5849933624268, \"min\": 965.5849933624268}}, \"EndTime\": 1580414936.522536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414935.556456}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=554.009851905 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=128, train loss <loss>=16.7387563917\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] Epoch[129] Batch[0] avg_epoch_loss=16.811625\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:56 INFO 139704039475008] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=16.811624527\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] Epoch[129] Batch[5] avg_epoch_loss=16.793390\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=16.7933899562\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] Epoch[129] Batch [5]#011Speed: 932.50 samples/sec#011loss=16.793390\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.468002319336, \"sum\": 1089.468002319336, \"min\": 1089.468002319336}}, \"EndTime\": 1580414937.612563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414936.522603}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=558.922250515 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] #quality_metric: host=algo-1, epoch=129, train loss <loss>=16.7895750046\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:57 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] Epoch[130] Batch[0] avg_epoch_loss=16.711121\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=16.7111206055\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] Epoch[130] Batch[5] avg_epoch_loss=16.781550\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=16.7815500895\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] Epoch[130] Batch [5]#011Speed: 927.58 samples/sec#011loss=16.781550\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] processed a total of 552 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1008.5439682006836, \"sum\": 1008.5439682006836, \"min\": 1008.5439682006836}}, \"EndTime\": 1580414938.621669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414937.612651}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=547.252659791 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] #quality_metric: host=algo-1, epoch=130, train loss <loss>=16.7596176995\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:58 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] Epoch[131] Batch[0] avg_epoch_loss=16.815138\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=16.8151378632\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] Epoch[131] Batch[5] avg_epoch_loss=16.723925\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=16.7239252726\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] Epoch[131] Batch [5]#011Speed: 916.97 samples/sec#011loss=16.723925\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.2011699676514, \"sum\": 1083.2011699676514, \"min\": 1083.2011699676514}}, \"EndTime\": 1580414939.705446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414938.621758}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=542.769867847 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] #quality_metric: host=algo-1, epoch=131, train loss <loss>=16.7552021027\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:08:59 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] Epoch[132] Batch[0] avg_epoch_loss=16.782961\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=16.7829608917\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] Epoch[132] Batch[5] avg_epoch_loss=16.779103\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=16.7791026433\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] Epoch[132] Batch [5]#011Speed: 909.34 samples/sec#011loss=16.779103\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1069.4448947906494, \"sum\": 1069.4448947906494, \"min\": 1069.4448947906494}}, \"EndTime\": 1580414940.775474, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414939.705535}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=553.489231713 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] #quality_metric: host=algo-1, epoch=132, train loss <loss>=16.7664361954\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:00 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] Epoch[133] Batch[0] avg_epoch_loss=16.678076\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=16.6780757904\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] Epoch[133] Batch[5] avg_epoch_loss=16.743692\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=16.7436917623\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] Epoch[133] Batch [5]#011Speed: 943.08 samples/sec#011loss=16.743692\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1106.1508655548096, \"sum\": 1106.1508655548096, \"min\": 1106.1508655548096}}, \"EndTime\": 1580414941.882192, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414940.775564}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=552.301838485 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] #quality_metric: host=algo-1, epoch=133, train loss <loss>=16.7158327103\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:01 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] Epoch[134] Batch[0] avg_epoch_loss=16.823685\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=16.8236846924\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] Epoch[134] Batch[5] avg_epoch_loss=16.726186\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=16.7261857986\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] Epoch[134] Batch [5]#011Speed: 918.61 samples/sec#011loss=16.726186\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1110.870122909546, \"sum\": 1110.870122909546, \"min\": 1110.870122909546}}, \"EndTime\": 1580414942.993624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414941.88228}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=543.655385577 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] #quality_metric: host=algo-1, epoch=134, train loss <loss>=16.7361883163\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:02 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:03 INFO 139704039475008] Epoch[135] Batch[0] avg_epoch_loss=16.769314\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=16.7693138123\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:03 INFO 139704039475008] Epoch[135] Batch[5] avg_epoch_loss=16.765631\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:03 INFO 139704039475008] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=16.765630722\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:03 INFO 139704039475008] Epoch[135] Batch [5]#011Speed: 928.74 samples/sec#011loss=16.765631\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] Epoch[135] Batch[10] avg_epoch_loss=16.750635\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=16.7326393127\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] Epoch[135] Batch [10]#011Speed: 928.67 samples/sec#011loss=16.732639\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1153.9959907531738, \"sum\": 1153.9959907531738, \"min\": 1153.9959907531738}}, \"EndTime\": 1580414944.14819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414942.99371}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=557.13445534 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=135, train loss <loss>=16.7506346269\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] Epoch[136] Batch[0] avg_epoch_loss=16.580408\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=16.5804080963\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] Epoch[136] Batch[5] avg_epoch_loss=16.706724\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=16.7067241669\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:04 INFO 139704039475008] Epoch[136] Batch [5]#011Speed: 922.70 samples/sec#011loss=16.706724\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] processed a total of 559 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 987.6940250396729, \"sum\": 987.6940250396729, \"min\": 987.6940250396729}}, \"EndTime\": 1580414945.136422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414944.148274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=565.889618243 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=136, train loss <loss>=16.7165514628\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] Epoch[137] Batch[0] avg_epoch_loss=16.744328\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=16.7443275452\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] Epoch[137] Batch[5] avg_epoch_loss=16.746799\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=16.7467988332\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:05 INFO 139704039475008] Epoch[137] Batch [5]#011Speed: 916.73 samples/sec#011loss=16.746799\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.3108921051025, \"sum\": 1056.3108921051025, \"min\": 1056.3108921051025}}, \"EndTime\": 1580414946.193347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414945.136509}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=578.36239958 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=137, train loss <loss>=16.7377063751\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] Epoch[138] Batch[0] avg_epoch_loss=16.814606\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=16.8146057129\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] Epoch[138] Batch[5] avg_epoch_loss=16.696756\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=16.6967563629\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:06 INFO 139704039475008] Epoch[138] Batch [5]#011Speed: 891.47 samples/sec#011loss=16.696756\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.9240226745605, \"sum\": 1067.9240226745605, \"min\": 1067.9240226745605}}, \"EndTime\": 1580414947.261845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414946.193426}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=555.218987641 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=138, train loss <loss>=16.7008562088\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] Epoch[139] Batch[0] avg_epoch_loss=16.703949\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:07 INFO 139704039475008] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=16.7039489746\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] Epoch[139] Batch[5] avg_epoch_loss=16.736777\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=16.7367769877\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] Epoch[139] Batch [5]#011Speed: 913.79 samples/sec#011loss=16.736777\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1064.5129680633545, \"sum\": 1064.5129680633545, \"min\": 1064.5129680633545}}, \"EndTime\": 1580414948.326976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414947.26193}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=562.627822082 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=139, train loss <loss>=16.7577587128\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] Epoch[140] Batch[0] avg_epoch_loss=16.653833\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:08 INFO 139704039475008] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=16.6538333893\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] Epoch[140] Batch[5] avg_epoch_loss=16.713915\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=16.7139151891\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] Epoch[140] Batch [5]#011Speed: 934.30 samples/sec#011loss=16.713915\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] processed a total of 547 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 996.9708919525146, \"sum\": 996.9708919525146, \"min\": 996.9708919525146}}, \"EndTime\": 1580414949.324505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414948.327069}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=548.598065176 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=140, train loss <loss>=16.7102440728\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] Epoch[141] Batch[0] avg_epoch_loss=16.787014\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:09 INFO 139704039475008] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=16.7870140076\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] Epoch[141] Batch[5] avg_epoch_loss=16.756319\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=16.756319046\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] Epoch[141] Batch [5]#011Speed: 936.52 samples/sec#011loss=16.756319\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.9529628753662, \"sum\": 1046.9529628753662, \"min\": 1046.9529628753662}}, \"EndTime\": 1580414950.372071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414949.324585}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=557.736173709 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=141, train loss <loss>=16.7232093811\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] Epoch[142] Batch[0] avg_epoch_loss=16.797394\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:10 INFO 139704039475008] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=16.7973937988\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] Epoch[142] Batch[5] avg_epoch_loss=16.767280\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=16.7672802607\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] Epoch[142] Batch [5]#011Speed: 925.94 samples/sec#011loss=16.767280\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1055.2849769592285, \"sum\": 1055.2849769592285, \"min\": 1055.2849769592285}}, \"EndTime\": 1580414951.427991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414950.37216}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=569.442620581 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=142, train loss <loss>=16.7313621521\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] Epoch[143] Batch[0] avg_epoch_loss=16.671631\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:11 INFO 139704039475008] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=16.6716308594\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] Epoch[143] Batch[5] avg_epoch_loss=16.676421\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=16.6764205297\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] Epoch[143] Batch [5]#011Speed: 887.33 samples/sec#011loss=16.676421\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1063.37308883667, \"sum\": 1063.37308883667, \"min\": 1063.37308883667}}, \"EndTime\": 1580414952.491977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414951.428081}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=576.395880075 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=143, train loss <loss>=16.7317220688\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] Epoch[144] Batch[0] avg_epoch_loss=16.773693\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:12 INFO 139704039475008] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=16.7736930847\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] Epoch[144] Batch[5] avg_epoch_loss=16.693387\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=16.6933870316\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] Epoch[144] Batch [5]#011Speed: 904.33 samples/sec#011loss=16.693387\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.8048973083496, \"sum\": 1079.8048973083496, \"min\": 1079.8048973083496}}, \"EndTime\": 1580414953.572332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414952.492065}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=544.476695853 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] #quality_metric: host=algo-1, epoch=144, train loss <loss>=16.7315608978\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:13 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] Epoch[145] Batch[0] avg_epoch_loss=16.813507\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=16.8135070801\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] Epoch[145] Batch[5] avg_epoch_loss=16.724079\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=16.7240791321\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] Epoch[145] Batch [5]#011Speed: 897.27 samples/sec#011loss=16.724079\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.927921295166, \"sum\": 1079.927921295166, \"min\": 1079.927921295166}}, \"EndTime\": 1580414954.652813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414953.572421}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=549.04453861 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] #quality_metric: host=algo-1, epoch=145, train loss <loss>=16.746062851\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:14 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] Epoch[146] Batch[0] avg_epoch_loss=16.746000\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=16.7460002899\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] Epoch[146] Batch[5] avg_epoch_loss=16.737493\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=16.7374925613\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] Epoch[146] Batch [5]#011Speed: 885.01 samples/sec#011loss=16.737493\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] processed a total of 571 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.573823928833, \"sum\": 1027.573823928833, \"min\": 1027.573823928833}}, \"EndTime\": 1580414955.680993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414954.652903}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=555.608482088 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] #quality_metric: host=algo-1, epoch=146, train loss <loss>=16.7358048757\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:15 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] Epoch[147] Batch[0] avg_epoch_loss=16.663652\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=16.66365242\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] Epoch[147] Batch[5] avg_epoch_loss=16.733068\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=16.7330675125\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] Epoch[147] Batch [5]#011Speed: 916.90 samples/sec#011loss=16.733068\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.2279872894287, \"sum\": 1088.2279872894287, \"min\": 1088.2279872894287}}, \"EndTime\": 1580414956.769807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414955.68108}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=581.61115434 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] #quality_metric: host=algo-1, epoch=147, train loss <loss>=16.738060379\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:16 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] Epoch[148] Batch[0] avg_epoch_loss=16.722639\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=16.7226390839\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] Epoch[148] Batch[5] avg_epoch_loss=16.692655\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=16.6926549276\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] Epoch[148] Batch [5]#011Speed: 905.17 samples/sec#011loss=16.692655\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.1279029846191, \"sum\": 1084.1279029846191, \"min\": 1084.1279029846191}}, \"EndTime\": 1580414957.854493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414956.769894}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=549.687287997 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] #quality_metric: host=algo-1, epoch=148, train loss <loss>=16.6776193619\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:17 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] Epoch[149] Batch[0] avg_epoch_loss=16.640757\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=16.6407566071\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] Epoch[149] Batch[5] avg_epoch_loss=16.693269\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=16.6932690938\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] Epoch[149] Batch [5]#011Speed: 930.33 samples/sec#011loss=16.693269\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.5849418640137, \"sum\": 1094.5849418640137, \"min\": 1094.5849418640137}}, \"EndTime\": 1580414958.949613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414957.854579}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=530.731801148 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] #quality_metric: host=algo-1, epoch=149, train loss <loss>=16.7473266602\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:18 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] Epoch[150] Batch[0] avg_epoch_loss=16.769630\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=16.7696304321\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] Epoch[150] Batch[5] avg_epoch_loss=16.699618\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=16.6996177038\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] Epoch[150] Batch [5]#011Speed: 931.88 samples/sec#011loss=16.699618\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] processed a total of 568 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1023.8430500030518, \"sum\": 1023.8430500030518, \"min\": 1023.8430500030518}}, \"EndTime\": 1580414959.974021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414958.949702}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=554.704328011 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] #quality_metric: host=algo-1, epoch=150, train loss <loss>=16.6750958761\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:19 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:20 INFO 139704039475008] Epoch[151] Batch[0] avg_epoch_loss=16.690622\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=16.6906223297\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:20 INFO 139704039475008] Epoch[151] Batch[5] avg_epoch_loss=16.702216\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:20 INFO 139704039475008] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=16.7022164663\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:20 INFO 139704039475008] Epoch[151] Batch [5]#011Speed: 939.08 samples/sec#011loss=16.702216\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.7880115509033, \"sum\": 1085.7880115509033, \"min\": 1085.7880115509033}}, \"EndTime\": 1580414961.060351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414959.974107}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=572.787793441 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=151, train loss <loss>=16.7402305603\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] Epoch[152] Batch[0] avg_epoch_loss=16.719593\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=16.7195930481\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] Epoch[152] Batch[5] avg_epoch_loss=16.642593\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=16.6425933838\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:21 INFO 139704039475008] Epoch[152] Batch [5]#011Speed: 920.32 samples/sec#011loss=16.642593\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] processed a total of 542 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 999.2551803588867, \"sum\": 999.2551803588867, \"min\": 999.2551803588867}}, \"EndTime\": 1580414962.060171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414961.060439}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=542.331400723 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=152, train loss <loss>=16.7012229496\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] Epoch[153] Batch[0] avg_epoch_loss=16.715719\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=16.715719223\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] Epoch[153] Batch[5] avg_epoch_loss=16.707682\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=16.7076816559\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:22 INFO 139704039475008] Epoch[153] Batch [5]#011Speed: 923.41 samples/sec#011loss=16.707682\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] processed a total of 566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1015.1700973510742, \"sum\": 1015.1700973510742, \"min\": 1015.1700973510742}}, \"EndTime\": 1580414963.075965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414962.060261}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=557.470678837 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=153, train loss <loss>=16.7094571855\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] Epoch[154] Batch[0] avg_epoch_loss=16.781425\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=16.7814254761\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] Epoch[154] Batch[5] avg_epoch_loss=16.743305\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=16.7433045705\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:23 INFO 139704039475008] Epoch[154] Batch [5]#011Speed: 934.49 samples/sec#011loss=16.743305\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 984.8299026489258, \"sum\": 984.8299026489258, \"min\": 984.8299026489258}}, \"EndTime\": 1580414964.061365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414963.076052}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=580.734758611 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=154, train loss <loss>=16.7229164971\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] Epoch[155] Batch[0] avg_epoch_loss=16.719883\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=16.7198829651\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] Epoch[155] Batch[5] avg_epoch_loss=16.743945\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=16.7439448039\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:24 INFO 139704039475008] Epoch[155] Batch [5]#011Speed: 888.01 samples/sec#011loss=16.743945\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1090.3630256652832, \"sum\": 1090.3630256652832, \"min\": 1090.3630256652832}}, \"EndTime\": 1580414965.152287, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414964.061452}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=535.537145734 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=155, train loss <loss>=16.7333061218\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] Epoch[156] Batch[0] avg_epoch_loss=16.757572\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=16.7575721741\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] Epoch[156] Batch[5] avg_epoch_loss=16.732323\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=16.7323226929\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:25 INFO 139704039475008] Epoch[156] Batch [5]#011Speed: 935.30 samples/sec#011loss=16.732323\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.8860836029053, \"sum\": 1074.8860836029053, \"min\": 1074.8860836029053}}, \"EndTime\": 1580414966.227735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414965.152375}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] #throughput_metric: host=algo-1, train throughput=559.061435227 records/second\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] #quality_metric: host=algo-1, epoch=156, train loss <loss>=16.7575229645\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Loading parameters from best epoch (116)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 13.92507553100586, \"sum\": 13.92507553100586, \"min\": 13.92507553100586}}, \"EndTime\": 1580414966.242288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.227823}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] stopping training now\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Final loss: 16.2728413582 (occurred at epoch 116)\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] #quality_metric: host=algo-1, train final_loss <loss>=16.2728413582\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 WARNING 139704039475008] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 259.30213928222656, \"sum\": 259.30213928222656, \"min\": 259.30213928222656}}, \"EndTime\": 1580414966.50227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.242352}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 333.73117446899414, \"sum\": 333.73117446899414, \"min\": 333.73117446899414}}, \"EndTime\": 1580414966.576662, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.502339}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 12.003183364868164, \"sum\": 12.003183364868164, \"min\": 12.003183364868164}}, \"EndTime\": 1580414966.588764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.576725}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:26 INFO 139704039475008] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03600120544433594, \"sum\": 0.03600120544433594, \"min\": 0.03600120544433594}}, \"EndTime\": 1580414966.589487, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.588806}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 2550.4350662231445, \"sum\": 2550.4350662231445, \"min\": 2550.4350662231445}}, \"EndTime\": 1580414969.139891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414966.589528}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, RMSE): 7349953.00952\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, mean_wQuantileLoss): 0.6631862\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.1]): 0.53367376\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.2]): 0.71857834\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.3]): 0.7963378\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.4]): 0.8326759\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.5]): 0.825649\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.6]): 0.76851976\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.7]): 0.66521806\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.8]): 0.51549464\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #test_score (algo-1, wQuantileLoss[0.9]): 0.31252882\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.663186192513\u001b[0m\n",
      "\u001b[34m[01/30/2020 20:09:29 INFO 139704039475008] #quality_metric: host=algo-1, test RMSE <loss>=7349953.00952\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 170218.80316734314, \"sum\": 170218.80316734314, \"min\": 170218.80316734314}, \"setuptime\": {\"count\": 1, \"max\": 10.14399528503418, \"sum\": 10.14399528503418, \"min\": 10.14399528503418}}, \"EndTime\": 1580414969.16208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580414969.139964}\n",
      "\u001b[0m\n",
      "\n",
      "2020-01-30 20:09:39 Uploading - Uploading generated training model\n",
      "2020-01-30 20:09:39 Completed - Training job completed\n",
      "Training seconds: 209\n",
      "Billable seconds: 209\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://forecastingincomingsolarenergyjan2020/vincent/train/train.json\",\n",
    "    \"test\": \"s3://forecastingincomingsolarenergyjan2020/vincent/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading netCDF4 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import SDK to load data from netCDF4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'H'\n",
    "prediction_length = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeseries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-74ca5ceda86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'timeseries' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[120], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '1994-01-01 00:00:00',\n",
       "  'target': [7437900,\n",
       "   8199600,\n",
       "   10694400,\n",
       "   8704800,\n",
       "   12803100,\n",
       "   12027300,\n",
       "   12519000,\n",
       "   11570100,\n",
       "   9417300,\n",
       "   9941700,\n",
       "   11968200,\n",
       "   12077400,\n",
       "   4561500,\n",
       "   13197300,\n",
       "   12252600,\n",
       "   11796900,\n",
       "   11721300,\n",
       "   7685400,\n",
       "   5451600,\n",
       "   9810900,\n",
       "   12423600,\n",
       "   11590800,\n",
       "   1609800,\n",
       "   3166500,\n",
       "   3104700,\n",
       "   2655300,\n",
       "   6268200,\n",
       "   11357100,\n",
       "   10005900,\n",
       "   11276400,\n",
       "   11739600,\n",
       "   11870100,\n",
       "   11089500,\n",
       "   8790000,\n",
       "   11895600,\n",
       "   12165300,\n",
       "   7313100,\n",
       "   8252700,\n",
       "   11886600,\n",
       "   12072600,\n",
       "   10689000,\n",
       "   12416400,\n",
       "   12456000,\n",
       "   12309900,\n",
       "   9021300,\n",
       "   9142200,\n",
       "   12788700,\n",
       "   13036200,\n",
       "   11170200,\n",
       "   13284000,\n",
       "   11163600,\n",
       "   8958300,\n",
       "   7813500,\n",
       "   13773600,\n",
       "   11419800,\n",
       "   11549700,\n",
       "   11442600,\n",
       "   12351300,\n",
       "   10773000,\n",
       "   13398300,\n",
       "   14266200,\n",
       "   13605900,\n",
       "   7520400,\n",
       "   3917700,\n",
       "   8274000,\n",
       "   13047900,\n",
       "   15183000,\n",
       "   14596800,\n",
       "   14742600,\n",
       "   14959500,\n",
       "   14130000,\n",
       "   14401800,\n",
       "   14026500,\n",
       "   15988200,\n",
       "   16366200,\n",
       "   16184100,\n",
       "   14604600,\n",
       "   13388400,\n",
       "   17387400,\n",
       "   17657100,\n",
       "   17504400,\n",
       "   17134200,\n",
       "   16662600,\n",
       "   14343600,\n",
       "   8371200,\n",
       "   6264900,\n",
       "   4595700,\n",
       "   2858700,\n",
       "   3267300,\n",
       "   5874300,\n",
       "   11259300,\n",
       "   9503700,\n",
       "   10217100,\n",
       "   18846900,\n",
       "   18447300,\n",
       "   18719100,\n",
       "   16971600,\n",
       "   10020000,\n",
       "   18915300,\n",
       "   14541900,\n",
       "   19118400,\n",
       "   19968900,\n",
       "   15298500,\n",
       "   10589100,\n",
       "   12572100,\n",
       "   18650700,\n",
       "   19277400,\n",
       "   14770800,\n",
       "   22081800,\n",
       "   22152000,\n",
       "   19551000,\n",
       "   22544400,\n",
       "   5637600,\n",
       "   1752900,\n",
       "   1836300,\n",
       "   13347000,\n",
       "   16548600,\n",
       "   5025300,\n",
       "   9642000,\n",
       "   23787900,\n",
       "   22641900,\n",
       "   21170100,\n",
       "   19276200,\n",
       "   23799900,\n",
       "   10562100,\n",
       "   10595700,\n",
       "   22674000,\n",
       "   14618100,\n",
       "   24726600,\n",
       "   21721800,\n",
       "   21429300,\n",
       "   18830700,\n",
       "   20777100,\n",
       "   18309300,\n",
       "   22853700,\n",
       "   25850400,\n",
       "   22198500,\n",
       "   12409500,\n",
       "   25104900,\n",
       "   25654800,\n",
       "   24021600,\n",
       "   21522300,\n",
       "   26623800,\n",
       "   27378000,\n",
       "   27258300,\n",
       "   26766600,\n",
       "   11901000,\n",
       "   24991800,\n",
       "   23052000,\n",
       "   23409000,\n",
       "   17607900,\n",
       "   18339300,\n",
       "   26211000,\n",
       "   27756900,\n",
       "   4753800,\n",
       "   10194900,\n",
       "   27622200,\n",
       "   21342600,\n",
       "   22178400,\n",
       "   23879100,\n",
       "   11683200,\n",
       "   10107900,\n",
       "   4683600,\n",
       "   13442100,\n",
       "   17223600,\n",
       "   24314400,\n",
       "   25288500,\n",
       "   29382300,\n",
       "   28519500,\n",
       "   28016400,\n",
       "   21775200,\n",
       "   27607500,\n",
       "   27711900,\n",
       "   28748400,\n",
       "   28966500,\n",
       "   28785000,\n",
       "   28369800,\n",
       "   26683800,\n",
       "   27287700,\n",
       "   28120800,\n",
       "   27894300,\n",
       "   27756900,\n",
       "   25275000,\n",
       "   26827800,\n",
       "   24197400,\n",
       "   27721200,\n",
       "   25876800,\n",
       "   14297700,\n",
       "   21905400,\n",
       "   29499300,\n",
       "   29211600,\n",
       "   27363900,\n",
       "   26386200,\n",
       "   27834000,\n",
       "   28438800,\n",
       "   29639100,\n",
       "   28782000,\n",
       "   27997200,\n",
       "   28281900,\n",
       "   15512100,\n",
       "   27552600,\n",
       "   29576400,\n",
       "   29376000,\n",
       "   26856900,\n",
       "   9561000,\n",
       "   28809300,\n",
       "   28492200,\n",
       "   29228100,\n",
       "   24418500,\n",
       "   9593400,\n",
       "   24051000,\n",
       "   26674200,\n",
       "   27418500,\n",
       "   27996300,\n",
       "   26843400,\n",
       "   29630100,\n",
       "   26277300,\n",
       "   28711800,\n",
       "   28153200,\n",
       "   27720300,\n",
       "   21610800,\n",
       "   16884900,\n",
       "   24473400,\n",
       "   21722100,\n",
       "   27455100,\n",
       "   25038600,\n",
       "   26630700,\n",
       "   21466800,\n",
       "   20194500,\n",
       "   17749200,\n",
       "   25061100,\n",
       "   18042600,\n",
       "   27549600,\n",
       "   28202700,\n",
       "   27887400,\n",
       "   28307400,\n",
       "   27046200,\n",
       "   27049500,\n",
       "   24581700,\n",
       "   27210600,\n",
       "   28447200,\n",
       "   27582600,\n",
       "   26307600,\n",
       "   27106800,\n",
       "   22431300,\n",
       "   24995100,\n",
       "   25425000,\n",
       "   27566100,\n",
       "   24822900,\n",
       "   27515400,\n",
       "   25156200,\n",
       "   22476600,\n",
       "   25640100,\n",
       "   26698200,\n",
       "   25467900,\n",
       "   23038200,\n",
       "   24062400,\n",
       "   25153500,\n",
       "   26205000,\n",
       "   16696800,\n",
       "   22713000,\n",
       "   24162600,\n",
       "   21382800,\n",
       "   19601700,\n",
       "   19132500,\n",
       "   23094000,\n",
       "   26624400,\n",
       "   24564000,\n",
       "   24150000,\n",
       "   21825600,\n",
       "   8597700,\n",
       "   23881500,\n",
       "   25645500,\n",
       "   25407300,\n",
       "   23691300,\n",
       "   9206100,\n",
       "   13252800,\n",
       "   23594700,\n",
       "   24240600,\n",
       "   24568200,\n",
       "   21426300,\n",
       "   5718600,\n",
       "   12217200,\n",
       "   9767400,\n",
       "   24757500,\n",
       "   24924000,\n",
       "   23269200,\n",
       "   19087200,\n",
       "   21433200,\n",
       "   13264200,\n",
       "   20099700,\n",
       "   23046900,\n",
       "   23784300,\n",
       "   20214900,\n",
       "   21864300,\n",
       "   19905900,\n",
       "   2488500,\n",
       "   23481600,\n",
       "   23028000,\n",
       "   22245900,\n",
       "   5541600,\n",
       "   22499100,\n",
       "   22316700,\n",
       "   21735000,\n",
       "   22378200,\n",
       "   21857700,\n",
       "   21636000,\n",
       "   21397200,\n",
       "   21211800,\n",
       "   20420100,\n",
       "   17730000,\n",
       "   20090400,\n",
       "   20118000,\n",
       "   17304300,\n",
       "   18946200,\n",
       "   19169400,\n",
       "   17444400,\n",
       "   19750500,\n",
       "   4497900,\n",
       "   7159500,\n",
       "   19379400,\n",
       "   7951500,\n",
       "   18979500,\n",
       "   16578000,\n",
       "   2387700,\n",
       "   7866000,\n",
       "   18607800,\n",
       "   8252400,\n",
       "   15888900,\n",
       "   17957400,\n",
       "   13127700,\n",
       "   16103100,\n",
       "   17731500,\n",
       "   13401900,\n",
       "   6371400,\n",
       "   15388200,\n",
       "   12622200,\n",
       "   16852800,\n",
       "   17004900,\n",
       "   9921900,\n",
       "   9683100,\n",
       "   15408000,\n",
       "   15811800,\n",
       "   10533000,\n",
       "   11590500,\n",
       "   3276600,\n",
       "   7461300,\n",
       "   14222100,\n",
       "   15001800,\n",
       "   14462700,\n",
       "   10932900,\n",
       "   15290700,\n",
       "   12027600,\n",
       "   13481700,\n",
       "   10948800,\n",
       "   5500800,\n",
       "   14371500,\n",
       "   14166900,\n",
       "   13881900,\n",
       "   13974300,\n",
       "   14172900,\n",
       "   13446600,\n",
       "   13544700,\n",
       "   13427400,\n",
       "   11554800,\n",
       "   10320300,\n",
       "   7045800,\n",
       "   5149200,\n",
       "   10584300,\n",
       "   1093200,\n",
       "   2410200,\n",
       "   13180200,\n",
       "   12717300,\n",
       "   13010400,\n",
       "   12999900,\n",
       "   12810900,\n",
       "   12364500,\n",
       "   12475800,\n",
       "   12586200,\n",
       "   9589800,\n",
       "   10530600,\n",
       "   11386200,\n",
       "   12036600,\n",
       "   11645400,\n",
       "   10968900,\n",
       "   11333400,\n",
       "   11743500,\n",
       "   11288700,\n",
       "   3126600,\n",
       "   906000,\n",
       "   6975900,\n",
       "   10076700,\n",
       "   12107400,\n",
       "   11287500,\n",
       "   4458600,\n",
       "   12486900,\n",
       "   11932500,\n",
       "   10188600,\n",
       "   3411900,\n",
       "   1299600,\n",
       "   9032400,\n",
       "   6495600,\n",
       "   12930000,\n",
       "   11347500,\n",
       "   8400900,\n",
       "   3675300,\n",
       "   6149400,\n",
       "   6622800,\n",
       "   12842700,\n",
       "   12534000,\n",
       "   12640200,\n",
       "   12029700,\n",
       "   8218200,\n",
       "   897600,\n",
       "   1736700,\n",
       "   1829400,\n",
       "   14730600,\n",
       "   4659300,\n",
       "   5227500,\n",
       "   7802400,\n",
       "   4716300,\n",
       "   1436400,\n",
       "   7198800,\n",
       "   10884600,\n",
       "   14005200,\n",
       "   10362000,\n",
       "   14250300,\n",
       "   14436900,\n",
       "   4709100,\n",
       "   14906100,\n",
       "   7393500,\n",
       "   14862000,\n",
       "   1902600,\n",
       "   2938800,\n",
       "   15353400,\n",
       "   15255000,\n",
       "   15771600,\n",
       "   15444900,\n",
       "   15477000,\n",
       "   12000,\n",
       "   1313700,\n",
       "   3243900,\n",
       "   4035900,\n",
       "   3287400,\n",
       "   2799300,\n",
       "   4298700,\n",
       "   7221000,\n",
       "   15655500,\n",
       "   17345700,\n",
       "   17731500,\n",
       "   17870100,\n",
       "   17862000,\n",
       "   18236700,\n",
       "   18657600,\n",
       "   18039900,\n",
       "   7938600,\n",
       "   14849700,\n",
       "   19800000,\n",
       "   19771500,\n",
       "   19243500,\n",
       "   16930200,\n",
       "   20005800,\n",
       "   16619100,\n",
       "   20647200,\n",
       "   21316200,\n",
       "   21204900,\n",
       "   18894900,\n",
       "   20136600,\n",
       "   19968000,\n",
       "   19851300,\n",
       "   20361300,\n",
       "   3537600,\n",
       "   15045900,\n",
       "   16830900,\n",
       "   16044600,\n",
       "   19200900,\n",
       "   22450500,\n",
       "   22054500,\n",
       "   20571600,\n",
       "   11404200,\n",
       "   10730400,\n",
       "   18736800,\n",
       "   12054900,\n",
       "   11178000,\n",
       "   13825200,\n",
       "   9008400,\n",
       "   6233100,\n",
       "   11845800,\n",
       "   15081600,\n",
       "   2786700,\n",
       "   1788300,\n",
       "   24624900,\n",
       "   25132500,\n",
       "   24104100,\n",
       "   24154800,\n",
       "   24428400,\n",
       "   7841400,\n",
       "   5943900,\n",
       "   20200800,\n",
       "   10136700,\n",
       "   7181400,\n",
       "   11094000,\n",
       "   27009300,\n",
       "   19342200,\n",
       "   1811700,\n",
       "   10136400,\n",
       "   27688800,\n",
       "   25998000,\n",
       "   4230900,\n",
       "   19225500,\n",
       "   22419300,\n",
       "   26192100,\n",
       "   26698800,\n",
       "   26062200,\n",
       "   10311900,\n",
       "   10245900,\n",
       "   11333100,\n",
       "   26386500,\n",
       "   17673300,\n",
       "   28121400,\n",
       "   26566200,\n",
       "   14961900,\n",
       "   16417500,\n",
       "   19146000,\n",
       "   13559100,\n",
       "   22229700,\n",
       "   13672500,\n",
       "   14584800,\n",
       "   8337600,\n",
       "   20109300,\n",
       "   17841300,\n",
       "   18640800,\n",
       "   24267900,\n",
       "   27397200,\n",
       "   26605500,\n",
       "   28194900,\n",
       "   10163400,\n",
       "   29221500,\n",
       "   28743300,\n",
       "   21492900,\n",
       "   9711900,\n",
       "   23759700,\n",
       "   8852400,\n",
       "   22581000,\n",
       "   17891700,\n",
       "   3911400,\n",
       "   12607800,\n",
       "   11083800,\n",
       "   19437000,\n",
       "   12629100,\n",
       "   18104100,\n",
       "   9981000,\n",
       "   18996000,\n",
       "   13550400,\n",
       "   25976400,\n",
       "   24817500,\n",
       "   24752400,\n",
       "   29532900,\n",
       "   27228600,\n",
       "   21281400,\n",
       "   28296000,\n",
       "   23708400,\n",
       "   17811900,\n",
       "   28219500,\n",
       "   23471400,\n",
       "   27177600,\n",
       "   20811600,\n",
       "   10274100,\n",
       "   13078800,\n",
       "   13148400,\n",
       "   15253500,\n",
       "   20461800,\n",
       "   11436900,\n",
       "   17902500,\n",
       "   22656900,\n",
       "   27939300,\n",
       "   26853600,\n",
       "   12886800,\n",
       "   7151700,\n",
       "   17602500,\n",
       "   8013900,\n",
       "   9632700,\n",
       "   16325700,\n",
       "   25364100,\n",
       "   14165700,\n",
       "   19059300,\n",
       "   25462500,\n",
       "   21273600,\n",
       "   24463200,\n",
       "   28192500,\n",
       "   29036100,\n",
       "   22339500,\n",
       "   26402400,\n",
       "   17883600,\n",
       "   15735300,\n",
       "   14091000,\n",
       "   28623000,\n",
       "   28872900,\n",
       "   27261600,\n",
       "   28549800,\n",
       "   28535700,\n",
       "   28054500,\n",
       "   26860500,\n",
       "   27216900,\n",
       "   26431500,\n",
       "   16317300,\n",
       "   24177000,\n",
       "   27974400,\n",
       "   28281000,\n",
       "   21758100,\n",
       "   23130900,\n",
       "   26864100,\n",
       "   18547800,\n",
       "   18106200,\n",
       "   15814200,\n",
       "   23751900,\n",
       "   24789600,\n",
       "   26923800,\n",
       "   26769300,\n",
       "   26904900,\n",
       "   27070500,\n",
       "   27408600,\n",
       "   27492600,\n",
       "   27625500,\n",
       "   27195600,\n",
       "   26607300,\n",
       "   26807700,\n",
       "   26628300,\n",
       "   25440300,\n",
       "   23733600,\n",
       "   9499200,\n",
       "   6394800,\n",
       "   17863500,\n",
       "   22615500,\n",
       "   25275900,\n",
       "   23827500,\n",
       "   23124600,\n",
       "   21593700,\n",
       "   20481000,\n",
       "   24926700,\n",
       "   24511800,\n",
       "   24652500,\n",
       "   25120800,\n",
       "   20384400,\n",
       "   23632800,\n",
       "   25548300,\n",
       "   23546700,\n",
       "   20415600,\n",
       "   10705500,\n",
       "   13799400,\n",
       "   18705300,\n",
       "   14013000,\n",
       "   16587300,\n",
       "   6273000,\n",
       "   14166900,\n",
       "   23337000,\n",
       "   23778900,\n",
       "   20597100,\n",
       "   19752900,\n",
       "   21148200,\n",
       "   21214500,\n",
       "   20509800,\n",
       "   14761500,\n",
       "   17644800,\n",
       "   19924500,\n",
       "   20318400,\n",
       "   20572500,\n",
       "   19059600,\n",
       "   20634000,\n",
       "   10575300,\n",
       "   16389000,\n",
       "   16456200,\n",
       "   12445500,\n",
       "   19931400,\n",
       "   15985500,\n",
       "   20361000,\n",
       "   18146400,\n",
       "   11484300,\n",
       "   19402500,\n",
       "   19308300,\n",
       "   11016300,\n",
       "   15738900,\n",
       "   13631700,\n",
       "   20458200,\n",
       "   19636500,\n",
       "   14518800,\n",
       "   15958200,\n",
       "   18261000,\n",
       "   7105200,\n",
       "   15973200,\n",
       "   17762400,\n",
       "   10536900,\n",
       "   18781200,\n",
       "   18704100,\n",
       "   17667600,\n",
       "   15186300,\n",
       "   5460300,\n",
       "   18089100,\n",
       "   17856600,\n",
       "   18075300,\n",
       "   17715000,\n",
       "   16880700,\n",
       "   16292100,\n",
       "   16017600,\n",
       "   15581400,\n",
       "   13204200,\n",
       "   16207800,\n",
       "   13685100,\n",
       "   14337300,\n",
       "   14938200,\n",
       "   14835600,\n",
       "   15307200,\n",
       "   14962800,\n",
       "   14866500,\n",
       "   10683000,\n",
       "   11815800,\n",
       "   10580100,\n",
       "   11314200,\n",
       "   13910400,\n",
       "   13110000,\n",
       "   15180600,\n",
       "   13290600,\n",
       "   14392200,\n",
       "   11895600,\n",
       "   7975500,\n",
       "   10401000,\n",
       "   6557400,\n",
       "   8102100,\n",
       "   8457900,\n",
       "   2641500,\n",
       "   3871500,\n",
       "   13197900,\n",
       "   13407900,\n",
       "   12295500,\n",
       "   13100100,\n",
       "   2985900,\n",
       "   1444800,\n",
       "   12177000,\n",
       "   13453800,\n",
       "   12525900,\n",
       "   10543500,\n",
       "   6214500,\n",
       "   8417400,\n",
       "   1683900,\n",
       "   1512300,\n",
       "   1729500,\n",
       "   1512600,\n",
       "   2126100,\n",
       "   3476400,\n",
       "   1262700,\n",
       "   3074400,\n",
       "   12523800,\n",
       "   12089700,\n",
       "   12300600,\n",
       "   12206100,\n",
       "   12275700,\n",
       "   9737700,\n",
       "   1958400,\n",
       "   12325200,\n",
       "   12225900,\n",
       "   12256500,\n",
       "   1851300,\n",
       "   1408500,\n",
       "   10060800,\n",
       "   11388000,\n",
       "   12441000,\n",
       "   12450300],\n",
       "  'cat': [0]},\n",
       " {'start': '1994-01-01 00:00:00',\n",
       "  'target': [8586600,\n",
       "   4217700,\n",
       "   6853500,\n",
       "   8925600,\n",
       "   13162500,\n",
       "   11469300,\n",
       "   12754800,\n",
       "   11862600,\n",
       "   10196400,\n",
       "   8593500,\n",
       "   12269100,\n",
       "   12332100,\n",
       "   2813700,\n",
       "   12774000,\n",
       "   12423900,\n",
       "   11388900,\n",
       "   12181500,\n",
       "   10353000,\n",
       "   5945700,\n",
       "   7513200,\n",
       "   12394500,\n",
       "   11132100,\n",
       "   2417700,\n",
       "   8851200,\n",
       "   3222600,\n",
       "   2521500,\n",
       "   11775600,\n",
       "   11252700,\n",
       "   11235600,\n",
       "   8781300,\n",
       "   12099000,\n",
       "   11793900,\n",
       "   11154300,\n",
       "   11121300,\n",
       "   11843700,\n",
       "   11990100,\n",
       "   8593800,\n",
       "   8846400,\n",
       "   12046800,\n",
       "   12262800,\n",
       "   9807900,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   10223616,\n",
       "   5499600,\n",
       "   12874500,\n",
       "   12911400,\n",
       "   3384000,\n",
       "   13937100,\n",
       "   14633100,\n",
       "   14345700,\n",
       "   6594300,\n",
       "   3279000,\n",
       "   2840100,\n",
       "   13837800,\n",
       "   15491400,\n",
       "   15272700,\n",
       "   11006400,\n",
       "   13276200,\n",
       "   13108800,\n",
       "   15493500,\n",
       "   14010600,\n",
       "   16217700,\n",
       "   15788100,\n",
       "   16349700,\n",
       "   7819800,\n",
       "   11295600,\n",
       "   14451600,\n",
       "   17658900,\n",
       "   17643600,\n",
       "   17347800,\n",
       "   16580400,\n",
       "   15803700,\n",
       "   6570300,\n",
       "   6195300,\n",
       "   5154300,\n",
       "   4813200,\n",
       "   4278300,\n",
       "   4107900,\n",
       "   11549700,\n",
       "   11819400,\n",
       "   10877700,\n",
       "   19165500,\n",
       "   18245700,\n",
       "   19199100,\n",
       "   18824100,\n",
       "   8999100,\n",
       "   18374400,\n",
       "   13371300,\n",
       "   12816900,\n",
       "   20987400,\n",
       "   13519200,\n",
       "   4457400,\n",
       "   14327100,\n",
       "   21602400,\n",
       "   13390500,\n",
       "   14319300,\n",
       "   22336200,\n",
       "   23085000,\n",
       "   16503600,\n",
       "   17400000,\n",
       "   12650100,\n",
       "   1474200,\n",
       "   2179500,\n",
       "   8283900,\n",
       "   8193300,\n",
       "   4938300,\n",
       "   8865000,\n",
       "   24336900,\n",
       "   23337900,\n",
       "   21714000,\n",
       "   17890200,\n",
       "   24207900,\n",
       "   7583100,\n",
       "   2507400,\n",
       "   19010700,\n",
       "   15521100,\n",
       "   24005700,\n",
       "   23389500,\n",
       "   22857900,\n",
       "   21312300,\n",
       "   14448000,\n",
       "   22304400,\n",
       "   21307200,\n",
       "   26447100,\n",
       "   24239100,\n",
       "   14193900,\n",
       "   25148400,\n",
       "   25341600,\n",
       "   23854500,\n",
       "   20454900,\n",
       "   27023400,\n",
       "   27205800,\n",
       "   27658800,\n",
       "   26862900,\n",
       "   14811300,\n",
       "   23191500,\n",
       "   26097900,\n",
       "   23950800,\n",
       "   13851600,\n",
       "   6936600,\n",
       "   23089800,\n",
       "   28314600,\n",
       "   6069900,\n",
       "   12609000,\n",
       "   28195800,\n",
       "   25615800,\n",
       "   14866500,\n",
       "   23496600,\n",
       "   12068400,\n",
       "   11712600,\n",
       "   4858200,\n",
       "   10857900,\n",
       "   15139500,\n",
       "   18663900,\n",
       "   22956600,\n",
       "   29707800,\n",
       "   28956000,\n",
       "   29140500,\n",
       "   26069700,\n",
       "   26783100,\n",
       "   26805600,\n",
       "   29565000,\n",
       "   29114700,\n",
       "   28917300,\n",
       "   28719900,\n",
       "   26604600,\n",
       "   27624600,\n",
       "   26999100,\n",
       "   28125000,\n",
       "   28131000,\n",
       "   25557600,\n",
       "   26049300,\n",
       "   25801200,\n",
       "   27964800,\n",
       "   26116800,\n",
       "   16383300,\n",
       "   16765500,\n",
       "   26063400,\n",
       "   30105300,\n",
       "   29066100,\n",
       "   20844900,\n",
       "   23102700,\n",
       "   29102100,\n",
       "   30437400,\n",
       "   30064500,\n",
       "   28451400,\n",
       "   29018700,\n",
       "   22638300,\n",
       "   26738100,\n",
       "   30212700,\n",
       "   28991700,\n",
       "   24525300,\n",
       "   11431200,\n",
       "   29403600,\n",
       "   29570100,\n",
       "   29967600,\n",
       "   24925200,\n",
       "   24436500,\n",
       "   11848800,\n",
       "   27066600,\n",
       "   27657000,\n",
       "   30271200,\n",
       "   25400100,\n",
       "   29979300,\n",
       "   29711700,\n",
       "   28376100,\n",
       "   27427800,\n",
       "   26241300,\n",
       "   21972600,\n",
       "   22262100,\n",
       "   26565300,\n",
       "   27031800,\n",
       "   29979900,\n",
       "   26008500,\n",
       "   24964500,\n",
       "   17664000,\n",
       "   17384400,\n",
       "   22791900,\n",
       "   22335000,\n",
       "   23786100,\n",
       "   27839400,\n",
       "   22850400,\n",
       "   25785600,\n",
       "   28520700,\n",
       "   27216300,\n",
       "   27462600,\n",
       "   23326500,\n",
       "   28013100,\n",
       "   25708500,\n",
       "   26599200,\n",
       "   24418500,\n",
       "   28086600,\n",
       "   26177100,\n",
       "   22393800,\n",
       "   25425000,\n",
       "   24721800,\n",
       "   27050400,\n",
       "   26134200,\n",
       "   26157300,\n",
       "   23240700,\n",
       "   22223100,\n",
       "   26337900,\n",
       "   24420000,\n",
       "   24750900,\n",
       "   18390900,\n",
       "   25272000,\n",
       "   24238500,\n",
       "   11129400,\n",
       "   20262600,\n",
       "   23914200,\n",
       "   21272400,\n",
       "   22236900,\n",
       "   23909700,\n",
       "   21874500,\n",
       "   23376000,\n",
       "   21655500,\n",
       "   15536400,\n",
       "   15162000,\n",
       "   9011700,\n",
       "   13053300,\n",
       "   22984200,\n",
       "   22898700,\n",
       "   21481800,\n",
       "   10477800,\n",
       "   9549300,\n",
       "   21801900,\n",
       "   24941700,\n",
       "   21947100,\n",
       "   21676500,\n",
       "   6610200,\n",
       "   15634800,\n",
       "   8424000,\n",
       "   23903100,\n",
       "   24162300,\n",
       "   21494700,\n",
       "   17989800,\n",
       "   16733700,\n",
       "   13609200,\n",
       "   15342600,\n",
       "   22151400,\n",
       "   23263500,\n",
       "   23153700,\n",
       "   20467800,\n",
       "   17924700,\n",
       "   2223000,\n",
       "   22504800,\n",
       "   22280700,\n",
       "   22045500,\n",
       "   9679500,\n",
       "   21183900,\n",
       "   19068900,\n",
       "   19389900,\n",
       "   21621900,\n",
       "   21032400,\n",
       "   20541600,\n",
       "   20658600,\n",
       "   20549400,\n",
       "   19177200,\n",
       "   17036700,\n",
       "   19461300,\n",
       "   19261500,\n",
       "   18722100,\n",
       "   18264000,\n",
       "   18532200,\n",
       "   18842100,\n",
       "   19089900,\n",
       "   15693600,\n",
       "   5874900,\n",
       "   18716400,\n",
       "   6489000,\n",
       "   18359700,\n",
       "   17485200,\n",
       "   1290600,\n",
       "   5935200,\n",
       "   16815600,\n",
       "   4072200,\n",
       "   13372800,\n",
       "   17430600,\n",
       "   12062700,\n",
       "   17260200,\n",
       "   17017800,\n",
       "   14811300,\n",
       "   1482300,\n",
       "   10523700,\n",
       "   7228200,\n",
       "   16048500,\n",
       "   16305900,\n",
       "   7407300,\n",
       "   5362800,\n",
       "   14190600,\n",
       "   11600400,\n",
       "   13267800,\n",
       "   8138400,\n",
       "   2129400,\n",
       "   3533400,\n",
       "   13605000,\n",
       "   14198100,\n",
       "   9846900,\n",
       "   7374900,\n",
       "   14616900,\n",
       "   11847900,\n",
       "   13132200,\n",
       "   11829600,\n",
       "   4027200,\n",
       "   12553200,\n",
       "   13459500,\n",
       "   13315500,\n",
       "   13330800,\n",
       "   13347300,\n",
       "   12824700,\n",
       "   12785400,\n",
       "   12664200,\n",
       "   12156300,\n",
       "   6571800,\n",
       "   7791900,\n",
       "   2823600,\n",
       "   9145500,\n",
       "   3337200,\n",
       "   1855800,\n",
       "   11579700,\n",
       "   12129600,\n",
       "   12270600,\n",
       "   12310500,\n",
       "   12042300,\n",
       "   11649000,\n",
       "   11796300,\n",
       "   11970600,\n",
       "   10623900,\n",
       "   4768500,\n",
       "   10201200,\n",
       "   10664100,\n",
       "   11796600,\n",
       "   10533000,\n",
       "   7824900,\n",
       "   10088100,\n",
       "   4516200,\n",
       "   2893800,\n",
       "   1434900,\n",
       "   2347200,\n",
       "   10913700,\n",
       "   11381700,\n",
       "   10573800,\n",
       "   2984700,\n",
       "   11238600,\n",
       "   11503500,\n",
       "   10723500,\n",
       "   6303900,\n",
       "   795600,\n",
       "   6589200,\n",
       "   3050400,\n",
       "   12188100,\n",
       "   12444000,\n",
       "   7867500,\n",
       "   4216500,\n",
       "   3196800,\n",
       "   9718200,\n",
       "   12121500,\n",
       "   11955300,\n",
       "   12162600,\n",
       "   11473500,\n",
       "   5161200,\n",
       "   1008900,\n",
       "   1595700,\n",
       "   1718700,\n",
       "   11185800,\n",
       "   4901700,\n",
       "   5733900,\n",
       "   5153400,\n",
       "   4538400,\n",
       "   1144500,\n",
       "   10530300,\n",
       "   9432900,\n",
       "   12796200,\n",
       "   10203600,\n",
       "   13725300,\n",
       "   13945800,\n",
       "   2457600,\n",
       "   14397000,\n",
       "   7187100,\n",
       "   14461800,\n",
       "   1568100,\n",
       "   1135500,\n",
       "   11865600,\n",
       "   10444200,\n",
       "   15023100,\n",
       "   14611500,\n",
       "   15086700,\n",
       "   14725500,\n",
       "   2745000,\n",
       "   6053100,\n",
       "   4960200,\n",
       "   4587600,\n",
       "   1518000,\n",
       "   3297000,\n",
       "   9227100,\n",
       "   10447200,\n",
       "   16646700,\n",
       "   17315700,\n",
       "   17323200,\n",
       "   17037900,\n",
       "   17466000,\n",
       "   17976300,\n",
       "   17396400,\n",
       "   14036700,\n",
       "   12795000,\n",
       "   18894600,\n",
       "   18839100,\n",
       "   18363000,\n",
       "   8327400,\n",
       "   19407300,\n",
       "   19947000,\n",
       "   19626600,\n",
       "   20216700,\n",
       "   20009400,\n",
       "   16299600,\n",
       "   17177700,\n",
       "   19048800,\n",
       "   17016300,\n",
       "   19542300,\n",
       "   4787700,\n",
       "   11312100,\n",
       "   12134700,\n",
       "   12526500,\n",
       "   11059500,\n",
       "   21784200,\n",
       "   18975300,\n",
       "   8641200,\n",
       "   7115400,\n",
       "   6036300,\n",
       "   11207100,\n",
       "   10030200,\n",
       "   8467500,\n",
       "   19275000,\n",
       "   7970700,\n",
       "   7954200,\n",
       "   8835000,\n",
       "   14102100,\n",
       "   4421400,\n",
       "   4685400,\n",
       "   21140100,\n",
       "   24066600,\n",
       "   23645400,\n",
       "   19863300,\n",
       "   24118500,\n",
       "   11792700,\n",
       "   7220100,\n",
       "   23931600,\n",
       "   12167100,\n",
       "   11472300,\n",
       "   5415900,\n",
       "   25627200,\n",
       "   21998700,\n",
       "   1883700,\n",
       "   7298400,\n",
       "   26345700,\n",
       "   22416300,\n",
       "   7081200,\n",
       "   14884800,\n",
       "   23233500,\n",
       "   23695800,\n",
       "   25850700,\n",
       "   14864700,\n",
       "   6911100,\n",
       "   4656900,\n",
       "   11715300,\n",
       "   18278100,\n",
       "   12190200,\n",
       "   27008700,\n",
       "   26332500,\n",
       "   17850000,\n",
       "   11868600,\n",
       "   15955800,\n",
       "   18385800,\n",
       "   14615400,\n",
       "   6195300,\n",
       "   12554700,\n",
       "   6264600,\n",
       "   17857500,\n",
       "   14662200,\n",
       "   17889000,\n",
       "   22924500,\n",
       "   26754600,\n",
       "   26181600,\n",
       "   19839900,\n",
       "   18432000,\n",
       "   28434000,\n",
       "   27472500,\n",
       "   25766100,\n",
       "   21588600,\n",
       "   23978100,\n",
       "   15113700,\n",
       "   15972900,\n",
       "   17266200,\n",
       "   4422900,\n",
       "   13764300,\n",
       "   11786400,\n",
       "   8926500,\n",
       "   12425400,\n",
       "   17643600,\n",
       "   10298100,\n",
       "   21759900,\n",
       "   19152300,\n",
       "   21975900,\n",
       "   21072000,\n",
       "   27630900,\n",
       "   28655700,\n",
       "   27655800,\n",
       "   17785800,\n",
       "   26459700,\n",
       "   23815800,\n",
       "   22467300,\n",
       "   21647700,\n",
       "   24267600,\n",
       "   17285400,\n",
       "   19038300,\n",
       "   17717400,\n",
       "   11853600,\n",
       "   12476100,\n",
       "   13052400,\n",
       "   19888500,\n",
       "   15193800,\n",
       "   14535300,\n",
       "   8339700,\n",
       "   14210400,\n",
       "   20143200,\n",
       "   19330800,\n",
       "   7890900,\n",
       "   10994700,\n",
       "   13620900,\n",
       "   17927400,\n",
       "   16162500,\n",
       "   18819300,\n",
       "   11256900,\n",
       "   19754100,\n",
       "   19571100,\n",
       "   17100000,\n",
       "   26874300,\n",
       "   24563400,\n",
       "   25043100,\n",
       "   19529700,\n",
       "   21109200,\n",
       "   21849600,\n",
       "   11055300,\n",
       "   12665700,\n",
       "   23088900,\n",
       "   25845600,\n",
       "   26680200,\n",
       "   20267400,\n",
       "   26445000,\n",
       "   24315300,\n",
       "   22895100,\n",
       "   25221900,\n",
       "   17155800,\n",
       "   12467400,\n",
       "   24087000,\n",
       "   25144200,\n",
       "   24497100,\n",
       "   22462500,\n",
       "   23605800,\n",
       "   25111200,\n",
       "   18053700,\n",
       "   16486200,\n",
       "   16481100,\n",
       "   23508000,\n",
       "   23539200,\n",
       "   23107800,\n",
       "   23380200,\n",
       "   24232200,\n",
       "   26109900,\n",
       "   24868800,\n",
       "   26202900,\n",
       "   26518800,\n",
       "   24882900,\n",
       "   24094200,\n",
       "   25530000,\n",
       "   24483600,\n",
       "   22698000,\n",
       "   21612300,\n",
       "   18427800,\n",
       "   13529700,\n",
       "   6141600,\n",
       "   24162300,\n",
       "   23952000,\n",
       "   19673100,\n",
       "   21616200,\n",
       "   22326900,\n",
       "   14624700,\n",
       "   22664400,\n",
       "   23739900,\n",
       "   22443900,\n",
       "   16842900,\n",
       "   20186100,\n",
       "   22979100,\n",
       "   23649900,\n",
       "   23814900,\n",
       "   15617100,\n",
       "   8975400,\n",
       "   10433100,\n",
       "   20368800,\n",
       "   10062900,\n",
       "   14480400,\n",
       "   8906100,\n",
       "   13613400,\n",
       "   20065200,\n",
       "   18730800,\n",
       "   15895200,\n",
       "   17464200,\n",
       "   20079000,\n",
       "   20598600,\n",
       "   21209100,\n",
       "   19339200,\n",
       "   15418500,\n",
       "   20067600,\n",
       "   19269600,\n",
       "   19775100,\n",
       "   13378200,\n",
       "   18930600,\n",
       "   8748000,\n",
       "   10370100,\n",
       "   13129200,\n",
       "   14238600,\n",
       "   19917900,\n",
       "   12109800,\n",
       "   19216800,\n",
       "   17180400,\n",
       "   6444900,\n",
       "   17672700,\n",
       "   17656200,\n",
       "   4293600,\n",
       "   9542400,\n",
       "   10830000,\n",
       "   19080900,\n",
       "   18137400,\n",
       "   17860800,\n",
       "   17702100,\n",
       "   17019300,\n",
       "   11507100,\n",
       "   4237800,\n",
       "   13851000,\n",
       "   7616400,\n",
       "   17491800,\n",
       "   17537700,\n",
       "   16339200,\n",
       "   15357600,\n",
       "   1891200,\n",
       "   16866300,\n",
       "   16863600,\n",
       "   16965600,\n",
       "   16375200,\n",
       "   15796500,\n",
       "   15123000,\n",
       "   14643900,\n",
       "   15338100,\n",
       "   13799100,\n",
       "   12225900,\n",
       "   7824600,\n",
       "   13911300,\n",
       "   13683600,\n",
       "   14463300,\n",
       "   14711100,\n",
       "   13968900,\n",
       "   13836000,\n",
       "   5304300,\n",
       "   9938700,\n",
       "   11010600,\n",
       "   7315200,\n",
       "   12894900,\n",
       "   6500400,\n",
       "   14498700,\n",
       "   13111500,\n",
       "   11877000,\n",
       "   6265500,\n",
       "   9430200,\n",
       "   9513600,\n",
       "   2233800,\n",
       "   9992400,\n",
       "   12088200,\n",
       "   2203500,\n",
       "   1239000,\n",
       "   12687300,\n",
       "   12694500,\n",
       "   10619100,\n",
       "   12662700,\n",
       "   3336300,\n",
       "   1410300,\n",
       "   11132400,\n",
       "   12737700,\n",
       "   10054800,\n",
       "   8691900,\n",
       "   5707500,\n",
       "   6139800,\n",
       "   1832700,\n",
       "   1394400,\n",
       "   1530600,\n",
       "   819600,\n",
       "   1477500,\n",
       "   3016200,\n",
       "   1544700,\n",
       "   1500600,\n",
       "   12006300,\n",
       "   11742900,\n",
       "   11602500,\n",
       "   11478000,\n",
       "   11530800,\n",
       "   9611100,\n",
       "   2235300,\n",
       "   11827200,\n",
       "   11599500,\n",
       "   11818200,\n",
       "   1551600,\n",
       "   1371000,\n",
       "   6581700,\n",
       "   11353800,\n",
       "   11883300,\n",
       "   12104100],\n",
       "  'cat': [1]},\n",
       " {'start': '1994-01-01 00:00:00',\n",
       "  'target': [12294300,\n",
       "   11696400,\n",
       "   11480400,\n",
       "   11420700,\n",
       "   12928800,\n",
       "   12384900,\n",
       "   12661800,\n",
       "   11452200,\n",
       "   9756300,\n",
       "   11471700,\n",
       "   12269700,\n",
       "   12397200,\n",
       "   6925500,\n",
       "   12757500,\n",
       "   11985300,\n",
       "   12255600,\n",
       "   10968000,\n",
       "   9228000,\n",
       "   3846600,\n",
       "   9927900,\n",
       "   12065400,\n",
       "   9109800,\n",
       "   1859100,\n",
       "   1922700,\n",
       "   1747800,\n",
       "   5605200,\n",
       "   11279400,\n",
       "   11698800,\n",
       "   8359200,\n",
       "   11806800,\n",
       "   11795400,\n",
       "   12058800,\n",
       "   11247300,\n",
       "   7971600,\n",
       "   11046900,\n",
       "   12301200,\n",
       "   10334400,\n",
       "   8269800,\n",
       "   11832900,\n",
       "   12226200,\n",
       "   12032400,\n",
       "   12561900,\n",
       "   12553200,\n",
       "   12913200,\n",
       "   10510800,\n",
       "   9413700,\n",
       "   12953400,\n",
       "   13225500,\n",
       "   11782500,\n",
       "   13544700,\n",
       "   10784700,\n",
       "   8221500,\n",
       "   9276300,\n",
       "   13869000,\n",
       "   13356900,\n",
       "   9737100,\n",
       "   10748700,\n",
       "   12615900,\n",
       "   8059200,\n",
       "   13754100,\n",
       "   14385600,\n",
       "   12757200,\n",
       "   9274800,\n",
       "   6200700,\n",
       "   13987500,\n",
       "   13108800,\n",
       "   15343800,\n",
       "   14288700,\n",
       "   15143700,\n",
       "   15057600,\n",
       "   15241500,\n",
       "   14668200,\n",
       "   13561500,\n",
       "   16066500,\n",
       "   16330500,\n",
       "   16581900,\n",
       "   13862400,\n",
       "   15507900,\n",
       "   17012100,\n",
       "   17459700,\n",
       "   17567700,\n",
       "   17468100,\n",
       "   17459700,\n",
       "   16766400,\n",
       "   8936700,\n",
       "   8368500,\n",
       "   5919900,\n",
       "   4887000,\n",
       "   3045600,\n",
       "   8977800,\n",
       "   10405200,\n",
       "   10303800,\n",
       "   13499400,\n",
       "   18872400,\n",
       "   17406900,\n",
       "   17477400,\n",
       "   12145200,\n",
       "   12139500,\n",
       "   13229100,\n",
       "   11156700,\n",
       "   20277300,\n",
       "   20262300,\n",
       "   14482200,\n",
       "   12927000,\n",
       "   18389100,\n",
       "   16000500,\n",
       "   21045900,\n",
       "   21332100,\n",
       "   22163100,\n",
       "   22007100,\n",
       "   15922800,\n",
       "   22765800,\n",
       "   4259100,\n",
       "   1546500,\n",
       "   1881900,\n",
       "   14885700,\n",
       "   23048400,\n",
       "   7080300,\n",
       "   10037400,\n",
       "   23496300,\n",
       "   22758300,\n",
       "   22450200,\n",
       "   15636900,\n",
       "   22077600,\n",
       "   11318400,\n",
       "   20640000,\n",
       "   23395800,\n",
       "   17827200,\n",
       "   24942900,\n",
       "   21195600,\n",
       "   18225600,\n",
       "   14821500,\n",
       "   22965000,\n",
       "   23644200,\n",
       "   26229900,\n",
       "   26125200,\n",
       "   21087000,\n",
       "   12887400,\n",
       "   25786500,\n",
       "   26157300,\n",
       "   24976800,\n",
       "   24765000,\n",
       "   27244500,\n",
       "   28198200,\n",
       "   27895800,\n",
       "   27724200,\n",
       "   8931900,\n",
       "   26639700,\n",
       "   24130200,\n",
       "   24851400,\n",
       "   25288200,\n",
       "   23970300,\n",
       "   27931500,\n",
       "   23909400,\n",
       "   9204000,\n",
       "   13224300,\n",
       "   28290300,\n",
       "   21903000,\n",
       "   15856200,\n",
       "   23028000,\n",
       "   20535600,\n",
       "   10686900,\n",
       "   6126600,\n",
       "   16491600,\n",
       "   21417300,\n",
       "   25452900,\n",
       "   27542700,\n",
       "   30037200,\n",
       "   29190300,\n",
       "   28870200,\n",
       "   25145400,\n",
       "   26626200,\n",
       "   29409600,\n",
       "   29533200,\n",
       "   29497500,\n",
       "   29820900,\n",
       "   28475700,\n",
       "   23920800,\n",
       "   22660800,\n",
       "   28402800,\n",
       "   29044800,\n",
       "   28764300,\n",
       "   26391900,\n",
       "   28047000,\n",
       "   28146600,\n",
       "   27935400,\n",
       "   26976900,\n",
       "   25355100,\n",
       "   28476300,\n",
       "   30239700,\n",
       "   29716500,\n",
       "   28795200,\n",
       "   28687500,\n",
       "   12173100,\n",
       "   29971500,\n",
       "   30558900,\n",
       "   29731200,\n",
       "   27243000,\n",
       "   29494200,\n",
       "   25986000,\n",
       "   25997400,\n",
       "   25338600,\n",
       "   28632000,\n",
       "   26789100,\n",
       "   22641000,\n",
       "   30047400,\n",
       "   29877600,\n",
       "   30147600,\n",
       "   28665900,\n",
       "   11199600,\n",
       "   23355300,\n",
       "   28164300,\n",
       "   27692700,\n",
       "   29750700,\n",
       "   23768700,\n",
       "   28172700,\n",
       "   28968600,\n",
       "   28589100,\n",
       "   28415700,\n",
       "   29282700,\n",
       "   28465500,\n",
       "   26365500,\n",
       "   24573600,\n",
       "   26754600,\n",
       "   27718500,\n",
       "   26898600,\n",
       "   27325800,\n",
       "   26124600,\n",
       "   27220500,\n",
       "   26349300,\n",
       "   28802400,\n",
       "   17984400,\n",
       "   28392300,\n",
       "   28986600,\n",
       "   29370300,\n",
       "   28458600,\n",
       "   28086900,\n",
       "   28413300,\n",
       "   27065400,\n",
       "   28877700,\n",
       "   26353800,\n",
       "   26097000,\n",
       "   27891300,\n",
       "   27702600,\n",
       "   22014900,\n",
       "   24916500,\n",
       "   23562000,\n",
       "   28148100,\n",
       "   27544800,\n",
       "   26211900,\n",
       "   25159200,\n",
       "   24712200,\n",
       "   27051300,\n",
       "   27626100,\n",
       "   25110600,\n",
       "   25417500,\n",
       "   25887300,\n",
       "   26485800,\n",
       "   27226200,\n",
       "   13280100,\n",
       "   25746900,\n",
       "   24516300,\n",
       "   24124800,\n",
       "   15732300,\n",
       "   21576300,\n",
       "   20271300,\n",
       "   25655400,\n",
       "   25197600,\n",
       "   24811800,\n",
       "   21753000,\n",
       "   11024100,\n",
       "   23833200,\n",
       "   24500700,\n",
       "   24410700,\n",
       "   23966400,\n",
       "   7115700,\n",
       "   19154100,\n",
       "   25218900,\n",
       "   24290400,\n",
       "   24615600,\n",
       "   15783300,\n",
       "   3677100,\n",
       "   4671300,\n",
       "   11840100,\n",
       "   23947200,\n",
       "   23954400,\n",
       "   21082800,\n",
       "   19829700,\n",
       "   18897900,\n",
       "   15993300,\n",
       "   17540700,\n",
       "   22544700,\n",
       "   23143800,\n",
       "   22007400,\n",
       "   21657600,\n",
       "   13743300,\n",
       "   12715500,\n",
       "   23651100,\n",
       "   23022000,\n",
       "   20565900,\n",
       "   18351900,\n",
       "   22564200,\n",
       "   22007100,\n",
       "   16770000,\n",
       "   22336500,\n",
       "   20885700,\n",
       "   21684900,\n",
       "   21341700,\n",
       "   21287100,\n",
       "   20941800,\n",
       "   15675600,\n",
       "   20125200,\n",
       "   20358300,\n",
       "   16504200,\n",
       "   19043700,\n",
       "   18693300,\n",
       "   14943600,\n",
       "   17036700,\n",
       "   5926200,\n",
       "   9111300,\n",
       "   19298400,\n",
       "   10853100,\n",
       "   19050300,\n",
       "   14026500,\n",
       "   3642600,\n",
       "   13585800,\n",
       "   18805800,\n",
       "   9470100,\n",
       "   17085600,\n",
       "   17950200,\n",
       "   17167200,\n",
       "   16257300,\n",
       "   17799000,\n",
       "   14420100,\n",
       "   13660800,\n",
       "   16733700,\n",
       "   17089200,\n",
       "   17019000,\n",
       "   16691400,\n",
       "   7906500,\n",
       "   16321500,\n",
       "   15833100,\n",
       "   15625500,\n",
       "   10937400,\n",
       "   14346900,\n",
       "   3834300,\n",
       "   12186300,\n",
       "   14234700,\n",
       "   15144600,\n",
       "   11480400,\n",
       "   14876100,\n",
       "   15326400,\n",
       "   10137300,\n",
       "   12209400,\n",
       "   12636300,\n",
       "   13154100,\n",
       "   14175900,\n",
       "   14317800,\n",
       "   14065500,\n",
       "   13893300,\n",
       "   14248800,\n",
       "   13638300,\n",
       "   13777200,\n",
       "   13668600,\n",
       "   11205300,\n",
       "   10898400,\n",
       "   7512600,\n",
       "   4707300,\n",
       "   11379600,\n",
       "   3204000,\n",
       "   5015700,\n",
       "   13649700,\n",
       "   12783300,\n",
       "   12943500,\n",
       "   13019100,\n",
       "   13045800,\n",
       "   12784200,\n",
       "   12526800,\n",
       "   12294300,\n",
       "   11292000,\n",
       "   11753400,\n",
       "   12420900,\n",
       "   12415500,\n",
       "   11706900,\n",
       "   9786900,\n",
       "   11871600,\n",
       "   12011400,\n",
       "   11816400,\n",
       "   1407900,\n",
       "   612900,\n",
       "   9308400,\n",
       "   10729200,\n",
       "   12431700,\n",
       "   10408800,\n",
       "   10326600,\n",
       "   12800700,\n",
       "   12255900,\n",
       "   11570700,\n",
       "   3717000,\n",
       "   911700,\n",
       "   4633500,\n",
       "   11155500,\n",
       "   13045500,\n",
       "   10807800,\n",
       "   6151500,\n",
       "   9739800,\n",
       "   10708500,\n",
       "   5825700,\n",
       "   13034400,\n",
       "   12754500,\n",
       "   12831600,\n",
       "   12646500,\n",
       "   8153100,\n",
       "   1254300,\n",
       "   3291000,\n",
       "   1933800,\n",
       "   10363800,\n",
       "   5782200,\n",
       "   4446600,\n",
       "   6757500,\n",
       "   4007400,\n",
       "   2615400,\n",
       "   13910100,\n",
       "   13760700,\n",
       "   14299800,\n",
       "   11938200,\n",
       "   14619300,\n",
       "   13511400,\n",
       "   7156500,\n",
       "   14855700,\n",
       "   7705500,\n",
       "   14615400,\n",
       "   2909100,\n",
       "   2577900,\n",
       "   14306700,\n",
       "   15907500,\n",
       "   15986700,\n",
       "   15889800,\n",
       "   16388100,\n",
       "   14315700,\n",
       "   3774000,\n",
       "   3531000,\n",
       "   3960900,\n",
       "   3474300,\n",
       "   8219400,\n",
       "   10349400,\n",
       "   7829700,\n",
       "   16044000,\n",
       "   17543700,\n",
       "   17061600,\n",
       "   17610000,\n",
       "   18141900,\n",
       "   18568800,\n",
       "   18976800,\n",
       "   18470700,\n",
       "   6569400,\n",
       "   13199400,\n",
       "   19800600,\n",
       "   19922100,\n",
       "   19676100,\n",
       "   15988200,\n",
       "   19751700,\n",
       "   20716800,\n",
       "   20593200,\n",
       "   21276600,\n",
       "   21376500,\n",
       "   16981200,\n",
       "   20181600,\n",
       "   19010400,\n",
       "   21411000,\n",
       "   21012900,\n",
       "   2557200,\n",
       "   8232000,\n",
       "   13069500,\n",
       "   7124700,\n",
       "   17016000,\n",
       "   21529800,\n",
       "   19350900,\n",
       "   21531600,\n",
       "   16974300,\n",
       "   12106800,\n",
       "   21103800,\n",
       "   11939100,\n",
       "   16539900,\n",
       "   10547100,\n",
       "   6076800,\n",
       "   4707900,\n",
       "   14668500,\n",
       "   16734000,\n",
       "   19379400,\n",
       "   2360700,\n",
       "   23871300,\n",
       "   24843600,\n",
       "   24394500,\n",
       "   24715500,\n",
       "   24742200,\n",
       "   8311200,\n",
       "   5327100,\n",
       "   7634100,\n",
       "   9101100,\n",
       "   6400200,\n",
       "   18695400,\n",
       "   25838700,\n",
       "   19527000,\n",
       "   4411200,\n",
       "   15481200,\n",
       "   27293400,\n",
       "   26756700,\n",
       "   6822000,\n",
       "   20590500,\n",
       "   20417100,\n",
       "   227400,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   13416600,\n",
       "   16239300,\n",
       "   27801300,\n",
       "   23796600,\n",
       "   28562100,\n",
       "   22761900,\n",
       "   15853500,\n",
       "   18588000,\n",
       "   15361800,\n",
       "   18405000,\n",
       "   25760400,\n",
       "   18301500,\n",
       "   17646300,\n",
       "   17441700,\n",
       "   13533600,\n",
       "   16240800,\n",
       "   16882800,\n",
       "   23508900,\n",
       "   25757100,\n",
       "   27001800,\n",
       "   27473400,\n",
       "   12961800,\n",
       "   28308900,\n",
       "   27184800,\n",
       "   11740800,\n",
       "   12189600,\n",
       "   20966100,\n",
       "   14370600,\n",
       "   23312700,\n",
       "   18302100,\n",
       "   7596900,\n",
       "   9218400,\n",
       "   13992900,\n",
       "   20244000,\n",
       "   24784200,\n",
       "   24651600,\n",
       "   7967400,\n",
       "   16782600,\n",
       "   7998900,\n",
       "   26454600,\n",
       "   27881100,\n",
       "   22875000,\n",
       "   29525400,\n",
       "   26943900,\n",
       "   28493400,\n",
       "   29438400,\n",
       "   23142300,\n",
       "   23147700,\n",
       "   25493100,\n",
       "   18577200,\n",
       "   28387200,\n",
       "   20507400,\n",
       "   17089200,\n",
       "   14970900,\n",
       "   21013800,\n",
       "   20129100,\n",
       "   26164200,\n",
       "   25784400,\n",
       "   11517600,\n",
       "   20087100,\n",
       "   21368400,\n",
       "   22909500,\n",
       "   11769000,\n",
       "   8127000,\n",
       "   17779200,\n",
       "   6798600,\n",
       "   7579800,\n",
       "   22975200,\n",
       "   20407200,\n",
       "   20134500,\n",
       "   22204800,\n",
       "   17346300,\n",
       "   22748700,\n",
       "   24685800,\n",
       "   28866600,\n",
       "   27252000,\n",
       "   21265800,\n",
       "   27160200,\n",
       "   15327600,\n",
       "   21873900,\n",
       "   11631300,\n",
       "   28728300,\n",
       "   28219500,\n",
       "   24360000,\n",
       "   27635400,\n",
       "   26771100,\n",
       "   27132300,\n",
       "   24645900,\n",
       "   24799200,\n",
       "   27068100,\n",
       "   19726200,\n",
       "   21894300,\n",
       "   27297000,\n",
       "   27285300,\n",
       "   22454700,\n",
       "   20709900,\n",
       "   22910700,\n",
       "   21647400,\n",
       "   23399400,\n",
       "   12240900,\n",
       "   15749400,\n",
       "   24358500,\n",
       "   22678800,\n",
       "   26293500,\n",
       "   26415300,\n",
       "   23736300,\n",
       "   26017800,\n",
       "   26242800,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   10485760,\n",
       "   9982800,\n",
       "   18261600,\n",
       "   17931300,\n",
       "   12732600,\n",
       "   16809000,\n",
       "   6023700,\n",
       "   13909800,\n",
       "   22590900,\n",
       "   22619700,\n",
       "   21943200,\n",
       "   18473400,\n",
       "   13981200,\n",
       "   22091100,\n",
       "   18615600,\n",
       "   13130400,\n",
       "   20071200,\n",
       "   19720800,\n",
       "   21200700,\n",
       "   20008500,\n",
       "   19413300,\n",
       "   18502500,\n",
       "   15855000,\n",
       "   19506900,\n",
       "   18583800,\n",
       "   12470700,\n",
       "   18137400,\n",
       "   15034500,\n",
       "   19314000,\n",
       "   17109300,\n",
       "   17235600,\n",
       "   18518700,\n",
       "   19022700,\n",
       "   15603300,\n",
       "   17728200,\n",
       "   18460200,\n",
       "   20162100,\n",
       "   19273800,\n",
       "   8524800,\n",
       "   14907900,\n",
       "   17978100,\n",
       "   10467600,\n",
       "   16605000,\n",
       "   18237300,\n",
       "   15203100,\n",
       "   18714600,\n",
       "   18348300,\n",
       "   18140400,\n",
       "   16140600,\n",
       "   14024400,\n",
       "   17414700,\n",
       "   17151900,\n",
       "   17460300,\n",
       "   17093700,\n",
       "   16530300,\n",
       "   16136400,\n",
       "   15848700,\n",
       "   15821400,\n",
       "   13202100,\n",
       "   15713700,\n",
       "   14971500,\n",
       "   11298600,\n",
       "   14823000,\n",
       "   14093400,\n",
       "   13886700,\n",
       "   14441700,\n",
       "   14450100,\n",
       "   14175600,\n",
       "   10925400,\n",
       "   9182700,\n",
       "   13284600,\n",
       "   13293000,\n",
       "   13161000,\n",
       "   14288400,\n",
       "   11331000,\n",
       "   11586600,\n",
       "   13000500,\n",
       "   12277500,\n",
       "   12344700,\n",
       "   10732200,\n",
       "   6451800,\n",
       "   3891000,\n",
       "   5336100,\n",
       "   4105800,\n",
       "   11667900,\n",
       "   12696000,\n",
       "   8698800,\n",
       "   12199200,\n",
       "   2287200,\n",
       "   3490500,\n",
       "   12620100,\n",
       "   12828300,\n",
       "   12328200,\n",
       "   12200100,\n",
       "   6339900,\n",
       "   6981000,\n",
       "   2153700,\n",
       "   1685700,\n",
       "   1451400,\n",
       "   1236900,\n",
       "   5262600,\n",
       "   8252400,\n",
       "   921000,\n",
       "   6650400,\n",
       "   12210300,\n",
       "   12130200,\n",
       "   12087900,\n",
       "   12020400,\n",
       "   12134400,\n",
       "   8948400,\n",
       "   2364900,\n",
       "   11969100,\n",
       "   12061500,\n",
       "   12093000,\n",
       "   3227700,\n",
       "   1742400,\n",
       "   12027300,\n",
       "   11946900,\n",
       "   12409200,\n",
       "   12015600],\n",
       "  'cat': [2]},\n",
       " {'start': '1994-01-01 00:00:00',\n",
       "  'target': [7257900,\n",
       "   9165000,\n",
       "   10383900,\n",
       "   8267700,\n",
       "   12787800,\n",
       "   12032400,\n",
       "   12429900,\n",
       "   11448000,\n",
       "   9078600,\n",
       "   11133000,\n",
       "   11975400,\n",
       "   12030000,\n",
       "   5292000,\n",
       "   12425400,\n",
       "   11407500,\n",
       "   11361000,\n",
       "   11485500,\n",
       "   9136500,\n",
       "   4239000,\n",
       "   9159300,\n",
       "   12096300,\n",
       "   11373000,\n",
       "   2056800,\n",
       "   2777100,\n",
       "   3424500,\n",
       "   3486300,\n",
       "   6575700,\n",
       "   11363100,\n",
       "   9123000,\n",
       "   11626200,\n",
       "   11718600,\n",
       "   11922900,\n",
       "   11354100,\n",
       "   7910100,\n",
       "   11506800,\n",
       "   12183000,\n",
       "   6939000,\n",
       "   8418600,\n",
       "   11796600,\n",
       "   12100500,\n",
       "   10829400,\n",
       "   12225600,\n",
       "   12327900,\n",
       "   12281700,\n",
       "   9999000,\n",
       "   9108000,\n",
       "   12702600,\n",
       "   12971100,\n",
       "   11265300,\n",
       "   13205400,\n",
       "   10101900,\n",
       "   8417100,\n",
       "   8680500,\n",
       "   13536900,\n",
       "   11056500,\n",
       "   10705800,\n",
       "   9862500,\n",
       "   12131100,\n",
       "   11079600,\n",
       "   13687500,\n",
       "   14057400,\n",
       "   13416000,\n",
       "   6662100,\n",
       "   2907300,\n",
       "   10042500,\n",
       "   12613500,\n",
       "   14904000,\n",
       "   14346300,\n",
       "   14494800,\n",
       "   14899800,\n",
       "   14120100,\n",
       "   14049300,\n",
       "   14121300,\n",
       "   15549000,\n",
       "   16053900,\n",
       "   5681100,\n",
       "   13787100,\n",
       "   13456200,\n",
       "   16382100,\n",
       "   17070300,\n",
       "   17009100,\n",
       "   16753500,\n",
       "   17014800,\n",
       "   15699000,\n",
       "   8626200,\n",
       "   6813300,\n",
       "   5342100,\n",
       "   2829900,\n",
       "   3249600,\n",
       "   8116500,\n",
       "   11670900,\n",
       "   10901400,\n",
       "   11943000,\n",
       "   18591300,\n",
       "   18199800,\n",
       "   18569100,\n",
       "   17652000,\n",
       "   11055900,\n",
       "   18594300,\n",
       "   12359400,\n",
       "   19264200,\n",
       "   19584000,\n",
       "   14627400,\n",
       "   12498300,\n",
       "   14036700,\n",
       "   17657700,\n",
       "   20637900,\n",
       "   16489500,\n",
       "   21952500,\n",
       "   21982800,\n",
       "   19113600,\n",
       "   22402500,\n",
       "   5201400,\n",
       "   1855800,\n",
       "   1895700,\n",
       "   12776100,\n",
       "   19230600,\n",
       "   5679600,\n",
       "   11034900,\n",
       "   23549100,\n",
       "   22984200,\n",
       "   21553800,\n",
       "   21604500,\n",
       "   23721000,\n",
       "   9254400,\n",
       "   12091200,\n",
       "   23208300,\n",
       "   15050400,\n",
       "   24586800,\n",
       "   21342000,\n",
       "   21774900,\n",
       "   16854900,\n",
       "   21657900,\n",
       "   18320100,\n",
       "   23792400,\n",
       "   25838100,\n",
       "   22422600,\n",
       "   11931000,\n",
       "   25228800,\n",
       "   25807200,\n",
       "   23803500,\n",
       "   22782000,\n",
       "   26741700,\n",
       "   27501600,\n",
       "   27315900,\n",
       "   26704500,\n",
       "   12846900,\n",
       "   24964800,\n",
       "   22756800,\n",
       "   23646300,\n",
       "   17722500,\n",
       "   21180300,\n",
       "   27066600,\n",
       "   27919500,\n",
       "   5167500,\n",
       "   12533100,\n",
       "   27732600,\n",
       "   19652100,\n",
       "   19933800,\n",
       "   24236700,\n",
       "   12122400,\n",
       "   10848300,\n",
       "   4756500,\n",
       "   12299400,\n",
       "   16302900,\n",
       "   24125400,\n",
       "   26838000,\n",
       "   29751600,\n",
       "   28920000,\n",
       "   28160700,\n",
       "   22637400,\n",
       "   26681700,\n",
       "   27384300,\n",
       "   28860600,\n",
       "   29374500,\n",
       "   29262000,\n",
       "   28525500,\n",
       "   25377300,\n",
       "   26916600,\n",
       "   28134000,\n",
       "   28320600,\n",
       "   28176900,\n",
       "   25102500,\n",
       "   25992600,\n",
       "   25709100,\n",
       "   25731000,\n",
       "   27206100,\n",
       "   15667200,\n",
       "   26467800,\n",
       "   29537100,\n",
       "   29504400,\n",
       "   28397700,\n",
       "   28098900,\n",
       "   25830000,\n",
       "   28950600,\n",
       "   30086700,\n",
       "   28697700,\n",
       "   27637800,\n",
       "   28997100,\n",
       "   15008100,\n",
       "   27467700,\n",
       "   30205200,\n",
       "   29399700,\n",
       "   26527800,\n",
       "   11098800,\n",
       "   29596200,\n",
       "   29002200,\n",
       "   29322900,\n",
       "   20556900,\n",
       "   9430800,\n",
       "   23970900,\n",
       "   28377900,\n",
       "   27954900,\n",
       "   28952400,\n",
       "   25684200,\n",
       "   29383500,\n",
       "   27849900,\n",
       "   28600200,\n",
       "   28768800,\n",
       "   28647600,\n",
       "   21870000,\n",
       "   14700900,\n",
       "   18371700,\n",
       "   19969200,\n",
       "   28340400,\n",
       "   23625900,\n",
       "   27163500,\n",
       "   24965700,\n",
       "   23286300,\n",
       "   19945500,\n",
       "   29021700,\n",
       "   24269700,\n",
       "   28241100,\n",
       "   29286300,\n",
       "   25012500,\n",
       "   28037400,\n",
       "   28278900,\n",
       "   28346700,\n",
       "   27484500,\n",
       "   28958400,\n",
       "   29597400,\n",
       "   28242900,\n",
       "   28417800,\n",
       "   28221000,\n",
       "   20437200,\n",
       "   24381300,\n",
       "   26655300,\n",
       "   28154400,\n",
       "   27707400,\n",
       "   26749500,\n",
       "   25839300,\n",
       "   24555900,\n",
       "   25607400,\n",
       "   25999800,\n",
       "   27303000,\n",
       "   25642800,\n",
       "   25329000,\n",
       "   26264400,\n",
       "   27304500,\n",
       "   17297400,\n",
       "   25613700,\n",
       "   23084400,\n",
       "   21042600,\n",
       "   19082100,\n",
       "   19940400,\n",
       "   23848500,\n",
       "   26405100,\n",
       "   24404400,\n",
       "   23316000,\n",
       "   22141800,\n",
       "   7430400,\n",
       "   25311600,\n",
       "   25278300,\n",
       "   25312200,\n",
       "   23489700,\n",
       "   7951800,\n",
       "   15440400,\n",
       "   21817200,\n",
       "   25146000,\n",
       "   24181800,\n",
       "   22989600,\n",
       "   3518400,\n",
       "   11523000,\n",
       "   12052500,\n",
       "   24214500,\n",
       "   24729300,\n",
       "   22934100,\n",
       "   20370900,\n",
       "   19077900,\n",
       "   14892300,\n",
       "   18739500,\n",
       "   22951500,\n",
       "   23607300,\n",
       "   22163400,\n",
       "   21738000,\n",
       "   17935200,\n",
       "   4136100,\n",
       "   23401800,\n",
       "   22908000,\n",
       "   22060800,\n",
       "   15943800,\n",
       "   22422600,\n",
       "   21105900,\n",
       "   21031500,\n",
       "   22147800,\n",
       "   21587100,\n",
       "   21457800,\n",
       "   21168600,\n",
       "   20979300,\n",
       "   20325900,\n",
       "   16911000,\n",
       "   19867200,\n",
       "   19959300,\n",
       "   17923500,\n",
       "   18708900,\n",
       "   18626100,\n",
       "   16439100,\n",
       "   18307200,\n",
       "   4290000,\n",
       "   6869400,\n",
       "   19330800,\n",
       "   8498400,\n",
       "   18774300,\n",
       "   16764000,\n",
       "   2035500,\n",
       "   7881000,\n",
       "   18445800,\n",
       "   8505900,\n",
       "   15448500,\n",
       "   17672400,\n",
       "   14785500,\n",
       "   16167000,\n",
       "   17493900,\n",
       "   13339800,\n",
       "   6967500,\n",
       "   15773700,\n",
       "   14175000,\n",
       "   16593000,\n",
       "   16740000,\n",
       "   8943000,\n",
       "   12188100,\n",
       "   15360000,\n",
       "   15337800,\n",
       "   9318300,\n",
       "   14372100,\n",
       "   3947100,\n",
       "   8666100,\n",
       "   13004100,\n",
       "   14798700,\n",
       "   14448000,\n",
       "   12414600,\n",
       "   14904000,\n",
       "   11217300,\n",
       "   12883200,\n",
       "   11121900,\n",
       "   7073400,\n",
       "   14031300,\n",
       "   13875600,\n",
       "   13459800,\n",
       "   13680000,\n",
       "   13806300,\n",
       "   13110000,\n",
       "   13238100,\n",
       "   13161600,\n",
       "   10710000,\n",
       "   10740600,\n",
       "   3265500,\n",
       "   3174600,\n",
       "   11728800,\n",
       "   1368600,\n",
       "   3887400,\n",
       "   13126200,\n",
       "   12369600,\n",
       "   12512400,\n",
       "   12691800,\n",
       "   12594600,\n",
       "   12184200,\n",
       "   12039600,\n",
       "   12176400,\n",
       "   10108500,\n",
       "   8232900,\n",
       "   11158200,\n",
       "   11792700,\n",
       "   11021400,\n",
       "   10606200,\n",
       "   11114400,\n",
       "   11693100,\n",
       "   10319100,\n",
       "   2504400,\n",
       "   942600,\n",
       "   8598600,\n",
       "   10479900,\n",
       "   11811300,\n",
       "   10958100,\n",
       "   5262000,\n",
       "   12279000,\n",
       "   11771100,\n",
       "   10112100,\n",
       "   3111600,\n",
       "   1329600,\n",
       "   8762700,\n",
       "   8341800,\n",
       "   12673800,\n",
       "   10713000,\n",
       "   7017000,\n",
       "   4563900,\n",
       "   6621600,\n",
       "   6335100,\n",
       "   12567900,\n",
       "   12325200,\n",
       "   12458700,\n",
       "   11656200,\n",
       "   6148200,\n",
       "   1155900,\n",
       "   2863200,\n",
       "   2462700,\n",
       "   10184400,\n",
       "   6582900,\n",
       "   7332000,\n",
       "   7653600,\n",
       "   5426400,\n",
       "   2552700,\n",
       "   7785300,\n",
       "   13246500,\n",
       "   14114100,\n",
       "   11528100,\n",
       "   14118600,\n",
       "   14241600,\n",
       "   5906700,\n",
       "   14670000,\n",
       "   7531500,\n",
       "   14516100,\n",
       "   1942800,\n",
       "   3308400,\n",
       "   15037500,\n",
       "   15135600,\n",
       "   15593100,\n",
       "   15304200,\n",
       "   15667500,\n",
       "   14853900,\n",
       "   3445500,\n",
       "   3244800,\n",
       "   4366500,\n",
       "   2976300,\n",
       "   2580000,\n",
       "   5515800,\n",
       "   7489800,\n",
       "   15272100,\n",
       "   16985100,\n",
       "   18134100,\n",
       "   17720400,\n",
       "   17632800,\n",
       "   18036300,\n",
       "   18418500,\n",
       "   17821800,\n",
       "   4029600,\n",
       "   13943400,\n",
       "   19508700,\n",
       "   19518000,\n",
       "   19023600,\n",
       "   17109300,\n",
       "   19736100,\n",
       "   20536800,\n",
       "   20286000,\n",
       "   20886600,\n",
       "   20409900,\n",
       "   19035900,\n",
       "   19445700,\n",
       "   19528200,\n",
       "   20731200,\n",
       "   19354200,\n",
       "   3821400,\n",
       "   13899000,\n",
       "   16875000,\n",
       "   11238000,\n",
       "   19725600,\n",
       "   22278000,\n",
       "   21763500,\n",
       "   20640600,\n",
       "   7316100,\n",
       "   5536800,\n",
       "   13585200,\n",
       "   7843800,\n",
       "   13314600,\n",
       "   10935600,\n",
       "   9182100,\n",
       "   5590200,\n",
       "   14988600,\n",
       "   14768400,\n",
       "   4274700,\n",
       "   2038800,\n",
       "   24110100,\n",
       "   24893400,\n",
       "   24422700,\n",
       "   24663600,\n",
       "   24137100,\n",
       "   7368300,\n",
       "   5855700,\n",
       "   19829100,\n",
       "   9964800,\n",
       "   7838700,\n",
       "   11292300,\n",
       "   26509800,\n",
       "   19806000,\n",
       "   1862700,\n",
       "   11924100,\n",
       "   27518400,\n",
       "   25847400,\n",
       "   5419500,\n",
       "   19426200,\n",
       "   22668900,\n",
       "   25628400,\n",
       "   26478600,\n",
       "   26621100,\n",
       "   9802200,\n",
       "   11140500,\n",
       "   16439100,\n",
       "   26551500,\n",
       "   17919600,\n",
       "   28003200,\n",
       "   26105700,\n",
       "   16722300,\n",
       "   16379100,\n",
       "   16092900,\n",
       "   16351800,\n",
       "   23083200,\n",
       "   15719700,\n",
       "   15969000,\n",
       "   10374300,\n",
       "   17876400,\n",
       "   18076200,\n",
       "   16822200,\n",
       "   21790200,\n",
       "   26835900,\n",
       "   25025700,\n",
       "   24900000,\n",
       "   12803400,\n",
       "   28767300,\n",
       "   28416900,\n",
       "   23595900,\n",
       "   9561000,\n",
       "   24503100,\n",
       "   11779800,\n",
       "   21123900,\n",
       "   17974500,\n",
       "   5442300,\n",
       "   11273100,\n",
       "   14487300,\n",
       "   18722700,\n",
       "   13185600,\n",
       "   19697400,\n",
       "   10354200,\n",
       "   19330500,\n",
       "   11977500,\n",
       "   26622000,\n",
       "   25147200,\n",
       "   19388700,\n",
       "   29259600,\n",
       "   26468400,\n",
       "   26377200,\n",
       "   28750200,\n",
       "   23045100,\n",
       "   16147200,\n",
       "   28017300,\n",
       "   21606000,\n",
       "   26443800,\n",
       "   17331900,\n",
       "   11927100,\n",
       "   13364100,\n",
       "   16234500,\n",
       "   13844700,\n",
       "   22890000,\n",
       "   14535600,\n",
       "   13284900,\n",
       "   20533200,\n",
       "   27738300,\n",
       "   26356200,\n",
       "   14948100,\n",
       "   10522800,\n",
       "   19497300,\n",
       "   9544200,\n",
       "   2955300,\n",
       "   17475900,\n",
       "   24829800,\n",
       "   18048000,\n",
       "   23492400,\n",
       "   24024000,\n",
       "   20762100,\n",
       "   22024800,\n",
       "   25947300,\n",
       "   28554300,\n",
       "   23767800,\n",
       "   25162800,\n",
       "   14247300,\n",
       "   17597700,\n",
       "   12820800,\n",
       "   28212300,\n",
       "   27569400,\n",
       "   25740900,\n",
       "   28198800,\n",
       "   27761100,\n",
       "   28135500,\n",
       "   26523300,\n",
       "   24669300,\n",
       "   27819300,\n",
       "   21189300,\n",
       "   24963600,\n",
       "   27974100,\n",
       "   26997600,\n",
       "   18592800,\n",
       "   26047200,\n",
       "   27589800,\n",
       "   20520900,\n",
       "   14596200,\n",
       "   12325500,\n",
       "   21655500,\n",
       "   24986400,\n",
       "   25579800,\n",
       "   26339100,\n",
       "   26449200,\n",
       "   25968300,\n",
       "   26679300,\n",
       "   26683500,\n",
       "   27051600,\n",
       "   26763000,\n",
       "   26239500,\n",
       "   26162100,\n",
       "   26060100,\n",
       "   24708000,\n",
       "   24013200,\n",
       "   9102000,\n",
       "   5901000,\n",
       "   17113500,\n",
       "   22115700,\n",
       "   21665400,\n",
       "   22440900,\n",
       "   23849700,\n",
       "   17112000,\n",
       "   22329300,\n",
       "   24552300,\n",
       "   23757600,\n",
       "   24112500,\n",
       "   16505700,\n",
       "   19150800,\n",
       "   23551200,\n",
       "   25053900,\n",
       "   23814000,\n",
       "   20659200,\n",
       "   13584600,\n",
       "   13619400,\n",
       "   17270700,\n",
       "   15000600,\n",
       "   14754300,\n",
       "   7525800,\n",
       "   12530400,\n",
       "   22888200,\n",
       "   22506300,\n",
       "   20960100,\n",
       "   18480600,\n",
       "   18791100,\n",
       "   21580200,\n",
       "   18957600,\n",
       "   12334800,\n",
       "   13327200,\n",
       "   18405900,\n",
       "   20744700,\n",
       "   19638300,\n",
       "   19955100,\n",
       "   19725900,\n",
       "   10942200,\n",
       "   17754000,\n",
       "   18294300,\n",
       "   12507900,\n",
       "   14997600,\n",
       "   15542400,\n",
       "   19803000,\n",
       "   17049900,\n",
       "   12366300,\n",
       "   18800700,\n",
       "   18742800,\n",
       "   10803300,\n",
       "   14967300,\n",
       "   14865000,\n",
       "   19929900,\n",
       "   19454400,\n",
       "   11920200,\n",
       "   14642400,\n",
       "   17784900,\n",
       "   5798100,\n",
       "   16667700,\n",
       "   17937600,\n",
       "   9270300,\n",
       "   16680900,\n",
       "   18156300,\n",
       "   16782900,\n",
       "   15615600,\n",
       "   5418000,\n",
       "   17425800,\n",
       "   17247000,\n",
       "   17451900,\n",
       "   17021100,\n",
       "   16307700,\n",
       "   15824400,\n",
       "   15452100,\n",
       "   14883000,\n",
       "   13397400,\n",
       "   15677700,\n",
       "   13241700,\n",
       "   13963800,\n",
       "   14564700,\n",
       "   14488500,\n",
       "   14616600,\n",
       "   12983400,\n",
       "   14397300,\n",
       "   11752500,\n",
       "   10240800,\n",
       "   9789900,\n",
       "   12166800,\n",
       "   13235700,\n",
       "   12946200,\n",
       "   14491200,\n",
       "   12331800,\n",
       "   12172200,\n",
       "   12862200,\n",
       "   8812200,\n",
       "   10506300,\n",
       "   7737300,\n",
       "   8444700,\n",
       "   6930600,\n",
       "   3493800,\n",
       "   4746300,\n",
       "   12664200,\n",
       "   12871800,\n",
       "   9344100,\n",
       "   12401400,\n",
       "   2550300,\n",
       "   1456500,\n",
       "   11796600,\n",
       "   12809700,\n",
       "   12057900,\n",
       "   11790300,\n",
       "   6242400,\n",
       "   6928200,\n",
       "   1853400,\n",
       "   1443900,\n",
       "   1747500,\n",
       "   1648800,\n",
       "   2496900,\n",
       "   3503700,\n",
       "   1227000,\n",
       "   3486300,\n",
       "   12167100,\n",
       "   11095800,\n",
       "   11866200,\n",
       "   11741100,\n",
       "   11865600,\n",
       "   8915100,\n",
       "   2108700,\n",
       "   11844900,\n",
       "   11865300,\n",
       "   11823000,\n",
       "   1722900,\n",
       "   1389300,\n",
       "   11312100,\n",
       "   9662400,\n",
       "   12155400,\n",
       "   12516600],\n",
       "  'cat': [3]},\n",
       " {'start': '1994-01-01 00:00:00',\n",
       "  'target': [12251100,\n",
       "   12115500,\n",
       "   3086100,\n",
       "   9433800,\n",
       "   12378600,\n",
       "   11843100,\n",
       "   11923800,\n",
       "   9675600,\n",
       "   10098900,\n",
       "   6901800,\n",
       "   9807000,\n",
       "   11448300,\n",
       "   5919600,\n",
       "   12072900,\n",
       "   11275200,\n",
       "   10608300,\n",
       "   11282700,\n",
       "   9576900,\n",
       "   2680500,\n",
       "   11128500,\n",
       "   11696400,\n",
       "   8781900,\n",
       "   1605300,\n",
       "   1497600,\n",
       "   2223300,\n",
       "   11077500,\n",
       "   11219700,\n",
       "   11105400,\n",
       "   5152800,\n",
       "   11115600,\n",
       "   11151300,\n",
       "   11551500,\n",
       "   10215300,\n",
       "   7810800,\n",
       "   11264400,\n",
       "   10863000,\n",
       "   7615800,\n",
       "   8317800,\n",
       "   11003400,\n",
       "   10951500,\n",
       "   10167300,\n",
       "   11857800,\n",
       "   11822700,\n",
       "   11692800,\n",
       "   10913100,\n",
       "   2523600,\n",
       "   11746800,\n",
       "   12372000,\n",
       "   10849800,\n",
       "   12744900,\n",
       "   9276600,\n",
       "   10457400,\n",
       "   10723500,\n",
       "   13051500,\n",
       "   11433000,\n",
       "   6361800,\n",
       "   5931600,\n",
       "   11954400,\n",
       "   12901200,\n",
       "   13484700,\n",
       "   13636500,\n",
       "   11703000,\n",
       "   8632200,\n",
       "   11421600,\n",
       "   14007900,\n",
       "   10389600,\n",
       "   14532000,\n",
       "   14149500,\n",
       "   14510400,\n",
       "   13887000,\n",
       "   13694400,\n",
       "   12993600,\n",
       "   15241800,\n",
       "   14720400,\n",
       "   15671700,\n",
       "   14768700,\n",
       "   15036900,\n",
       "   12692400,\n",
       "   15465300,\n",
       "   16740600,\n",
       "   16972500,\n",
       "   16361100,\n",
       "   16838100,\n",
       "   16285500,\n",
       "   13220100,\n",
       "   7480500,\n",
       "   8973300,\n",
       "   14380200,\n",
       "   13773000,\n",
       "   17858100,\n",
       "   12752400,\n",
       "   16364100,\n",
       "   15098400,\n",
       "   18490200,\n",
       "   18060000,\n",
       "   16640700,\n",
       "   16228200,\n",
       "   4829400,\n",
       "   13529700,\n",
       "   6033900,\n",
       "   19882200,\n",
       "   17880300,\n",
       "   9891600,\n",
       "   14150400,\n",
       "   16600500,\n",
       "   12211500,\n",
       "   20752200,\n",
       "   16690800,\n",
       "   21902700,\n",
       "   21846000,\n",
       "   14894100,\n",
       "   22471500,\n",
       "   8782800,\n",
       "   1644000,\n",
       "   1906500,\n",
       "   6754500,\n",
       "   17062800,\n",
       "   5402100,\n",
       "   11530800,\n",
       "   22926900,\n",
       "   22839900,\n",
       "   20173200,\n",
       "   22024200,\n",
       "   23365500,\n",
       "   14944200,\n",
       "   22450200,\n",
       "   23861100,\n",
       "   18696900,\n",
       "   25178400,\n",
       "   22921200,\n",
       "   20688000,\n",
       "   19240800,\n",
       "   23148000,\n",
       "   9809700,\n",
       "   25711500,\n",
       "   25959900,\n",
       "   24715800,\n",
       "   18177000,\n",
       "   26052600,\n",
       "   25677900,\n",
       "   19699200,\n",
       "   25094700,\n",
       "   27179100,\n",
       "   27927000,\n",
       "   27372300,\n",
       "   27356400,\n",
       "   20851800,\n",
       "   26792100,\n",
       "   24321300,\n",
       "   20466000,\n",
       "   25455600,\n",
       "   26730600,\n",
       "   27963300,\n",
       "   27939300,\n",
       "   3991200,\n",
       "   15567300,\n",
       "   24279000,\n",
       "   26569500,\n",
       "   18111600,\n",
       "   23340900,\n",
       "   5553000,\n",
       "   8796000,\n",
       "   12813900,\n",
       "   13855200,\n",
       "   20198400,\n",
       "   26444400,\n",
       "   27448800,\n",
       "   29939100,\n",
       "   29228100,\n",
       "   29299800,\n",
       "   27299400,\n",
       "   26591400,\n",
       "   29984700,\n",
       "   29567100,\n",
       "   29163000,\n",
       "   29762400,\n",
       "   27883800,\n",
       "   28063200,\n",
       "   27019200,\n",
       "   27708000,\n",
       "   29480400,\n",
       "   28518000,\n",
       "   26469900,\n",
       "   25139700,\n",
       "   28534200,\n",
       "   29778900,\n",
       "   13948200,\n",
       "   25535700,\n",
       "   29202000,\n",
       "   28663200,\n",
       "   29387400,\n",
       "   29088600,\n",
       "   27029100,\n",
       "   28939800,\n",
       "   29716200,\n",
       "   30702300,\n",
       "   29392500,\n",
       "   25649700,\n",
       "   29775000,\n",
       "   24708000,\n",
       "   27009600,\n",
       "   28566000,\n",
       "   24169500,\n",
       "   22019100,\n",
       "   26946600,\n",
       "   30090900,\n",
       "   30070200,\n",
       "   29777700,\n",
       "   25113300,\n",
       "   20430300,\n",
       "   27532800,\n",
       "   26181600,\n",
       "   28181400,\n",
       "   27667500,\n",
       "   30109800,\n",
       "   26968800,\n",
       "   28920600,\n",
       "   29116500,\n",
       "   29448000,\n",
       "   28409400,\n",
       "   28921500,\n",
       "   20784000,\n",
       "   11661300,\n",
       "   20558100,\n",
       "   27417000,\n",
       "   14417700,\n",
       "   17914200,\n",
       "   22355700,\n",
       "   28523700,\n",
       "   27246000,\n",
       "   29076300,\n",
       "   28690500,\n",
       "   29394300,\n",
       "   28484100,\n",
       "   29756400,\n",
       "   28380600,\n",
       "   28437600,\n",
       "   28741500,\n",
       "   20528700,\n",
       "   27489900,\n",
       "   25921800,\n",
       "   25437900,\n",
       "   27315900,\n",
       "   23946300,\n",
       "   20535900,\n",
       "   24066300,\n",
       "   23184900,\n",
       "   26761500,\n",
       "   26313600,\n",
       "   25996500,\n",
       "   24910800,\n",
       "   15521100,\n",
       "   21680100,\n",
       "   27184200,\n",
       "   26824800,\n",
       "   20758500,\n",
       "   26665800,\n",
       "   21907800,\n",
       "   25317300,\n",
       "   24756600,\n",
       "   24766500,\n",
       "   20051700,\n",
       "   16801800,\n",
       "   14999400,\n",
       "   21699900,\n",
       "   22357200,\n",
       "   25774500,\n",
       "   18818700,\n",
       "   13610400,\n",
       "   12133200,\n",
       "   14074200,\n",
       "   22485600,\n",
       "   25119900,\n",
       "   24903000,\n",
       "   13977900,\n",
       "   16477500,\n",
       "   24474000,\n",
       "   24213300,\n",
       "   24224400,\n",
       "   24073500,\n",
       "   20332500,\n",
       "   2600400,\n",
       "   9897900,\n",
       "   16709400,\n",
       "   22852200,\n",
       "   24052500,\n",
       "   19979400,\n",
       "   6875100,\n",
       "   19486800,\n",
       "   17003400,\n",
       "   18892200,\n",
       "   21528300,\n",
       "   20954700,\n",
       "   20129100,\n",
       "   20630100,\n",
       "   21189000,\n",
       "   22888800,\n",
       "   21988200,\n",
       "   22140000,\n",
       "   16983300,\n",
       "   17628600,\n",
       "   20288700,\n",
       "   21127500,\n",
       "   21000300,\n",
       "   21421800,\n",
       "   20914800,\n",
       "   20878500,\n",
       "   20725200,\n",
       "   20479800,\n",
       "   20131500,\n",
       "   20000700,\n",
       "   19697700,\n",
       "   19236000,\n",
       "   18515700,\n",
       "   17172600,\n",
       "   17761800,\n",
       "   16463100,\n",
       "   15793200,\n",
       "   3342000,\n",
       "   3701400,\n",
       "   18728700,\n",
       "   13440300,\n",
       "   17893200,\n",
       "   13043400,\n",
       "   2300700,\n",
       "   11107800,\n",
       "   14718000,\n",
       "   13452600,\n",
       "   17735400,\n",
       "   17793000,\n",
       "   9274200,\n",
       "   16336500,\n",
       "   17502900,\n",
       "   14881200,\n",
       "   14027400,\n",
       "   16744200,\n",
       "   17038200,\n",
       "   16798200,\n",
       "   16582500,\n",
       "   14893800,\n",
       "   16159200,\n",
       "   15838500,\n",
       "   15973500,\n",
       "   11847000,\n",
       "   13765800,\n",
       "   4628400,\n",
       "   14534100,\n",
       "   13612800,\n",
       "   14831700,\n",
       "   14665200,\n",
       "   14381700,\n",
       "   14277600,\n",
       "   6656700,\n",
       "   11550000,\n",
       "   9030900,\n",
       "   13920900,\n",
       "   12432900,\n",
       "   13533900,\n",
       "   11707200,\n",
       "   13505400,\n",
       "   13230000,\n",
       "   13104900,\n",
       "   13268400,\n",
       "   13092300,\n",
       "   12967500,\n",
       "   9873000,\n",
       "   6572400,\n",
       "   6517800,\n",
       "   12541800,\n",
       "   4639800,\n",
       "   3168600,\n",
       "   12567300,\n",
       "   8251800,\n",
       "   12231600,\n",
       "   12376500,\n",
       "   12395400,\n",
       "   11991900,\n",
       "   11817300,\n",
       "   11980200,\n",
       "   11346000,\n",
       "   11440800,\n",
       "   11697300,\n",
       "   11656800,\n",
       "   11724000,\n",
       "   9118800,\n",
       "   10736100,\n",
       "   10386600,\n",
       "   11052900,\n",
       "   6402900,\n",
       "   785400,\n",
       "   8246400,\n",
       "   6324300,\n",
       "   11579700,\n",
       "   11294100,\n",
       "   10697400,\n",
       "   11816700,\n",
       "   11012400,\n",
       "   10002000,\n",
       "   3060600,\n",
       "   477300,\n",
       "   4029000,\n",
       "   11029800,\n",
       "   12188100,\n",
       "   8679900,\n",
       "   11227500,\n",
       "   11655000,\n",
       "   4454700,\n",
       "   11013300,\n",
       "   12245700,\n",
       "   12088800,\n",
       "   12094800,\n",
       "   11453400,\n",
       "   10734000,\n",
       "   2186100,\n",
       "   4905600,\n",
       "   3768000,\n",
       "   13070700,\n",
       "   13313100,\n",
       "   10827600,\n",
       "   11149500,\n",
       "   6724800,\n",
       "   3901200,\n",
       "   13037400,\n",
       "   13965900,\n",
       "   13959000,\n",
       "   13460700,\n",
       "   14024100,\n",
       "   14124000,\n",
       "   13101900,\n",
       "   14361900,\n",
       "   11685000,\n",
       "   13263300,\n",
       "   1936200,\n",
       "   4916100,\n",
       "   13026000,\n",
       "   11184000,\n",
       "   15600000,\n",
       "   15084300,\n",
       "   15833400,\n",
       "   6815400,\n",
       "   2952000,\n",
       "   3056100,\n",
       "   4112700,\n",
       "   4137000,\n",
       "   3287700,\n",
       "   7716900,\n",
       "   9544800,\n",
       "   13012200,\n",
       "   14995200,\n",
       "   15368400,\n",
       "   17631000,\n",
       "   17949300,\n",
       "   17872500,\n",
       "   18714000,\n",
       "   16377300,\n",
       "   14868900,\n",
       "   9552900,\n",
       "   18653100,\n",
       "   19455600,\n",
       "   19361400,\n",
       "   20039400,\n",
       "   19711500,\n",
       "   19245000,\n",
       "   20329200,\n",
       "   20912100,\n",
       "   20798700,\n",
       "   20419500,\n",
       "   19939200,\n",
       "   18845700,\n",
       "   21089100,\n",
       "   16384200,\n",
       "   2577600,\n",
       "   11245800,\n",
       "   19548600,\n",
       "   21530400,\n",
       "   9200400,\n",
       "   21990300,\n",
       "   22264200,\n",
       "   21164700,\n",
       "   6997800,\n",
       "   8901000,\n",
       "   21068100,\n",
       "   5159100,\n",
       "   9865200,\n",
       "   10278900,\n",
       "   15048300,\n",
       "   4792200,\n",
       "   11404500,\n",
       "   13843500,\n",
       "   21560100,\n",
       "   5273400,\n",
       "   23949600,\n",
       "   25233900,\n",
       "   25351200,\n",
       "   25020300,\n",
       "   22607100,\n",
       "   7954200,\n",
       "   4965900,\n",
       "   16651200,\n",
       "   6181200,\n",
       "   5826300,\n",
       "   16299900,\n",
       "   25188000,\n",
       "   15363000,\n",
       "   2553600,\n",
       "   18103500,\n",
       "   28147200,\n",
       "   27779400,\n",
       "   7089900,\n",
       "   21185100,\n",
       "   26101200,\n",
       "   26109600,\n",
       "   7145700,\n",
       "   27518400,\n",
       "   10956900,\n",
       "   27869700,\n",
       "   16812000,\n",
       "   26972700,\n",
       "   26870100,\n",
       "   29112000,\n",
       "   26414400,\n",
       "   21228600,\n",
       "   17184900,\n",
       "   10119300,\n",
       "   16458000,\n",
       "   25820100,\n",
       "   15779700,\n",
       "   3286500,\n",
       "   23363100,\n",
       "   10347000,\n",
       "   17042700,\n",
       "   21531900,\n",
       "   28844100,\n",
       "   28441500,\n",
       "   29084400,\n",
       "   28322400,\n",
       "   19701900,\n",
       "   26895000,\n",
       "   27971400,\n",
       "   13219200,\n",
       "   14570700,\n",
       "   22967400,\n",
       "   17918100,\n",
       "   25623000,\n",
       "   15300000,\n",
       "   26055600,\n",
       "   8905500,\n",
       "   20541900,\n",
       "   19965600,\n",
       "   25596300,\n",
       "   25416600,\n",
       "   23834100,\n",
       "   24602700,\n",
       "   17269200,\n",
       "   29417700,\n",
       "   29087700,\n",
       "   28818300,\n",
       "   30140400,\n",
       "   25950900,\n",
       "   30933300,\n",
       "   31464300,\n",
       "   23733000,\n",
       "   18244200,\n",
       "   29454900,\n",
       "   19585500,\n",
       "   20659800,\n",
       "   11239500,\n",
       "   18041100,\n",
       "   20952300,\n",
       "   19780800,\n",
       "   20404200,\n",
       "   18651900,\n",
       "   28840200,\n",
       "   24681000,\n",
       "   24114300,\n",
       "   18660000,\n",
       "   28299900,\n",
       "   22374000,\n",
       "   15560700,\n",
       "   6937500,\n",
       "   10527300,\n",
       "   15424500,\n",
       "   19984200,\n",
       "   20268900,\n",
       "   19822500,\n",
       "   21000900,\n",
       "   24307800,\n",
       "   21495300,\n",
       "   27890100,\n",
       "   29526000,\n",
       "   29839500,\n",
       "   21708000,\n",
       "   29229300,\n",
       "   12204300,\n",
       "   23004300,\n",
       "   21568200,\n",
       "   29820900,\n",
       "   29844600,\n",
       "   26444700,\n",
       "   28930500,\n",
       "   29511900,\n",
       "   22222800,\n",
       "   27210000,\n",
       "   28671900,\n",
       "   26646900,\n",
       "   27704400,\n",
       "   26466000,\n",
       "   25907700,\n",
       "   29015100,\n",
       "   28110900,\n",
       "   25240200,\n",
       "   27817500,\n",
       "   22334400,\n",
       "   15361200,\n",
       "   21406800,\n",
       "   17991000,\n",
       "   24539700,\n",
       "   25991700,\n",
       "   27373500,\n",
       "   27216000,\n",
       "   26899500,\n",
       "   26389800,\n",
       "   23411400,\n",
       "   27348600,\n",
       "   27747900,\n",
       "   27268800,\n",
       "   27246000,\n",
       "   27406200,\n",
       "   26039400,\n",
       "   24049800,\n",
       "   17043000,\n",
       "   8340900,\n",
       "   21427500,\n",
       "   25016100,\n",
       "   19266300,\n",
       "   23917500,\n",
       "   24033900,\n",
       "   16634100,\n",
       "   18050700,\n",
       "   25580700,\n",
       "   24032100,\n",
       "   24299700,\n",
       "   17265900,\n",
       "   18606600,\n",
       "   24040500,\n",
       "   25362900,\n",
       "   25278300,\n",
       "   22566600,\n",
       "   20928600,\n",
       "   20655600,\n",
       "   22240200,\n",
       "   4683900,\n",
       "   12736800,\n",
       "   7602600,\n",
       "   10500600,\n",
       "   24547200,\n",
       "   22563300,\n",
       "   21905100,\n",
       "   22154400,\n",
       "   17628300,\n",
       "   22250100,\n",
       "   19185000,\n",
       "   16149300,\n",
       "   14370900,\n",
       "   19782600,\n",
       "   21395700,\n",
       "   20149800,\n",
       "   20757000,\n",
       "   18523500,\n",
       "   17855700,\n",
       "   18309300,\n",
       "   18209700,\n",
       "   16798200,\n",
       "   17196300,\n",
       "   19188600,\n",
       "   19502700,\n",
       "   14971800,\n",
       "   21006900,\n",
       "   17443500,\n",
       "   19152900,\n",
       "   17577600,\n",
       "   17628300,\n",
       "   18067500,\n",
       "   20274000,\n",
       "   16945500,\n",
       "   17679900,\n",
       "   14852100,\n",
       "   17527800,\n",
       "   12775500,\n",
       "   18338700,\n",
       "   18394800,\n",
       "   16098900,\n",
       "   14700000,\n",
       "   18363900,\n",
       "   18168000,\n",
       "   12615000,\n",
       "   14869200,\n",
       "   17201400,\n",
       "   17148600,\n",
       "   17379600,\n",
       "   16937400,\n",
       "   16269600,\n",
       "   15765000,\n",
       "   15997200,\n",
       "   16209600,\n",
       "   13209300,\n",
       "   15665100,\n",
       "   15133200,\n",
       "   13231800,\n",
       "   15459600,\n",
       "   15204900,\n",
       "   14617500,\n",
       "   15147600,\n",
       "   14710200,\n",
       "   14585400,\n",
       "   9451200,\n",
       "   13090200,\n",
       "   10436400,\n",
       "   13398600,\n",
       "   13728600,\n",
       "   14194800,\n",
       "   13404300,\n",
       "   13550700,\n",
       "   12291900,\n",
       "   12918900,\n",
       "   12744300,\n",
       "   10872300,\n",
       "   8775600,\n",
       "   3529200,\n",
       "   4340400,\n",
       "   8151900,\n",
       "   11432700,\n",
       "   12477900,\n",
       "   7919400,\n",
       "   11679600,\n",
       "   4776900,\n",
       "   3631200,\n",
       "   11907000,\n",
       "   12264300,\n",
       "   12027900,\n",
       "   9699300,\n",
       "   7813800,\n",
       "   2229600,\n",
       "   2145600,\n",
       "   3815100,\n",
       "   1628400,\n",
       "   2398800,\n",
       "   6317400,\n",
       "   11037600,\n",
       "   1224300,\n",
       "   7247400,\n",
       "   11643000,\n",
       "   10793100,\n",
       "   11452500,\n",
       "   10871400,\n",
       "   11614800,\n",
       "   10750800,\n",
       "   5511000,\n",
       "   11698500,\n",
       "   11774700,\n",
       "   11540400,\n",
       "   7181100,\n",
       "   1349400,\n",
       "   9665100,\n",
       "   10938300,\n",
       "   11937600,\n",
       "   8480100],\n",
       "  'cat': [4]}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-eeffc01b6b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist_of_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_kgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mactual_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_kgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-287c750eac00>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, encoding, num_samples, quantiles)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-287c750eac00>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "list_of_df = predictor.predict(train_kgl[:5])\n",
    "actual_data = train_kgl[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function ot split the data into train, test, and validate data sets then create and return json file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
